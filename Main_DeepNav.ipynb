{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) detected: ['/physical_device:GPU:0']\n",
      "Found 4262 validated image filenames.\n",
      "Found 1066 validated image filenames.\n",
      "Epoch 1/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 64.3457 - mae: 0.8493\n",
      "Epoch 1: val_loss improved from inf to 54.86045, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1: train_loss=65.4874, val_loss=46.7905\n",
      "134/134 [==============================] - 110s 620ms/step - loss: 64.3457 - mae: 0.8493 - val_loss: 54.8605 - val_mae: 0.8342\n",
      "Epoch 2/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 56.7969 - mae: 0.8350\n",
      "Epoch 2: val_loss improved from 54.86045 to 51.56994, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 2: train_loss=41.9852, val_loss=50.4078\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 56.7969 - mae: 0.8350 - val_loss: 51.5699 - val_mae: 0.8266\n",
      "Epoch 3/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 54.4353 - mae: 0.8277\n",
      "Epoch 3: val_loss improved from 51.56994 to 49.32628, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 3: train_loss=50.2960, val_loss=60.7855\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 54.4353 - mae: 0.8277 - val_loss: 49.3263 - val_mae: 0.8246\n",
      "Epoch 4/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 50.7027 - mae: 0.8213\n",
      "Epoch 4: val_loss improved from 49.32628 to 48.15312, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 4: train_loss=49.3276, val_loss=60.6516\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 50.7027 - mae: 0.8213 - val_loss: 48.1531 - val_mae: 0.8216\n",
      "Epoch 5/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 47.8150 - mae: 0.8172\n",
      "Epoch 5: val_loss improved from 48.15312 to 45.82751, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 5: train_loss=34.4824, val_loss=37.6330\n",
      "134/134 [==============================] - 15s 110ms/step - loss: 47.8150 - mae: 0.8172 - val_loss: 45.8275 - val_mae: 0.8125\n",
      "Epoch 6/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 45.2454 - mae: 0.8126\n",
      "Epoch 6: val_loss improved from 45.82751 to 41.78761, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 6: train_loss=47.2111, val_loss=33.4019\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 45.2454 - mae: 0.8126 - val_loss: 41.7876 - val_mae: 0.8136\n",
      "Epoch 7/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 42.6912 - mae: 0.8090\n",
      "Epoch 7: val_loss improved from 41.78761 to 38.37326, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 7: train_loss=24.5220, val_loss=43.6801\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 42.6912 - mae: 0.8090 - val_loss: 38.3733 - val_mae: 0.8057\n",
      "Epoch 8/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 40.1891 - mae: 0.8052\n",
      "Epoch 8: val_loss improved from 38.37326 to 35.26360, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 8: train_loss=39.1926, val_loss=38.9789\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 40.1891 - mae: 0.8052 - val_loss: 35.2636 - val_mae: 0.7982\n",
      "Epoch 9/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 37.9901 - mae: 0.8010\n",
      "Epoch 9: val_loss improved from 35.26360 to 33.75166, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 9: train_loss=29.8596, val_loss=35.5320\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 37.9901 - mae: 0.8010 - val_loss: 33.7517 - val_mae: 0.8024\n",
      "Epoch 10/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 36.1137 - mae: 0.7976\n",
      "Epoch 10: val_loss improved from 33.75166 to 33.60112, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 10: train_loss=44.4897, val_loss=27.4523\n",
      "134/134 [==============================] - 15s 109ms/step - loss: 36.1137 - mae: 0.7976 - val_loss: 33.6011 - val_mae: 0.7988\n",
      "Epoch 11/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 34.4726 - mae: 0.7951\n",
      "Epoch 11: val_loss improved from 33.60112 to 32.39293, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 11: train_loss=30.9894, val_loss=29.7550\n",
      "134/134 [==============================] - 14s 105ms/step - loss: 34.4726 - mae: 0.7951 - val_loss: 32.3929 - val_mae: 0.7923\n",
      "Epoch 12/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 33.0892 - mae: 0.7906\n",
      "Epoch 12: val_loss improved from 32.39293 to 30.17500, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 12: train_loss=25.7962, val_loss=42.0099\n",
      "134/134 [==============================] - 16s 115ms/step - loss: 33.0892 - mae: 0.7906 - val_loss: 30.1750 - val_mae: 0.7897\n",
      "Epoch 13/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 29.9702 - mae: 0.7856\n",
      "Epoch 13: val_loss improved from 30.17500 to 27.38102, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 13: train_loss=22.2396, val_loss=21.3324\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 29.9702 - mae: 0.7856 - val_loss: 27.3810 - val_mae: 0.7853\n",
      "Epoch 14/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 27.8918 - mae: 0.7819\n",
      "Epoch 14: val_loss improved from 27.38102 to 25.01843, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 14: train_loss=14.9646, val_loss=18.1967\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 27.8918 - mae: 0.7819 - val_loss: 25.0184 - val_mae: 0.7800\n",
      "Epoch 15/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 26.2564 - mae: 0.7783\n",
      "Epoch 15: val_loss improved from 25.01843 to 23.62193, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Epoch 15: train_loss=19.4369, val_loss=31.7563\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 26.2564 - mae: 0.7783 - val_loss: 23.6219 - val_mae: 0.7760\n",
      "Epoch 16/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 24.8823 - mae: 0.7753\n",
      "Epoch 16: val_loss did not improve from 23.62193\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 16: train_loss=25.0673, val_loss=28.0945\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 24.8823 - mae: 0.7753 - val_loss: 29.5341 - val_mae: 0.7936\n",
      "Epoch 17/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 22.8519 - mae: 0.7705\n",
      "Epoch 17: val_loss improved from 23.62193 to 22.58142, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 17: train_loss=21.3626, val_loss=21.6936\n",
      "134/134 [==============================] - 17s 123ms/step - loss: 22.8519 - mae: 0.7705 - val_loss: 22.5814 - val_mae: 0.7681\n",
      "Epoch 18/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 22.9235 - mae: 0.7707\n",
      "Epoch 18: val_loss improved from 22.58142 to 22.54473, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 18: train_loss=18.2737, val_loss=22.6717\n",
      "134/134 [==============================] - 16s 121ms/step - loss: 22.9235 - mae: 0.7707 - val_loss: 22.5447 - val_mae: 0.7647\n",
      "Epoch 19/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 21.0901 - mae: 0.7654\n",
      "Epoch 19: val_loss improved from 22.54473 to 21.61077, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 19: train_loss=16.9527, val_loss=23.3600\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 21.0901 - mae: 0.7654 - val_loss: 21.6108 - val_mae: 0.7695\n",
      "Epoch 20/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 20.8600 - mae: 0.7640\n",
      "Epoch 20: val_loss improved from 21.61077 to 19.56438, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 20: train_loss=20.4321, val_loss=12.6509\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 20.8600 - mae: 0.7640 - val_loss: 19.5644 - val_mae: 0.7630\n",
      "Epoch 21/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 18.8361 - mae: 0.7595\n",
      "Epoch 21: val_loss improved from 19.56438 to 18.47613, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 21: train_loss=15.0668, val_loss=11.7409\n",
      "134/134 [==============================] - 16s 116ms/step - loss: 18.8361 - mae: 0.7595 - val_loss: 18.4761 - val_mae: 0.7617\n",
      "Epoch 22/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 18.7254 - mae: 0.7588\n",
      "Epoch 22: val_loss improved from 18.47613 to 17.50536, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 22: train_loss=10.3661, val_loss=12.9962\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 18.7254 - mae: 0.7588 - val_loss: 17.5054 - val_mae: 0.7544\n",
      "Epoch 23/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 18.0881 - mae: 0.7572\n",
      "Epoch 23: val_loss improved from 17.50536 to 16.68646, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 23: train_loss=9.2403, val_loss=28.0056\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 18.0881 - mae: 0.7572 - val_loss: 16.6865 - val_mae: 0.7540\n",
      "Epoch 24/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 16.2898 - mae: 0.7517\n",
      "Epoch 24: val_loss did not improve from 16.68646\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 24: train_loss=17.7592, val_loss=19.9879\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 16.2898 - mae: 0.7517 - val_loss: 18.4753 - val_mae: 0.7525\n",
      "Epoch 25/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 15.7355 - mae: 0.7487\n",
      "Epoch 25: val_loss did not improve from 16.68646\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 25: train_loss=11.0743, val_loss=24.1919\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 15.7355 - mae: 0.7487 - val_loss: 17.1848 - val_mae: 0.7482\n",
      "Epoch 26/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 15.5189 - mae: 0.7457\n",
      "Epoch 26: val_loss improved from 16.68646 to 14.28312, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 26: train_loss=12.3424, val_loss=10.1422\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 15.5189 - mae: 0.7457 - val_loss: 14.2831 - val_mae: 0.7413\n",
      "Epoch 27/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 15.3602 - mae: 0.7449\n",
      "Epoch 27: val_loss did not improve from 14.28312\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 27: train_loss=19.4985, val_loss=19.0459\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 15.3602 - mae: 0.7449 - val_loss: 15.9680 - val_mae: 0.7404\n",
      "Epoch 28/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 14.5581 - mae: 0.7403\n",
      "Epoch 28: val_loss did not improve from 14.28312\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 28: train_loss=10.5117, val_loss=39.6342\n",
      "134/134 [==============================] - 16s 118ms/step - loss: 14.5581 - mae: 0.7403 - val_loss: 15.7944 - val_mae: 0.7381\n",
      "Epoch 29/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 14.5828 - mae: 0.7394\n",
      "Epoch 29: val_loss improved from 14.28312 to 13.77722, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 29: train_loss=7.8801, val_loss=9.2229\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 14.5828 - mae: 0.7394 - val_loss: 13.7772 - val_mae: 0.7337\n",
      "Epoch 30/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 13.5316 - mae: 0.7341\n",
      "Epoch 30: val_loss improved from 13.77722 to 12.80815, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 30: train_loss=8.4030, val_loss=8.0098\n",
      "134/134 [==============================] - 16s 115ms/step - loss: 13.5316 - mae: 0.7341 - val_loss: 12.8082 - val_mae: 0.7311\n",
      "Epoch 31/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 13.7515 - mae: 0.7327\n",
      "Epoch 31: val_loss did not improve from 12.80815\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 31: train_loss=11.9663, val_loss=10.9480\n",
      "134/134 [==============================] - 16s 114ms/step - loss: 13.7515 - mae: 0.7327 - val_loss: 14.4387 - val_mae: 0.7309\n",
      "Epoch 32/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 12.9734 - mae: 0.7278\n",
      "Epoch 32: val_loss did not improve from 12.80815\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 32: train_loss=17.2916, val_loss=10.8365\n",
      "134/134 [==============================] - 15s 112ms/step - loss: 12.9734 - mae: 0.7278 - val_loss: 15.0137 - val_mae: 0.7339\n",
      "Epoch 33/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 13.2456 - mae: 0.7252\n",
      "Epoch 33: val_loss did not improve from 12.80815\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 33: train_loss=7.8907, val_loss=7.7722\n",
      "134/134 [==============================] - 14s 105ms/step - loss: 13.2456 - mae: 0.7252 - val_loss: 13.1332 - val_mae: 0.7198\n",
      "Epoch 34/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 12.2878 - mae: 0.7191\n",
      "Epoch 34: val_loss improved from 12.80815 to 11.29537, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Epoch 34: train_loss=7.6944, val_loss=7.3993\n",
      "134/134 [==============================] - 15s 113ms/step - loss: 12.2878 - mae: 0.7191 - val_loss: 11.2954 - val_mae: 0.7113\n",
      "Epoch 35/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 11.9243 - mae: 0.7149\n",
      "Epoch 35: val_loss did not improve from 11.29537\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 35: train_loss=6.1218, val_loss=6.9250\n",
      "134/134 [==============================] - 14s 106ms/step - loss: 11.9243 - mae: 0.7149 - val_loss: 11.7392 - val_mae: 0.7151\n",
      "Epoch 36/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 11.3639 - mae: 0.7091\n",
      "Epoch 36: val_loss improved from 11.29537 to 10.64839, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 36: train_loss=13.9327, val_loss=8.7399\n",
      "134/134 [==============================] - 15s 110ms/step - loss: 11.3639 - mae: 0.7091 - val_loss: 10.6484 - val_mae: 0.7047\n",
      "Epoch 37/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 10.5381 - mae: 0.7023\n",
      "Epoch 37: val_loss improved from 10.64839 to 9.54510, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 37: train_loss=16.5930, val_loss=11.1209\n",
      "134/134 [==============================] - 15s 111ms/step - loss: 10.5381 - mae: 0.7023 - val_loss: 9.5451 - val_mae: 0.6992\n",
      "Epoch 38/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 11.5703 - mae: 0.7006\n",
      "Epoch 38: val_loss did not improve from 9.54510\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 38: train_loss=22.9580, val_loss=10.7518\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 11.5703 - mae: 0.7006 - val_loss: 13.1230 - val_mae: 0.6945\n",
      "Epoch 39/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 10.1765 - mae: 0.6912\n",
      "Epoch 39: val_loss did not improve from 9.54510\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch 39: train_loss=6.0401, val_loss=9.2399\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 10.1765 - mae: 0.6912 - val_loss: 11.8455 - val_mae: 0.6911\n",
      "Epoch 40/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 10.0230 - mae: 0.6870\n",
      "Epoch 40: val_loss improved from 9.54510 to 9.45124, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 40: train_loss=10.4163, val_loss=7.9197\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 10.0230 - mae: 0.6870 - val_loss: 9.4512 - val_mae: 0.6826\n",
      "Epoch 41/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 9.7656 - mae: 0.6800\n",
      "Epoch 41: val_loss did not improve from 9.45124\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 41: train_loss=5.0036, val_loss=17.7652\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 9.7656 - mae: 0.6800 - val_loss: 10.0482 - val_mae: 0.6762\n",
      "Epoch 42/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 9.3766 - mae: 0.6758\n",
      "Epoch 42: val_loss did not improve from 9.45124\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 42: train_loss=10.5625, val_loss=8.1955\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 9.3766 - mae: 0.6758 - val_loss: 10.6645 - val_mae: 0.6764\n",
      "Epoch 43/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 8.9045 - mae: 0.6686\n",
      "Epoch 43: val_loss did not improve from 9.45124\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 43: train_loss=6.2296, val_loss=19.7694\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 8.9045 - mae: 0.6686 - val_loss: 9.8106 - val_mae: 0.6621\n",
      "Epoch 44/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 10.1026 - mae: 0.6671\n",
      "Epoch 44: val_loss did not improve from 9.45124\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 44: train_loss=6.2681, val_loss=6.8616\n",
      "134/134 [==============================] - 17s 124ms/step - loss: 10.1026 - mae: 0.6671 - val_loss: 10.8572 - val_mae: 0.6628\n",
      "Epoch 45/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 8.6949 - mae: 0.6583\n",
      "Epoch 45: val_loss improved from 9.45124 to 7.29987, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 45: train_loss=4.1415, val_loss=7.8946\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 8.6949 - mae: 0.6583 - val_loss: 7.2999 - val_mae: 0.6494\n",
      "Epoch 46/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 9.4388 - mae: 0.6556\n",
      "Epoch 46: val_loss did not improve from 7.29987\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 46: train_loss=5.6657, val_loss=15.1234\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 9.4388 - mae: 0.6556 - val_loss: 8.1155 - val_mae: 0.6426\n",
      "Epoch 47/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.9917 - mae: 0.6445\n",
      "Epoch 47: val_loss did not improve from 7.29987\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 47: train_loss=12.9202, val_loss=6.9051\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 7.9917 - mae: 0.6445 - val_loss: 8.4312 - val_mae: 0.6373\n",
      "Epoch 48/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.7788 - mae: 0.6362\n",
      "Epoch 48: val_loss did not improve from 7.29987\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 48: train_loss=9.6775, val_loss=10.6025\n",
      "134/134 [==============================] - 18s 130ms/step - loss: 7.7788 - mae: 0.6362 - val_loss: 9.2100 - val_mae: 0.6319\n",
      "Epoch 49/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.5623 - mae: 0.6305\n",
      "Epoch 49: val_loss did not improve from 7.29987\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 49: train_loss=12.9887, val_loss=14.7187\n",
      "134/134 [==============================] - 17s 128ms/step - loss: 7.5623 - mae: 0.6305 - val_loss: 11.2995 - val_mae: 0.6282\n",
      "Epoch 50/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.1694 - mae: 0.6237\n",
      "Epoch 50: val_loss improved from 7.29987 to 7.20922, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 50: train_loss=12.9751, val_loss=6.0669\n",
      "134/134 [==============================] - 18s 130ms/step - loss: 7.1694 - mae: 0.6237 - val_loss: 7.2092 - val_mae: 0.6159\n",
      "Epoch 51/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.1620 - mae: 0.6162\n",
      "Epoch 51: val_loss did not improve from 7.20922\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 51: train_loss=5.5498, val_loss=5.5703\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 7.1620 - mae: 0.6162 - val_loss: 8.4984 - val_mae: 0.6159\n",
      "Epoch 52/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.2042 - mae: 0.6144\n",
      "Epoch 52: val_loss improved from 7.20922 to 6.41063, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 52: train_loss=4.5249, val_loss=5.9165\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 7.2042 - mae: 0.6144 - val_loss: 6.4106 - val_mae: 0.5977\n",
      "Epoch 53/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 6.5850 - mae: 0.6042\n",
      "Epoch 53: val_loss did not improve from 6.41063\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 53: train_loss=7.8377, val_loss=5.8603\n",
      "134/134 [==============================] - 16s 119ms/step - loss: 6.5850 - mae: 0.6042 - val_loss: 7.6016 - val_mae: 0.5899\n",
      "Epoch 54/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.3442 - mae: 0.6039\n",
      "Epoch 54: val_loss improved from 6.41063 to 6.02405, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 54: train_loss=4.7072, val_loss=6.3192\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 7.3442 - mae: 0.6039 - val_loss: 6.0240 - val_mae: 0.5847\n",
      "Epoch 55/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 6.1959 - mae: 0.5914\n",
      "Epoch 55: val_loss improved from 6.02405 to 5.55446, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 55: train_loss=4.6620, val_loss=5.2934\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 6.1959 - mae: 0.5914 - val_loss: 5.5545 - val_mae: 0.5724\n",
      "Epoch 56/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 6.0414 - mae: 0.5847\n",
      "Epoch 56: val_loss did not improve from 5.55446\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 56: train_loss=3.5371, val_loss=4.7928\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 6.0414 - mae: 0.5847 - val_loss: 6.4101 - val_mae: 0.5670\n",
      "Epoch 57/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.7895 - mae: 0.5745\n",
      "Epoch 57: val_loss improved from 5.55446 to 5.29454, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 57: train_loss=4.0876, val_loss=6.5511\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 5.7895 - mae: 0.5745 - val_loss: 5.2945 - val_mae: 0.5520\n",
      "Epoch 58/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 6.3591 - mae: 0.5741\n",
      "Epoch 58: val_loss did not improve from 5.29454\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 58: train_loss=7.3885, val_loss=6.5181\n",
      "134/134 [==============================] - 17s 129ms/step - loss: 6.3591 - mae: 0.5741 - val_loss: 8.5794 - val_mae: 0.5611\n",
      "Epoch 59/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.8356 - mae: 0.5665\n",
      "Epoch 59: val_loss did not improve from 5.29454\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 59: train_loss=5.7395, val_loss=6.7875\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 5.8356 - mae: 0.5665 - val_loss: 5.4247 - val_mae: 0.5440\n",
      "Epoch 60/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.4755 - mae: 0.5582\n",
      "Epoch 60: val_loss improved from 5.29454 to 4.80976, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 60: train_loss=3.1928, val_loss=3.6723\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 5.4755 - mae: 0.5582 - val_loss: 4.8098 - val_mae: 0.5285\n",
      "Epoch 61/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.4135 - mae: 0.5495\n",
      "Epoch 61: val_loss did not improve from 4.80976\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 61: train_loss=3.5866, val_loss=6.5386\n",
      "134/134 [==============================] - 17s 125ms/step - loss: 5.4135 - mae: 0.5495 - val_loss: 5.1416 - val_mae: 0.5270\n",
      "Epoch 62/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.3433 - mae: 0.5434\n",
      "Epoch 62: val_loss did not improve from 4.80976\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 62: train_loss=3.0173, val_loss=4.3445\n",
      "134/134 [==============================] - 17s 127ms/step - loss: 5.3433 - mae: 0.5434 - val_loss: 5.1818 - val_mae: 0.5271\n",
      "Epoch 63/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.9854 - mae: 0.5427\n",
      "Epoch 63: val_loss did not improve from 4.80976\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 63: train_loss=4.5595, val_loss=3.2051\n",
      "134/134 [==============================] - 16s 120ms/step - loss: 5.9854 - mae: 0.5427 - val_loss: 5.2765 - val_mae: 0.5133\n",
      "Epoch 64/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.3654 - mae: 0.5336\n",
      "Epoch 64: val_loss did not improve from 4.80976\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 64: train_loss=3.8125, val_loss=3.5489\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 5.3654 - mae: 0.5336 - val_loss: 5.9577 - val_mae: 0.5134\n",
      "Epoch 65/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.0881 - mae: 0.5268\n",
      "Epoch 65: val_loss improved from 4.80976 to 4.72251, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 65: train_loss=3.2288, val_loss=3.9848\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 5.0881 - mae: 0.5268 - val_loss: 4.7225 - val_mae: 0.5019\n",
      "Epoch 66/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.6155 - mae: 0.5151\n",
      "Epoch 66: val_loss improved from 4.72251 to 4.49681, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 66: train_loss=5.0545, val_loss=3.3807\n",
      "134/134 [==============================] - 18s 131ms/step - loss: 4.6155 - mae: 0.5151 - val_loss: 4.4968 - val_mae: 0.4936\n",
      "Epoch 67/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.0710 - mae: 0.5153\n",
      "Epoch 67: val_loss did not improve from 4.49681\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 67: train_loss=4.1615, val_loss=4.9708\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 5.0710 - mae: 0.5153 - val_loss: 5.3350 - val_mae: 0.4830\n",
      "Epoch 68/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.6984 - mae: 0.5103\n",
      "Epoch 68: val_loss improved from 4.49681 to 4.18484, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 68: train_loss=2.2272, val_loss=1.8416\n",
      "134/134 [==============================] - 16s 117ms/step - loss: 4.6984 - mae: 0.5103 - val_loss: 4.1848 - val_mae: 0.4817\n",
      "Epoch 69/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.4056 - mae: 0.4973\n",
      "Epoch 69: val_loss improved from 4.18484 to 3.91006, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 69: train_loss=3.1169, val_loss=2.5516\n",
      "134/134 [==============================] - 19s 143ms/step - loss: 4.4056 - mae: 0.4973 - val_loss: 3.9101 - val_mae: 0.4677\n",
      "Epoch 70/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.2265 - mae: 0.4902\n",
      "Epoch 70: val_loss did not improve from 3.91006\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 70: train_loss=2.7775, val_loss=2.9498\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 4.2265 - mae: 0.4902 - val_loss: 4.1148 - val_mae: 0.4625\n",
      "Epoch 71/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.2543 - mae: 0.4882\n",
      "Epoch 71: val_loss did not improve from 3.91006\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 71: train_loss=3.1850, val_loss=3.4725\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 4.2543 - mae: 0.4882 - val_loss: 4.5625 - val_mae: 0.4522\n",
      "Epoch 72/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 6.0598 - mae: 0.5002\n",
      "Epoch 72: val_loss did not improve from 3.91006\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 72: train_loss=3.6565, val_loss=2.4978\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 6.0598 - mae: 0.5002 - val_loss: 5.2082 - val_mae: 0.4747\n",
      "Epoch 73/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.4114 - mae: 0.4813\n",
      "Epoch 73: val_loss improved from 3.91006 to 3.66411, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 73: train_loss=2.2860, val_loss=2.2076\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 4.4114 - mae: 0.4813 - val_loss: 3.6641 - val_mae: 0.4488\n",
      "Epoch 74/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.0890 - mae: 0.4697\n",
      "Epoch 74: val_loss did not improve from 3.66411\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 74: train_loss=3.8406, val_loss=6.1133\n",
      "134/134 [==============================] - 20s 148ms/step - loss: 4.0890 - mae: 0.4697 - val_loss: 7.1096 - val_mae: 0.4620\n",
      "Epoch 75/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 8.5166 - mae: 0.5073\n",
      "Epoch 75: val_loss did not improve from 3.66411\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 75: train_loss=2.9271, val_loss=15.9892\n",
      "134/134 [==============================] - 20s 147ms/step - loss: 8.5166 - mae: 0.5073 - val_loss: 4.6642 - val_mae: 0.4538\n",
      "Epoch 76/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.0690 - mae: 0.4649\n",
      "Epoch 76: val_loss improved from 3.66411 to 3.59667, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 76: train_loss=2.2473, val_loss=2.6150\n",
      "134/134 [==============================] - 20s 151ms/step - loss: 4.0690 - mae: 0.4649 - val_loss: 3.5967 - val_mae: 0.4342\n",
      "Epoch 77/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.8555 - mae: 0.4600\n",
      "Epoch 77: val_loss improved from 3.59667 to 3.40338, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 77: train_loss=2.7304, val_loss=4.0144\n",
      "134/134 [==============================] - 20s 151ms/step - loss: 3.8555 - mae: 0.4600 - val_loss: 3.4034 - val_mae: 0.4342\n",
      "Epoch 78/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.6191 - mae: 0.4496\n",
      "Epoch 78: val_loss improved from 3.40338 to 3.31708, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 78: train_loss=1.9623, val_loss=2.4511\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 3.6191 - mae: 0.4496 - val_loss: 3.3171 - val_mae: 0.4146\n",
      "Epoch 79/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.6256 - mae: 0.4426\n",
      "Epoch 79: val_loss improved from 3.31708 to 3.06116, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 79: train_loss=1.7149, val_loss=2.9055\n",
      "134/134 [==============================] - 21s 153ms/step - loss: 3.6256 - mae: 0.4426 - val_loss: 3.0612 - val_mae: 0.4074\n",
      "Epoch 80/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.5698 - mae: 0.4355\n",
      "Epoch 80: val_loss did not improve from 3.06116\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 80: train_loss=2.0292, val_loss=5.3410\n",
      "134/134 [==============================] - 20s 148ms/step - loss: 3.5698 - mae: 0.4355 - val_loss: 3.6551 - val_mae: 0.4084\n",
      "Epoch 81/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.7382 - mae: 0.4350\n",
      "Epoch 81: val_loss did not improve from 3.06116\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 81: train_loss=3.8485, val_loss=4.5016\n",
      "134/134 [==============================] - 20s 147ms/step - loss: 3.7382 - mae: 0.4350 - val_loss: 7.6550 - val_mae: 0.4342\n",
      "Epoch 82/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.4594 - mae: 0.4429\n",
      "Epoch 82: val_loss did not improve from 3.06116\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 82: train_loss=2.1692, val_loss=3.0500\n",
      "134/134 [==============================] - 20s 148ms/step - loss: 4.4594 - mae: 0.4429 - val_loss: 3.7068 - val_mae: 0.4045\n",
      "Epoch 83/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.4072 - mae: 0.4249\n",
      "Epoch 83: val_loss improved from 3.06116 to 2.96037, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 83: train_loss=1.7189, val_loss=2.6435\n",
      "134/134 [==============================] - 21s 153ms/step - loss: 3.4072 - mae: 0.4249 - val_loss: 2.9604 - val_mae: 0.3867\n",
      "Epoch 84/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.3046 - mae: 0.4204\n",
      "Epoch 84: val_loss improved from 2.96037 to 2.82648, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch 84: train_loss=2.4310, val_loss=2.1653\n",
      "134/134 [==============================] - 20s 151ms/step - loss: 3.3046 - mae: 0.4204 - val_loss: 2.8265 - val_mae: 0.3874\n",
      "Epoch 85/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.3041 - mae: 0.4131\n",
      "Epoch 85: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 85: train_loss=1.0900, val_loss=2.2691\n",
      "134/134 [==============================] - 20s 150ms/step - loss: 3.3041 - mae: 0.4131 - val_loss: 3.0103 - val_mae: 0.3736\n",
      "Epoch 86/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.2080 - mae: 0.4065\n",
      "Epoch 86: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 86: train_loss=1.8547, val_loss=2.1207\n",
      "134/134 [==============================] - 20s 151ms/step - loss: 3.2080 - mae: 0.4065 - val_loss: 3.0149 - val_mae: 0.3716\n",
      "Epoch 87/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.3560 - mae: 0.4044\n",
      "Epoch 87: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Epoch 87: train_loss=1.5458, val_loss=2.0728\n",
      "134/134 [==============================] - 20s 151ms/step - loss: 3.3560 - mae: 0.4044 - val_loss: 2.8699 - val_mae: 0.3716\n",
      "Epoch 88/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.3355 - mae: 0.4026\n",
      "Epoch 88: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 88: train_loss=1.8221, val_loss=4.9917\n",
      "134/134 [==============================] - 21s 153ms/step - loss: 3.3355 - mae: 0.4026 - val_loss: 2.9907 - val_mae: 0.3683\n",
      "Epoch 89/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.4159 - mae: 0.4056\n",
      "Epoch 89: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 89: train_loss=4.1352, val_loss=3.1349\n",
      "134/134 [==============================] - 20s 150ms/step - loss: 3.4159 - mae: 0.4056 - val_loss: 3.8030 - val_mae: 0.3727\n",
      "Epoch 90/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.8042 - mae: 0.4093\n",
      "Epoch 90: val_loss did not improve from 2.82648\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 90: train_loss=2.0305, val_loss=2.3336\n",
      "134/134 [==============================] - 20s 149ms/step - loss: 3.8042 - mae: 0.4093 - val_loss: 4.1869 - val_mae: 0.3793\n",
      "Epoch 91/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.1031 - mae: 0.3986\n",
      "Epoch 91: val_loss improved from 2.82648 to 2.58570, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 91: train_loss=2.1707, val_loss=2.8248\n",
      "134/134 [==============================] - 21s 157ms/step - loss: 3.1031 - mae: 0.3986 - val_loss: 2.5857 - val_mae: 0.3488\n",
      "Epoch 92/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.8594 - mae: 0.3830\n",
      "Epoch 92: val_loss improved from 2.58570 to 2.41544, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 92: train_loss=1.0040, val_loss=1.7310\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 2.8594 - mae: 0.3830 - val_loss: 2.4154 - val_mae: 0.3359\n",
      "Epoch 93/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.9005 - mae: 0.3757\n",
      "Epoch 93: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 93: train_loss=1.3034, val_loss=4.9889\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 2.9005 - mae: 0.3757 - val_loss: 2.7038 - val_mae: 0.3464\n",
      "Epoch 94/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.8006 - mae: 0.3726\n",
      "Epoch 94: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 94: train_loss=1.5202, val_loss=1.7161\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 2.8006 - mae: 0.3726 - val_loss: 2.4614 - val_mae: 0.3434\n",
      "Epoch 95/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.7918 - mae: 0.3723\n",
      "Epoch 95: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 95: train_loss=1.4562, val_loss=4.3235\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 2.7918 - mae: 0.3723 - val_loss: 2.6082 - val_mae: 0.3367\n",
      "Epoch 96/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.0752 - mae: 0.3772\n",
      "Epoch 96: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 96: train_loss=1.3309, val_loss=2.6889\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 3.0752 - mae: 0.3772 - val_loss: 2.5065 - val_mae: 0.3296\n",
      "Epoch 97/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.7750 - mae: 0.3672\n",
      "Epoch 97: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 97: train_loss=1.2326, val_loss=1.3312\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 2.7750 - mae: 0.3672 - val_loss: 2.6516 - val_mae: 0.3190\n",
      "Epoch 98/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 7.2967 - mae: 0.4279\n",
      "Epoch 98: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 98: train_loss=1.5377, val_loss=1.9413\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 7.2967 - mae: 0.4279 - val_loss: 4.4301 - val_mae: 0.3588\n",
      "Epoch 99/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.0964 - mae: 0.3778\n",
      "Epoch 99: val_loss did not improve from 2.41544\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 99: train_loss=1.3611, val_loss=2.2256\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 3.0964 - mae: 0.3778 - val_loss: 2.5679 - val_mae: 0.3298\n",
      "Epoch 100/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.6768 - mae: 0.3618\n",
      "Epoch 100: val_loss improved from 2.41544 to 2.35451, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 100: train_loss=0.9959, val_loss=1.3523\n",
      "134/134 [==============================] - 19s 144ms/step - loss: 2.6768 - mae: 0.3618 - val_loss: 2.3545 - val_mae: 0.3136\n",
      "Epoch 101/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.6788 - mae: 0.3586\n",
      "Epoch 101: val_loss improved from 2.35451 to 2.32050, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch 101: train_loss=1.3734, val_loss=1.9843\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 2.6788 - mae: 0.3586 - val_loss: 2.3205 - val_mae: 0.3083\n",
      "Epoch 102/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5730 - mae: 0.3527\n",
      "Epoch 102: val_loss did not improve from 2.32050\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 102: train_loss=1.2530, val_loss=0.9670\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 2.5730 - mae: 0.3527 - val_loss: 2.4681 - val_mae: 0.3043\n",
      "Epoch 103/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5626 - mae: 0.3489\n",
      "Epoch 103: val_loss improved from 2.32050 to 2.24816, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 103: train_loss=1.0014, val_loss=1.4728\n",
      "134/134 [==============================] - 19s 137ms/step - loss: 2.5626 - mae: 0.3489 - val_loss: 2.2482 - val_mae: 0.2989\n",
      "Epoch 104/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5239 - mae: 0.3473\n",
      "Epoch 104: val_loss did not improve from 2.24816\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 104: train_loss=1.0525, val_loss=1.4704\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 2.5239 - mae: 0.3473 - val_loss: 2.3736 - val_mae: 0.3001\n",
      "Epoch 105/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5676 - mae: 0.3444\n",
      "Epoch 105: val_loss did not improve from 2.24816\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 105: train_loss=1.1820, val_loss=1.5181\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 2.5676 - mae: 0.3444 - val_loss: 2.5184 - val_mae: 0.2966\n",
      "Epoch 106/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.6805 - mae: 0.3441\n",
      "Epoch 106: val_loss improved from 2.24816 to 2.09977, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 106: train_loss=1.3734, val_loss=0.9562\n",
      "134/134 [==============================] - 19s 139ms/step - loss: 2.6805 - mae: 0.3441 - val_loss: 2.0998 - val_mae: 0.2884\n",
      "Epoch 107/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.3919 - mae: 0.3351\n",
      "Epoch 107: val_loss did not improve from 2.09977\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 107: train_loss=1.3927, val_loss=1.4245\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 2.3919 - mae: 0.3351 - val_loss: 2.5776 - val_mae: 0.3070\n",
      "Epoch 108/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.4352 - mae: 0.3359\n",
      "Epoch 108: val_loss improved from 2.09977 to 2.06554, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 108: train_loss=0.9764, val_loss=1.8609\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 2.4352 - mae: 0.3359 - val_loss: 2.0655 - val_mae: 0.2862\n",
      "Epoch 109/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5236 - mae: 0.3342\n",
      "Epoch 109: val_loss did not improve from 2.06554\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 109: train_loss=1.0046, val_loss=10.6951\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 2.5236 - mae: 0.3342 - val_loss: 2.1168 - val_mae: 0.2833\n",
      "Epoch 110/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.3397 - mae: 0.3298\n",
      "Epoch 110: val_loss improved from 2.06554 to 2.03589, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 110: train_loss=1.2533, val_loss=1.9286\n",
      "134/134 [==============================] - 18s 137ms/step - loss: 2.3397 - mae: 0.3298 - val_loss: 2.0359 - val_mae: 0.2761\n",
      "Epoch 111/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.6675 - mae: 0.3342\n",
      "Epoch 111: val_loss did not improve from 2.03589\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 111: train_loss=1.4833, val_loss=2.3265\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 2.6675 - mae: 0.3342 - val_loss: 2.3753 - val_mae: 0.2847\n",
      "Epoch 112/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.3305 - mae: 0.3251\n",
      "Epoch 112: val_loss improved from 2.03589 to 1.80861, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 112: train_loss=0.9326, val_loss=0.9966\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 2.3305 - mae: 0.3251 - val_loss: 1.8086 - val_mae: 0.2761\n",
      "Epoch 113/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.3042 - mae: 0.3240\n",
      "Epoch 113: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 113: train_loss=1.1597, val_loss=2.7551\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 2.3042 - mae: 0.3240 - val_loss: 2.1162 - val_mae: 0.2741\n",
      "Epoch 114/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.2710 - mae: 0.3219\n",
      "Epoch 114: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Epoch 114: train_loss=1.5292, val_loss=8.1135\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 2.2710 - mae: 0.3219 - val_loss: 3.2489 - val_mae: 0.2931\n",
      "Epoch 115/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.8131 - mae: 0.3335\n",
      "Epoch 115: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Epoch 115: train_loss=0.8413, val_loss=1.6418\n",
      "134/134 [==============================] - 19s 137ms/step - loss: 2.8131 - mae: 0.3335 - val_loss: 1.9013 - val_mae: 0.2729\n",
      "Epoch 116/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.2498 - mae: 0.3188\n",
      "Epoch 116: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 116: train_loss=1.4415, val_loss=1.9488\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 2.2498 - mae: 0.3188 - val_loss: 2.2582 - val_mae: 0.2652\n",
      "Epoch 117/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.4598 - mae: 0.3152\n",
      "Epoch 117: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 117: train_loss=1.9612, val_loss=6.4174\n",
      "134/134 [==============================] - 19s 143ms/step - loss: 2.4598 - mae: 0.3152 - val_loss: 4.5501 - val_mae: 0.2967\n",
      "Epoch 118/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.6272 - mae: 0.3269\n",
      "Epoch 118: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 118: train_loss=1.0335, val_loss=1.5909\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 2.6272 - mae: 0.3269 - val_loss: 3.0203 - val_mae: 0.2946\n",
      "Epoch 119/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.5802 - mae: 0.3384\n",
      "Epoch 119: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 119: train_loss=2.3113, val_loss=1.9889\n",
      "134/134 [==============================] - 18s 132ms/step - loss: 3.5802 - mae: 0.3384 - val_loss: 2.5282 - val_mae: 0.2843\n",
      "Epoch 120/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.4807 - mae: 0.3211\n",
      "Epoch 120: val_loss did not improve from 1.80861\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 120: train_loss=1.1075, val_loss=1.8397\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 2.4807 - mae: 0.3211 - val_loss: 2.2863 - val_mae: 0.2713\n",
      "Epoch 121/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.2258 - mae: 0.3113\n",
      "Epoch 121: val_loss improved from 1.80861 to 1.63870, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 121: train_loss=0.8112, val_loss=1.2373\n",
      "134/134 [==============================] - 19s 139ms/step - loss: 2.2258 - mae: 0.3113 - val_loss: 1.6387 - val_mae: 0.2494\n",
      "Epoch 122/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1962 - mae: 0.3115\n",
      "Epoch 122: val_loss improved from 1.63870 to 1.58780, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Epoch 122: train_loss=0.8376, val_loss=1.4679\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 2.1962 - mae: 0.3115 - val_loss: 1.5878 - val_mae: 0.2502\n",
      "Epoch 123/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1234 - mae: 0.3045\n",
      "Epoch 123: val_loss did not improve from 1.58780\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 123: train_loss=0.8087, val_loss=1.6892\n",
      "134/134 [==============================] - 19s 143ms/step - loss: 2.1234 - mae: 0.3045 - val_loss: 2.0141 - val_mae: 0.2480\n",
      "Epoch 124/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1758 - mae: 0.3054\n",
      "Epoch 124: val_loss did not improve from 1.58780\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 124: train_loss=0.8437, val_loss=1.1640\n",
      "134/134 [==============================] - 19s 141ms/step - loss: 2.1758 - mae: 0.3054 - val_loss: 1.7532 - val_mae: 0.2476\n",
      "Epoch 125/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.1827 - mae: 0.3177\n",
      "Epoch 125: val_loss did not improve from 1.58780\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 125: train_loss=4.1707, val_loss=13.3599\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 3.1827 - mae: 0.3177 - val_loss: 6.0972 - val_mae: 0.3180\n",
      "Epoch 126/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 4.3270 - mae: 0.3479\n",
      "Epoch 126: val_loss did not improve from 1.58780\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 126: train_loss=1.0064, val_loss=1.3497\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 4.3270 - mae: 0.3479 - val_loss: 1.8541 - val_mae: 0.2553\n",
      "Epoch 127/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1252 - mae: 0.3018\n",
      "Epoch 127: val_loss did not improve from 1.58780\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 127: train_loss=0.8655, val_loss=8.3146\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 2.1252 - mae: 0.3018 - val_loss: 1.8444 - val_mae: 0.2426\n",
      "Epoch 128/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.0902 - mae: 0.2946\n",
      "Epoch 128: val_loss improved from 1.58780 to 1.57263, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 128: train_loss=0.9126, val_loss=1.1060\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 2.0902 - mae: 0.2946 - val_loss: 1.5726 - val_mae: 0.2379\n",
      "Epoch 129/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.0532 - mae: 0.2968\n",
      "Epoch 129: val_loss improved from 1.57263 to 1.48040, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 129: train_loss=0.6804, val_loss=6.7410\n",
      "134/134 [==============================] - 19s 137ms/step - loss: 2.0532 - mae: 0.2968 - val_loss: 1.4804 - val_mae: 0.2311\n",
      "Epoch 130/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1271 - mae: 0.2998\n",
      "Epoch 130: val_loss improved from 1.48040 to 1.45608, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 130: train_loss=0.5981, val_loss=1.8792\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 2.1271 - mae: 0.2998 - val_loss: 1.4561 - val_mae: 0.2295\n",
      "Epoch 131/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.9448 - mae: 0.2897\n",
      "Epoch 131: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 131: train_loss=1.4886, val_loss=0.8240\n",
      "134/134 [==============================] - 18s 136ms/step - loss: 1.9448 - mae: 0.2897 - val_loss: 2.2053 - val_mae: 0.2379\n",
      "Epoch 132/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.3525 - mae: 0.3119\n",
      "Epoch 132: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 132: train_loss=0.9345, val_loss=1.2256\n",
      "134/134 [==============================] - 18s 134ms/step - loss: 3.3525 - mae: 0.3119 - val_loss: 1.6954 - val_mae: 0.2436\n",
      "Epoch 133/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.0185 - mae: 0.2907\n",
      "Epoch 133: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 133: train_loss=0.6528, val_loss=1.2288\n",
      "134/134 [==============================] - 19s 142ms/step - loss: 2.0185 - mae: 0.2907 - val_loss: 1.8377 - val_mae: 0.2310\n",
      "Epoch 134/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.0158 - mae: 0.2873\n",
      "Epoch 134: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 134: train_loss=0.7926, val_loss=0.7976\n",
      "134/134 [==============================] - 19s 138ms/step - loss: 2.0158 - mae: 0.2873 - val_loss: 1.5032 - val_mae: 0.2171\n",
      "Epoch 135/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.9070 - mae: 0.2845\n",
      "Epoch 135: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 135: train_loss=0.7614, val_loss=1.5409\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 1.9070 - mae: 0.2845 - val_loss: 1.6720 - val_mae: 0.2248\n",
      "Epoch 136/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8957 - mae: 0.2840\n",
      "Epoch 136: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 136: train_loss=0.6732, val_loss=0.6523\n",
      "134/134 [==============================] - 19s 139ms/step - loss: 1.8957 - mae: 0.2840 - val_loss: 1.6405 - val_mae: 0.2154\n",
      "Epoch 137/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.9428 - mae: 0.2821\n",
      "Epoch 137: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 137: train_loss=0.7325, val_loss=0.9891\n",
      "134/134 [==============================] - 19s 140ms/step - loss: 1.9428 - mae: 0.2821 - val_loss: 1.5271 - val_mae: 0.2172\n",
      "Epoch 138/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7841 - mae: 0.2757\n",
      "Epoch 138: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 138: train_loss=0.7506, val_loss=0.7502\n",
      "134/134 [==============================] - 18s 135ms/step - loss: 1.7841 - mae: 0.2757 - val_loss: 1.5345 - val_mae: 0.2137\n",
      "Epoch 139/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8231 - mae: 0.2757\n",
      "Epoch 139: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 139: train_loss=1.6709, val_loss=2.0547\n",
      "134/134 [==============================] - 19s 143ms/step - loss: 1.8231 - mae: 0.2757 - val_loss: 2.9131 - val_mae: 0.2684\n",
      "Epoch 140/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1445 - mae: 0.2895\n",
      "Epoch 140: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 140: train_loss=0.7792, val_loss=0.7923\n",
      "134/134 [==============================] - 19s 139ms/step - loss: 2.1445 - mae: 0.2895 - val_loss: 1.8935 - val_mae: 0.2195\n",
      "Epoch 141/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8330 - mae: 0.2724\n",
      "Epoch 141: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 141: train_loss=0.6806, val_loss=0.7173\n",
      "134/134 [==============================] - 19s 139ms/step - loss: 1.8330 - mae: 0.2724 - val_loss: 1.8034 - val_mae: 0.2148\n",
      "Epoch 142/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8804 - mae: 0.2740\n",
      "Epoch 142: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 142: train_loss=0.5980, val_loss=1.2822\n",
      "134/134 [==============================] - 13s 93ms/step - loss: 1.8804 - mae: 0.2740 - val_loss: 1.7246 - val_mae: 0.2198\n",
      "Epoch 143/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.1274 - mae: 0.2817\n",
      "Epoch 143: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 143: train_loss=0.5750, val_loss=5.6590\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 2.1274 - mae: 0.2817 - val_loss: 1.7999 - val_mae: 0.2191\n",
      "Epoch 144/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8729 - mae: 0.2716\n",
      "Epoch 144: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 144: train_loss=0.5837, val_loss=1.2162\n",
      "134/134 [==============================] - 10s 74ms/step - loss: 1.8729 - mae: 0.2716 - val_loss: 1.5423 - val_mae: 0.2014\n",
      "Epoch 145/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7772 - mae: 0.2693\n",
      "Epoch 145: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 145: train_loss=0.7441, val_loss=1.2430\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.7772 - mae: 0.2693 - val_loss: 1.7450 - val_mae: 0.2090\n",
      "Epoch 146/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8047 - mae: 0.2668\n",
      "Epoch 146: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 146: train_loss=0.6735, val_loss=0.7937\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.8047 - mae: 0.2668 - val_loss: 1.5181 - val_mae: 0.2137\n",
      "Epoch 147/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.1035 - mae: 0.2875\n",
      "Epoch 147: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 147: train_loss=5.5711, val_loss=18.0192\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 3.1035 - mae: 0.2875 - val_loss: 15.2771 - val_mae: 0.3495\n",
      "Epoch 148/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.8744 - mae: 0.3015\n",
      "Epoch 148: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 148: train_loss=0.5741, val_loss=1.5664\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 2.8744 - mae: 0.3015 - val_loss: 1.6232 - val_mae: 0.2120\n",
      "Epoch 149/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7741 - mae: 0.2698\n",
      "Epoch 149: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 149: train_loss=0.6941, val_loss=1.1832\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.7741 - mae: 0.2698 - val_loss: 1.6887 - val_mae: 0.2070\n",
      "Epoch 150/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7226 - mae: 0.2629\n",
      "Epoch 150: val_loss did not improve from 1.45608\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 150: train_loss=0.4918, val_loss=1.1553\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.7226 - mae: 0.2629 - val_loss: 1.4593 - val_mae: 0.1924\n",
      "Epoch 151/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7654 - mae: 0.2628\n",
      "Epoch 151: val_loss improved from 1.45608 to 1.19889, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 151: train_loss=0.5181, val_loss=0.8781\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.7654 - mae: 0.2628 - val_loss: 1.1989 - val_mae: 0.1864\n",
      "Epoch 152/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6776 - mae: 0.2617\n",
      "Epoch 152: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 152: train_loss=0.9731, val_loss=0.7206\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.6776 - mae: 0.2617 - val_loss: 1.3314 - val_mae: 0.1886\n",
      "Epoch 153/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.6332 - mae: 0.2940\n",
      "Epoch 153: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 153: train_loss=2.6722, val_loss=1.5740\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 3.6332 - mae: 0.2940 - val_loss: 2.5511 - val_mae: 0.2308\n",
      "Epoch 154/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.8770 - mae: 0.2677\n",
      "Epoch 154: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 154: train_loss=0.5771, val_loss=1.0069\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.8770 - mae: 0.2677 - val_loss: 1.2748 - val_mae: 0.1950\n",
      "Epoch 155/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7091 - mae: 0.2589\n",
      "Epoch 155: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 155: train_loss=0.5580, val_loss=0.8594\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.7091 - mae: 0.2589 - val_loss: 1.3147 - val_mae: 0.1896\n",
      "Epoch 156/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6482 - mae: 0.2539\n",
      "Epoch 156: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 156: train_loss=0.6643, val_loss=1.8143\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.6482 - mae: 0.2539 - val_loss: 1.5479 - val_mae: 0.1921\n",
      "Epoch 157/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.5566 - mae: 0.2746\n",
      "Epoch 157: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 157: train_loss=1.0074, val_loss=1.5786\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 2.5566 - mae: 0.2746 - val_loss: 1.7831 - val_mae: 0.2224\n",
      "Epoch 158/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.9875 - mae: 0.2690\n",
      "Epoch 158: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 158: train_loss=0.6056, val_loss=0.8415\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.9875 - mae: 0.2690 - val_loss: 1.3705 - val_mae: 0.1890\n",
      "Epoch 159/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6634 - mae: 0.2532\n",
      "Epoch 159: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 159: train_loss=0.3420, val_loss=0.8672\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6634 - mae: 0.2532 - val_loss: 1.2732 - val_mae: 0.1782\n",
      "Epoch 160/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7027 - mae: 0.2531\n",
      "Epoch 160: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 160: train_loss=0.6930, val_loss=1.0534\n",
      "134/134 [==============================] - 10s 74ms/step - loss: 1.7027 - mae: 0.2531 - val_loss: 1.4750 - val_mae: 0.1850\n",
      "Epoch 161/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6377 - mae: 0.2496\n",
      "Epoch 161: val_loss did not improve from 1.19889\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 161: train_loss=0.4410, val_loss=4.4323\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6377 - mae: 0.2496 - val_loss: 1.2122 - val_mae: 0.1802\n",
      "Epoch 162/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6013 - mae: 0.2502\n",
      "Epoch 162: val_loss improved from 1.19889 to 1.11971, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 162: train_loss=0.4221, val_loss=0.7789\n",
      "134/134 [==============================] - 10s 75ms/step - loss: 1.6013 - mae: 0.2502 - val_loss: 1.1197 - val_mae: 0.1808\n",
      "Epoch 163/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6819 - mae: 0.2538\n",
      "Epoch 163: val_loss did not improve from 1.11971\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 163: train_loss=1.8454, val_loss=9.4080\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6819 - mae: 0.2538 - val_loss: 1.9736 - val_mae: 0.1987\n",
      "Epoch 164/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7166 - mae: 0.2531\n",
      "Epoch 164: val_loss improved from 1.11971 to 1.11726, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 164: train_loss=0.4368, val_loss=1.0862\n",
      "134/134 [==============================] - 10s 75ms/step - loss: 1.7166 - mae: 0.2531 - val_loss: 1.1173 - val_mae: 0.1817\n",
      "Epoch 165/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6325 - mae: 0.2489\n",
      "Epoch 165: val_loss did not improve from 1.11726\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 165: train_loss=0.3655, val_loss=0.5235\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6325 - mae: 0.2489 - val_loss: 1.2684 - val_mae: 0.1798\n",
      "Epoch 166/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5978 - mae: 0.2470\n",
      "Epoch 166: val_loss did not improve from 1.11726\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 166: train_loss=0.5492, val_loss=1.2050\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5978 - mae: 0.2470 - val_loss: 1.1911 - val_mae: 0.1847\n",
      "Epoch 167/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5925 - mae: 0.2476\n",
      "Epoch 167: val_loss improved from 1.11726 to 1.06230, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 167: train_loss=0.3765, val_loss=1.1691\n",
      "134/134 [==============================] - 10s 76ms/step - loss: 1.5925 - mae: 0.2476 - val_loss: 1.0623 - val_mae: 0.1728\n",
      "Epoch 168/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6214 - mae: 0.2464\n",
      "Epoch 168: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 168: train_loss=0.4524, val_loss=0.6755\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6214 - mae: 0.2464 - val_loss: 1.2817 - val_mae: 0.1749\n",
      "Epoch 169/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6243 - mae: 0.2449\n",
      "Epoch 169: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 169: train_loss=0.5669, val_loss=0.7704\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.6243 - mae: 0.2449 - val_loss: 1.3120 - val_mae: 0.1747\n",
      "Epoch 170/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5535 - mae: 0.2449\n",
      "Epoch 170: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 170: train_loss=0.4250, val_loss=0.5758\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5535 - mae: 0.2449 - val_loss: 1.2190 - val_mae: 0.1786\n",
      "Epoch 171/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.1704 - mae: 0.2629\n",
      "Epoch 171: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 171: train_loss=3.2488, val_loss=6.8915\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 3.1704 - mae: 0.2629 - val_loss: 13.6448 - val_mae: 0.3307\n",
      "Epoch 172/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 3.4559 - mae: 0.2845\n",
      "Epoch 172: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 172: train_loss=0.5957, val_loss=0.7229\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 3.4559 - mae: 0.2845 - val_loss: 1.5090 - val_mae: 0.1794\n",
      "Epoch 173/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.7471 - mae: 0.2485\n",
      "Epoch 173: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 173: train_loss=0.6154, val_loss=4.3962\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.7471 - mae: 0.2485 - val_loss: 1.8039 - val_mae: 0.1768\n",
      "Epoch 174/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.6093 - mae: 0.2434\n",
      "Epoch 174: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 174: train_loss=0.3888, val_loss=0.5020\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.6093 - mae: 0.2434 - val_loss: 1.1857 - val_mae: 0.1704\n",
      "Epoch 175/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5713 - mae: 0.2402\n",
      "Epoch 175: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 175: train_loss=0.4401, val_loss=0.6038\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5713 - mae: 0.2402 - val_loss: 1.2719 - val_mae: 0.1723\n",
      "Epoch 176/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5846 - mae: 0.2389\n",
      "Epoch 176: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 176: train_loss=0.3939, val_loss=0.9241\n",
      "134/134 [==============================] - 10s 74ms/step - loss: 1.5846 - mae: 0.2389 - val_loss: 1.1784 - val_mae: 0.1599\n",
      "Epoch 177/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5322 - mae: 0.2384\n",
      "Epoch 177: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 177: train_loss=0.5104, val_loss=9.5215\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5322 - mae: 0.2384 - val_loss: 1.1739 - val_mae: 0.1643\n",
      "Epoch 178/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5473 - mae: 0.2395\n",
      "Epoch 178: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 178: train_loss=0.2474, val_loss=0.7491\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5473 - mae: 0.2395 - val_loss: 1.2014 - val_mae: 0.1618\n",
      "Epoch 179/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4813 - mae: 0.2335\n",
      "Epoch 179: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 179: train_loss=0.5925, val_loss=0.7138\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.4813 - mae: 0.2335 - val_loss: 1.2254 - val_mae: 0.1723\n",
      "Epoch 180/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5574 - mae: 0.2366\n",
      "Epoch 180: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 180: train_loss=0.4334, val_loss=0.5081\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.5574 - mae: 0.2366 - val_loss: 1.1149 - val_mae: 0.1633\n",
      "Epoch 181/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4685 - mae: 0.2328\n",
      "Epoch 181: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 181: train_loss=0.3640, val_loss=1.1228\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.4685 - mae: 0.2328 - val_loss: 1.0971 - val_mae: 0.1601\n",
      "Epoch 182/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4799 - mae: 0.2315\n",
      "Epoch 182: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 182: train_loss=0.3951, val_loss=1.4358\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.4799 - mae: 0.2315 - val_loss: 1.4761 - val_mae: 0.1614\n",
      "Epoch 183/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5394 - mae: 0.2353\n",
      "Epoch 183: val_loss did not improve from 1.06230\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 183: train_loss=0.4557, val_loss=0.8468\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.5394 - mae: 0.2353 - val_loss: 1.1794 - val_mae: 0.1827\n",
      "Epoch 184/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4915 - mae: 0.2354\n",
      "Epoch 184: val_loss improved from 1.06230 to 1.05735, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 184: train_loss=0.3398, val_loss=0.7840\n",
      "134/134 [==============================] - 10s 75ms/step - loss: 1.4915 - mae: 0.2354 - val_loss: 1.0574 - val_mae: 0.1522\n",
      "Epoch 185/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4481 - mae: 0.2331\n",
      "Epoch 185: val_loss improved from 1.05735 to 0.95135, saving model to best_CNN_FIX_model.h5\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 185: train_loss=0.3438, val_loss=0.7846\n",
      "134/134 [==============================] - 10s 76ms/step - loss: 1.4481 - mae: 0.2331 - val_loss: 0.9513 - val_mae: 0.1529\n",
      "Epoch 186/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4571 - mae: 0.2329\n",
      "Epoch 186: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 186: train_loss=0.5402, val_loss=1.0693\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.4571 - mae: 0.2329 - val_loss: 1.5554 - val_mae: 0.1981\n",
      "Epoch 187/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 5.4870 - mae: 0.2989\n",
      "Epoch 187: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 187: train_loss=1.1872, val_loss=2.1806\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 5.4870 - mae: 0.2989 - val_loss: 3.4372 - val_mae: 0.2338\n",
      "Epoch 188/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 2.2102 - mae: 0.2610\n",
      "Epoch 188: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 188: train_loss=0.5004, val_loss=1.0709\n",
      "134/134 [==============================] - 9s 71ms/step - loss: 2.2102 - mae: 0.2610 - val_loss: 2.0187 - val_mae: 0.1705\n",
      "Epoch 189/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5615 - mae: 0.2364\n",
      "Epoch 189: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 189: train_loss=0.4091, val_loss=0.6766\n",
      "134/134 [==============================] - 9s 71ms/step - loss: 1.5615 - mae: 0.2364 - val_loss: 1.5427 - val_mae: 0.1618\n",
      "Epoch 190/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5037 - mae: 0.2328\n",
      "Epoch 190: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 190: train_loss=0.3434, val_loss=2.7819\n",
      "134/134 [==============================] - 9s 70ms/step - loss: 1.5037 - mae: 0.2328 - val_loss: 1.4751 - val_mae: 0.1668\n",
      "Epoch 191/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5091 - mae: 0.2324\n",
      "Epoch 191: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 191: train_loss=0.5082, val_loss=0.9873\n",
      "134/134 [==============================] - 9s 70ms/step - loss: 1.5091 - mae: 0.2324 - val_loss: 1.3471 - val_mae: 0.1581\n",
      "Epoch 192/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4327 - mae: 0.2288\n",
      "Epoch 192: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 192: train_loss=0.3102, val_loss=0.6427\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.4327 - mae: 0.2288 - val_loss: 1.3837 - val_mae: 0.1500\n",
      "Epoch 193/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4039 - mae: 0.2266\n",
      "Epoch 193: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 193: train_loss=0.4912, val_loss=0.7995\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.4039 - mae: 0.2266 - val_loss: 1.4598 - val_mae: 0.1616\n",
      "Epoch 194/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4454 - mae: 0.2282\n",
      "Epoch 194: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 194: train_loss=0.3094, val_loss=0.7250\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.4454 - mae: 0.2282 - val_loss: 1.3244 - val_mae: 0.1544\n",
      "Epoch 195/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4281 - mae: 0.2256\n",
      "Epoch 195: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 195: train_loss=0.5684, val_loss=1.3244\n",
      "134/134 [==============================] - 10s 73ms/step - loss: 1.4281 - mae: 0.2256 - val_loss: 1.2184 - val_mae: 0.1481\n",
      "Epoch 196/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4176 - mae: 0.2245\n",
      "Epoch 196: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 196: train_loss=0.5036, val_loss=0.7500\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.4176 - mae: 0.2245 - val_loss: 1.6044 - val_mae: 0.1635\n",
      "Epoch 197/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.5022 - mae: 0.2272\n",
      "Epoch 197: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 197: train_loss=0.2898, val_loss=5.4462\n",
      "134/134 [==============================] - 10s 72ms/step - loss: 1.5022 - mae: 0.2272 - val_loss: 1.4010 - val_mae: 0.1481\n",
      "Epoch 198/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.4201 - mae: 0.2266\n",
      "Epoch 198: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 198: train_loss=0.4208, val_loss=0.7154\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.4201 - mae: 0.2266 - val_loss: 1.2020 - val_mae: 0.1592\n",
      "Epoch 199/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.3998 - mae: 0.2259\n",
      "Epoch 199: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 199: train_loss=0.3534, val_loss=0.4397\n",
      "134/134 [==============================] - 9s 70ms/step - loss: 1.3998 - mae: 0.2259 - val_loss: 1.2173 - val_mae: 0.1497\n",
      "Epoch 200/200\n",
      "134/134 [==============================] - ETA: 0s - loss: 1.3892 - mae: 0.2240\n",
      "Epoch 200: val_loss did not improve from 0.95135\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 200: train_loss=1.0681, val_loss=0.7964\n",
      "134/134 [==============================] - 10s 71ms/step - loss: 1.3892 - mae: 0.2240 - val_loss: 1.2904 - val_mae: 0.1494\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# ----------- GPU Configuration -----------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# ----------- 1. Load training and validation data -----------\n",
    "train_df = pd.read_csv('./csbf3nvm/NVM23_train_split.txt', delimiter=' ',\n",
    "                       names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "val_df = pd.read_csv('./csbf3nvm/NVM23_val_split.txt', delimiter=' ',\n",
    "                     names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ----------- 2. Define custom loss function -----------\n",
    "def custom_loss(y_true, y_pred):\n",
    "    beta = 400\n",
    "    x_pred = y_pred[:, :3]\n",
    "    q_pred = y_pred[:, 3:]\n",
    "    x_true = y_true[:, :3]\n",
    "    q_true = y_true[:, 3:]\n",
    "\n",
    "    q_norm = K.sqrt(K.sum(K.square(q_true), axis=-1, keepdims=True)) + K.epsilon()\n",
    "    q_true_normed = q_true / q_norm\n",
    "\n",
    "    position_loss = K.mean(K.square(x_pred - x_true), axis=-1)\n",
    "    quaternion_loss = K.mean(K.square(q_pred - q_true_normed), axis=-1)\n",
    "\n",
    "    return position_loss + beta * quaternion_loss\n",
    "\n",
    "# ----------- 3. Build CNN model (no pretrained) -----------\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(7)  # Output: [X, Y, Z, W, P, Q, R]\n",
    "])\n",
    "# ----------- 4. Compile model -----------\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=custom_loss,\n",
    "              metrics=['mae'])\n",
    "\n",
    "# ----------- 5. Custom callback (updated to log both train and val loss) -----------\n",
    "class TrainValLossLogger(Callback):\n",
    "    def __init__(self, train_gen, val_gen, output_file='loss_CNN_FIX.csv'):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.output_file = output_file\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        def compute_losses(gen):\n",
    "            x, y_true = next(iter(gen))\n",
    "            y_pred = self.model.predict(x)\n",
    "            x_pred, q_pred = y_pred[:, :3], y_pred[:, 3:]\n",
    "            x_true, q_true = y_true[:, :3], y_true[:, 3:]\n",
    "            q_true_normed = q_true / (np.linalg.norm(q_true, axis=1, keepdims=True) + 1e-7)\n",
    "            pos_loss = np.mean(np.square(x_pred - x_true))\n",
    "            quat_loss = np.mean(np.square(q_pred - q_true_normed))\n",
    "            total_loss = pos_loss + 400. * quat_loss\n",
    "            return total_loss, pos_loss, quat_loss\n",
    "\n",
    "        train_total, train_pos, train_quat = compute_losses(self.train_gen)\n",
    "        val_total, val_pos, val_quat = compute_losses(self.val_gen)\n",
    "\n",
    "        self.logs.append({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Train_loss': train_total,\n",
    "            'Train_pos_loss': train_pos,\n",
    "            'Train_quat_loss': train_quat,\n",
    "            'Val_loss': val_total,\n",
    "            'Val_pos_loss': val_pos,\n",
    "            'Val_quat_loss': val_quat\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(self.logs).to_csv(self.output_file, index=False)\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_total:.4f}, val_loss={val_total:.4f}\")\n",
    "\n",
    "# ----------- 6. Callbacks -----------\n",
    "loss_logger = TrainValLossLogger(train_generator, val_generator)\n",
    "checkpoint = ModelCheckpoint('best_CNN_FIX_model.h5', monitor='val_loss',save_best_only=True,mode='min',verbose=1)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-8, verbose=1)\n",
    "\n",
    "# ----------- 7. Train with validation -----------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=200,\n",
    "    callbacks=[checkpoint,loss_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) detected: ['/physical_device:GPU:0']\n",
      "Found 4262 validated image filenames.\n",
      "Found 1066 validated image filenames.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 7, 7, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              984064    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,508,807\n",
      "Trainable params: 4,484,407\n",
      "Non-trainable params: 24,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 155.1547 - mae: 0.9764\n",
      "Epoch 1: val_loss improved from inf to 48.70741, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 1s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 1: train_loss=53.6434, val_loss=41.4388\n",
      "67/67 [==============================] - 17s 158ms/step - loss: 155.1547 - mae: 0.9764 - val_loss: 48.7074 - val_mae: 0.8297\n",
      "Epoch 2/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 60.6383 - mae: 0.8577\n",
      "Epoch 2: val_loss improved from 48.70741 to 39.91471, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 49ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 2: train_loss=41.6942, val_loss=49.3181\n",
      "67/67 [==============================] - 12s 175ms/step - loss: 60.6383 - mae: 0.8577 - val_loss: 39.9147 - val_mae: 0.8114\n",
      "Epoch 3/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 47.0873 - mae: 0.8335\n",
      "Epoch 3: val_loss improved from 39.91471 to 32.82372, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 3: train_loss=36.5810, val_loss=27.0517\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 47.0873 - mae: 0.8335 - val_loss: 32.8237 - val_mae: 0.7983\n",
      "Epoch 4/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 38.3647 - mae: 0.8182\n",
      "Epoch 4: val_loss improved from 32.82372 to 27.61070, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 4: train_loss=27.8200, val_loss=38.0154\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 38.3647 - mae: 0.8182 - val_loss: 27.6107 - val_mae: 0.7894\n",
      "Epoch 5/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 31.9949 - mae: 0.8083\n",
      "Epoch 5: val_loss improved from 27.61070 to 23.79168, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 5: train_loss=17.8229, val_loss=24.6501\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 31.9949 - mae: 0.8083 - val_loss: 23.7917 - val_mae: 0.7825\n",
      "Epoch 6/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 26.8300 - mae: 0.7972\n",
      "Epoch 6: val_loss improved from 23.79168 to 20.91189, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 6: train_loss=17.9161, val_loss=24.6856\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 26.8300 - mae: 0.7972 - val_loss: 20.9119 - val_mae: 0.7753\n",
      "Epoch 7/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 22.9426 - mae: 0.7893\n",
      "Epoch 7: val_loss improved from 20.91189 to 18.30160, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 7: train_loss=21.7682, val_loss=17.3802\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 22.9426 - mae: 0.7893 - val_loss: 18.3016 - val_mae: 0.7704\n",
      "Epoch 8/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 20.5519 - mae: 0.7850\n",
      "Epoch 8: val_loss improved from 18.30160 to 16.75568, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 8: train_loss=13.6039, val_loss=19.4306\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 20.5519 - mae: 0.7850 - val_loss: 16.7557 - val_mae: 0.7655\n",
      "Epoch 9/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 18.9933 - mae: 0.7804\n",
      "Epoch 9: val_loss improved from 16.75568 to 16.24113, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 9: train_loss=9.4681, val_loss=13.5695\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 18.9933 - mae: 0.7804 - val_loss: 16.2411 - val_mae: 0.7653\n",
      "Epoch 10/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 17.3313 - mae: 0.7759\n",
      "Epoch 10: val_loss improved from 16.24113 to 15.67775, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 10: train_loss=11.1126, val_loss=14.3910\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 17.3313 - mae: 0.7759 - val_loss: 15.6777 - val_mae: 0.7630\n",
      "Epoch 11/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 15.7225 - mae: 0.7705\n",
      "Epoch 11: val_loss improved from 15.67775 to 13.94158, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 11: train_loss=8.8511, val_loss=14.5507\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 15.7225 - mae: 0.7705 - val_loss: 13.9416 - val_mae: 0.7575\n",
      "Epoch 12/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 14.5702 - mae: 0.7675\n",
      "Epoch 12: val_loss improved from 13.94158 to 13.81865, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 35ms/step\n",
      "Epoch 12: train_loss=9.8642, val_loss=13.3424\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 14.5702 - mae: 0.7675 - val_loss: 13.8186 - val_mae: 0.7564\n",
      "Epoch 13/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 13.9358 - mae: 0.7656\n",
      "Epoch 13: val_loss improved from 13.81865 to 12.58946, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 13: train_loss=9.1522, val_loss=12.6316\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 13.9358 - mae: 0.7656 - val_loss: 12.5895 - val_mae: 0.7533\n",
      "Epoch 14/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 13.1806 - mae: 0.7619\n",
      "Epoch 14: val_loss did not improve from 12.58946\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 14: train_loss=8.9173, val_loss=15.3617\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 13.1806 - mae: 0.7619 - val_loss: 12.6661 - val_mae: 0.7535\n",
      "Epoch 15/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 12.5135 - mae: 0.7593\n",
      "Epoch 15: val_loss improved from 12.58946 to 11.73294, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 15: train_loss=7.7494, val_loss=9.0600\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 12.5135 - mae: 0.7593 - val_loss: 11.7329 - val_mae: 0.7492\n",
      "Epoch 16/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 11.9815 - mae: 0.7575\n",
      "Epoch 16: val_loss did not improve from 11.73294\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 16: train_loss=9.4547, val_loss=13.4264\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 11.9815 - mae: 0.7575 - val_loss: 12.0944 - val_mae: 0.7510\n",
      "Epoch 17/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 11.7431 - mae: 0.7562\n",
      "Epoch 17: val_loss improved from 11.73294 to 11.65764, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 1s 48ms/step\n",
      "Epoch 17: train_loss=8.7438, val_loss=12.6346\n",
      "67/67 [==============================] - 12s 172ms/step - loss: 11.7431 - mae: 0.7562 - val_loss: 11.6576 - val_mae: 0.7498\n",
      "Epoch 18/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 11.3125 - mae: 0.7546\n",
      "Epoch 18: val_loss improved from 11.65764 to 11.57823, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 18: train_loss=7.6871, val_loss=13.7064\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 11.3125 - mae: 0.7546 - val_loss: 11.5782 - val_mae: 0.7492\n",
      "Epoch 19/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 10.6812 - mae: 0.7506\n",
      "Epoch 19: val_loss improved from 11.57823 to 10.82679, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 19: train_loss=7.4853, val_loss=10.0774\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 10.6812 - mae: 0.7506 - val_loss: 10.8268 - val_mae: 0.7444\n",
      "Epoch 20/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 10.5181 - mae: 0.7499\n",
      "Epoch 20: val_loss improved from 10.82679 to 10.80813, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 20: train_loss=6.8488, val_loss=10.0326\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 10.5181 - mae: 0.7499 - val_loss: 10.8081 - val_mae: 0.7447\n",
      "Epoch 21/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 10.4453 - mae: 0.7484\n",
      "Epoch 21: val_loss improved from 10.80813 to 10.48983, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 21: train_loss=8.1548, val_loss=17.2415\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 10.4453 - mae: 0.7484 - val_loss: 10.4898 - val_mae: 0.7423\n",
      "Epoch 22/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 10.1610 - mae: 0.7470\n",
      "Epoch 22: val_loss improved from 10.48983 to 9.95895, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 22: train_loss=7.7039, val_loss=10.9447\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 10.1610 - mae: 0.7470 - val_loss: 9.9589 - val_mae: 0.7387\n",
      "Epoch 23/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 10.0016 - mae: 0.7445\n",
      "Epoch 23: val_loss did not improve from 9.95895\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 23: train_loss=5.8792, val_loss=12.4947\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 10.0016 - mae: 0.7445 - val_loss: 10.0922 - val_mae: 0.7398\n",
      "Epoch 24/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 9.7033 - mae: 0.7439\n",
      "Epoch 24: val_loss improved from 9.95895 to 9.75790, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 24: train_loss=6.8818, val_loss=10.7949\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 9.7033 - mae: 0.7439 - val_loss: 9.7579 - val_mae: 0.7370\n",
      "Epoch 25/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 9.6630 - mae: 0.7429\n",
      "Epoch 25: val_loss did not improve from 9.75790\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 25: train_loss=6.9025, val_loss=11.6158\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 9.6630 - mae: 0.7429 - val_loss: 10.1237 - val_mae: 0.7388\n",
      "Epoch 26/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 9.3549 - mae: 0.7393\n",
      "Epoch 26: val_loss did not improve from 9.75790\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 26: train_loss=7.1350, val_loss=9.9028\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 9.3549 - mae: 0.7393 - val_loss: 9.8061 - val_mae: 0.7365\n",
      "Epoch 27/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 9.3056 - mae: 0.7390\n",
      "Epoch 27: val_loss improved from 9.75790 to 9.56911, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 27: train_loss=7.6431, val_loss=9.3556\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 9.3056 - mae: 0.7390 - val_loss: 9.5691 - val_mae: 0.7355\n",
      "Epoch 28/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.9430 - mae: 0.7364\n",
      "Epoch 28: val_loss improved from 9.56911 to 9.47005, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 28: train_loss=7.9158, val_loss=9.5767\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.9430 - mae: 0.7364 - val_loss: 9.4700 - val_mae: 0.7331\n",
      "Epoch 29/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.8439 - mae: 0.7357\n",
      "Epoch 29: val_loss improved from 9.47005 to 9.19712, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 29: train_loss=6.7274, val_loss=8.9959\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 8.8439 - mae: 0.7357 - val_loss: 9.1971 - val_mae: 0.7312\n",
      "Epoch 30/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.7051 - mae: 0.7333\n",
      "Epoch 30: val_loss improved from 9.19712 to 9.09114, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 30: train_loss=6.0605, val_loss=9.4204\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 8.7051 - mae: 0.7333 - val_loss: 9.0911 - val_mae: 0.7285\n",
      "Epoch 31/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.9814 - mae: 0.7328\n",
      "Epoch 31: val_loss did not improve from 9.09114\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 31: train_loss=8.2649, val_loss=12.1527\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 8.9814 - mae: 0.7328 - val_loss: 9.3726 - val_mae: 0.7292\n",
      "Epoch 32/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.4672 - mae: 0.7294\n",
      "Epoch 32: val_loss improved from 9.09114 to 9.04795, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 32: train_loss=6.5151, val_loss=7.5581\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 8.4672 - mae: 0.7294 - val_loss: 9.0480 - val_mae: 0.7254\n",
      "Epoch 33/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.3654 - mae: 0.7269\n",
      "Epoch 33: val_loss improved from 9.04795 to 8.77780, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 33: train_loss=6.4398, val_loss=9.1611\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 8.3654 - mae: 0.7269 - val_loss: 8.7778 - val_mae: 0.7227\n",
      "Epoch 34/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.3218 - mae: 0.7249\n",
      "Epoch 34: val_loss did not improve from 8.77780\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 34: train_loss=8.1041, val_loss=10.4532\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 8.3218 - mae: 0.7249 - val_loss: 9.0894 - val_mae: 0.7227\n",
      "Epoch 35/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.1478 - mae: 0.7220\n",
      "Epoch 35: val_loss improved from 8.77780 to 8.52081, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 35: train_loss=5.1969, val_loss=9.8363\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 8.1478 - mae: 0.7220 - val_loss: 8.5208 - val_mae: 0.7169\n",
      "Epoch 36/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 8.1199 - mae: 0.7203\n",
      "Epoch 36: val_loss did not improve from 8.52081\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 36: train_loss=7.8527, val_loss=7.0133\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 8.1199 - mae: 0.7203 - val_loss: 8.6783 - val_mae: 0.7167\n",
      "Epoch 37/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.9598 - mae: 0.7167\n",
      "Epoch 37: val_loss improved from 8.52081 to 8.24456, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 37: train_loss=6.0088, val_loss=7.8682\n",
      "67/67 [==============================] - 11s 170ms/step - loss: 7.9598 - mae: 0.7167 - val_loss: 8.2446 - val_mae: 0.7124\n",
      "Epoch 38/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.8136 - mae: 0.7133\n",
      "Epoch 38: val_loss improved from 8.24456 to 8.04568, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 38: train_loss=5.1315, val_loss=12.9643\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 7.8136 - mae: 0.7133 - val_loss: 8.0457 - val_mae: 0.7073\n",
      "Epoch 39/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.8402 - mae: 0.7112\n",
      "Epoch 39: val_loss improved from 8.04568 to 7.95635, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 39: train_loss=5.1773, val_loss=7.9916\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 7.8402 - mae: 0.7112 - val_loss: 7.9564 - val_mae: 0.7047\n",
      "Epoch 40/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.5486 - mae: 0.7075\n",
      "Epoch 40: val_loss improved from 7.95635 to 7.91308, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 40: train_loss=5.1210, val_loss=10.1145\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 7.5486 - mae: 0.7075 - val_loss: 7.9131 - val_mae: 0.7012\n",
      "Epoch 41/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.5089 - mae: 0.7025\n",
      "Epoch 41: val_loss improved from 7.91308 to 7.80940, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 41: train_loss=4.4396, val_loss=8.1098\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 7.5089 - mae: 0.7025 - val_loss: 7.8094 - val_mae: 0.6961\n",
      "Epoch 42/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.5001 - mae: 0.6975\n",
      "Epoch 42: val_loss did not improve from 7.80940\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 42: train_loss=5.8000, val_loss=8.6641\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 7.5001 - mae: 0.6975 - val_loss: 7.9192 - val_mae: 0.6925\n",
      "Epoch 43/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.3275 - mae: 0.6936\n",
      "Epoch 43: val_loss did not improve from 7.80940\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 43: train_loss=5.4727, val_loss=7.9518\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 7.3275 - mae: 0.6936 - val_loss: 7.8298 - val_mae: 0.6867\n",
      "Epoch 44/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.0697 - mae: 0.6865\n",
      "Epoch 44: val_loss did not improve from 7.80940\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 44: train_loss=5.6823, val_loss=7.8790\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 7.0697 - mae: 0.6865 - val_loss: 7.9154 - val_mae: 0.6831\n",
      "Epoch 45/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 7.0966 - mae: 0.6819\n",
      "Epoch 45: val_loss improved from 7.80940 to 7.68372, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "Epoch 45: train_loss=4.9576, val_loss=7.5718\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 7.0966 - mae: 0.6819 - val_loss: 7.6837 - val_mae: 0.6763\n",
      "Epoch 46/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.9156 - mae: 0.6746\n",
      "Epoch 46: val_loss improved from 7.68372 to 7.60606, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 46: train_loss=5.3050, val_loss=7.0597\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 6.9156 - mae: 0.6746 - val_loss: 7.6061 - val_mae: 0.6668\n",
      "Epoch 47/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.9174 - mae: 0.6666\n",
      "Epoch 47: val_loss improved from 7.60606 to 7.34975, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 47: train_loss=6.1493, val_loss=8.2937\n",
      "67/67 [==============================] - 11s 160ms/step - loss: 6.9174 - mae: 0.6666 - val_loss: 7.3498 - val_mae: 0.6572\n",
      "Epoch 48/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.7411 - mae: 0.6570\n",
      "Epoch 48: val_loss improved from 7.34975 to 7.08112, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 48: train_loss=5.0325, val_loss=10.4214\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 6.7411 - mae: 0.6570 - val_loss: 7.0811 - val_mae: 0.6449\n",
      "Epoch 49/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.6208 - mae: 0.6497\n",
      "Epoch 49: val_loss did not improve from 7.08112\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 49: train_loss=5.4438, val_loss=6.0807\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 6.6208 - mae: 0.6497 - val_loss: 7.1328 - val_mae: 0.6389\n",
      "Epoch 50/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.4742 - mae: 0.6393\n",
      "Epoch 50: val_loss improved from 7.08112 to 6.67909, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "Epoch 50: train_loss=4.1834, val_loss=6.8090\n",
      "67/67 [==============================] - 11s 159ms/step - loss: 6.4742 - mae: 0.6393 - val_loss: 6.6791 - val_mae: 0.6276\n",
      "Epoch 51/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.4543 - mae: 0.6314\n",
      "Epoch 51: val_loss did not improve from 6.67909\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 51: train_loss=4.5446, val_loss=8.9413\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 6.4543 - mae: 0.6314 - val_loss: 7.1096 - val_mae: 0.6207\n",
      "Epoch 52/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.2609 - mae: 0.6222\n",
      "Epoch 52: val_loss did not improve from 6.67909\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 52: train_loss=4.5221, val_loss=6.9286\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 6.2609 - mae: 0.6222 - val_loss: 6.7050 - val_mae: 0.6077\n",
      "Epoch 53/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 6.1249 - mae: 0.6106\n",
      "Epoch 53: val_loss improved from 6.67909 to 6.38664, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 53: train_loss=4.1629, val_loss=5.7690\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 6.1249 - mae: 0.6106 - val_loss: 6.3866 - val_mae: 0.5946\n",
      "Epoch 54/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.8685 - mae: 0.5972\n",
      "Epoch 54: val_loss improved from 6.38664 to 6.16897, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 54: train_loss=3.5241, val_loss=6.1541\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 5.8685 - mae: 0.5972 - val_loss: 6.1690 - val_mae: 0.5844\n",
      "Epoch 55/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.8643 - mae: 0.5897\n",
      "Epoch 55: val_loss did not improve from 6.16897\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 55: train_loss=3.8230, val_loss=12.2910\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 5.8643 - mae: 0.5897 - val_loss: 6.5157 - val_mae: 0.5753\n",
      "Epoch 56/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.6621 - mae: 0.5784\n",
      "Epoch 56: val_loss improved from 6.16897 to 5.98605, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 56: train_loss=4.2388, val_loss=5.3503\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 5.6621 - mae: 0.5784 - val_loss: 5.9861 - val_mae: 0.5611\n",
      "Epoch 57/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.5709 - mae: 0.5678\n",
      "Epoch 57: val_loss did not improve from 5.98605\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 57: train_loss=5.2262, val_loss=9.1497\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 5.5709 - mae: 0.5678 - val_loss: 6.2954 - val_mae: 0.5559\n",
      "Epoch 58/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.4820 - mae: 0.5577\n",
      "Epoch 58: val_loss improved from 5.98605 to 5.67412, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 58: train_loss=4.2905, val_loss=5.6385\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 5.4820 - mae: 0.5577 - val_loss: 5.6741 - val_mae: 0.5310\n",
      "Epoch 59/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.3496 - mae: 0.5428\n",
      "Epoch 59: val_loss improved from 5.67412 to 5.61201, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 59: train_loss=3.9682, val_loss=6.0538\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 5.3496 - mae: 0.5428 - val_loss: 5.6120 - val_mae: 0.5288\n",
      "Epoch 60/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.2452 - mae: 0.5356\n",
      "Epoch 60: val_loss improved from 5.61201 to 5.38603, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 60: train_loss=4.0301, val_loss=5.1087\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 5.2452 - mae: 0.5356 - val_loss: 5.3860 - val_mae: 0.5105\n",
      "Epoch 61/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 5.0820 - mae: 0.5221\n",
      "Epoch 61: val_loss improved from 5.38603 to 5.11506, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 54ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 61: train_loss=3.8671, val_loss=5.4204\n",
      "67/67 [==============================] - 12s 179ms/step - loss: 5.0820 - mae: 0.5221 - val_loss: 5.1151 - val_mae: 0.4953\n",
      "Epoch 62/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.9745 - mae: 0.5095\n",
      "Epoch 62: val_loss improved from 5.11506 to 4.85890, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "Epoch 62: train_loss=3.0057, val_loss=4.1404\n",
      "67/67 [==============================] - 12s 170ms/step - loss: 4.9745 - mae: 0.5095 - val_loss: 4.8589 - val_mae: 0.4846\n",
      "Epoch 63/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.7739 - mae: 0.5004\n",
      "Epoch 63: val_loss did not improve from 4.85890\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 63: train_loss=3.3609, val_loss=5.1993\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 4.7739 - mae: 0.5004 - val_loss: 5.3077 - val_mae: 0.4755\n",
      "Epoch 64/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.7758 - mae: 0.4904\n",
      "Epoch 64: val_loss improved from 4.85890 to 4.60663, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 64: train_loss=2.6178, val_loss=4.8126\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 4.7758 - mae: 0.4904 - val_loss: 4.6066 - val_mae: 0.4614\n",
      "Epoch 65/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.6208 - mae: 0.4769\n",
      "Epoch 65: val_loss did not improve from 4.60663\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 65: train_loss=2.8316, val_loss=6.7722\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 4.6208 - mae: 0.4769 - val_loss: 4.6586 - val_mae: 0.4493\n",
      "Epoch 66/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.3807 - mae: 0.4639\n",
      "Epoch 66: val_loss did not improve from 4.60663\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 66: train_loss=3.5414, val_loss=4.2268\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 4.3807 - mae: 0.4639 - val_loss: 5.0266 - val_mae: 0.4431\n",
      "Epoch 67/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.4069 - mae: 0.4543\n",
      "Epoch 67: val_loss improved from 4.60663 to 4.51970, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 67: train_loss=4.1421, val_loss=4.6670\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 4.4069 - mae: 0.4543 - val_loss: 4.5197 - val_mae: 0.4296\n",
      "Epoch 68/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.1714 - mae: 0.4437\n",
      "Epoch 68: val_loss improved from 4.51970 to 4.46136, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "Epoch 68: train_loss=2.5484, val_loss=5.9102\n",
      "67/67 [==============================] - 11s 160ms/step - loss: 4.1714 - mae: 0.4437 - val_loss: 4.4614 - val_mae: 0.4190\n",
      "Epoch 69/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.1603 - mae: 0.4365\n",
      "Epoch 69: val_loss did not improve from 4.46136\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 69: train_loss=2.6584, val_loss=4.1041\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 4.1603 - mae: 0.4365 - val_loss: 4.6032 - val_mae: 0.4053\n",
      "Epoch 70/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 4.0858 - mae: 0.4292\n",
      "Epoch 70: val_loss did not improve from 4.46136\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 70: train_loss=2.8230, val_loss=4.6813\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 4.0858 - mae: 0.4292 - val_loss: 4.6845 - val_mae: 0.4015\n",
      "Epoch 71/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.9141 - mae: 0.4163\n",
      "Epoch 71: val_loss improved from 4.46136 to 4.24924, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 71: train_loss=1.9660, val_loss=3.6737\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 3.9141 - mae: 0.4163 - val_loss: 4.2492 - val_mae: 0.3877\n",
      "Epoch 72/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.8692 - mae: 0.4104\n",
      "Epoch 72: val_loss improved from 4.24924 to 4.12805, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 72: train_loss=2.2626, val_loss=10.2598\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.8692 - mae: 0.4104 - val_loss: 4.1281 - val_mae: 0.3802\n",
      "Epoch 73/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.7118 - mae: 0.4024\n",
      "Epoch 73: val_loss improved from 4.12805 to 3.87135, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 73: train_loss=2.2101, val_loss=3.5445\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 3.7118 - mae: 0.4024 - val_loss: 3.8714 - val_mae: 0.3588\n",
      "Epoch 74/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.7537 - mae: 0.3939\n",
      "Epoch 74: val_loss did not improve from 3.87135\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 74: train_loss=2.1745, val_loss=6.5681\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 3.7537 - mae: 0.3939 - val_loss: 3.8777 - val_mae: 0.3641\n",
      "Epoch 75/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.6505 - mae: 0.3894\n",
      "Epoch 75: val_loss did not improve from 3.87135\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 75: train_loss=2.4270, val_loss=3.8245\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 3.6505 - mae: 0.3894 - val_loss: 3.9541 - val_mae: 0.3560\n",
      "Epoch 76/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.5147 - mae: 0.3814\n",
      "Epoch 76: val_loss improved from 3.87135 to 3.66932, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 76: train_loss=2.3119, val_loss=4.2928\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.5147 - mae: 0.3814 - val_loss: 3.6693 - val_mae: 0.3337\n",
      "Epoch 77/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.4481 - mae: 0.3770\n",
      "Epoch 77: val_loss did not improve from 3.66932\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 77: train_loss=2.4550, val_loss=3.4382\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 3.4481 - mae: 0.3770 - val_loss: 3.7636 - val_mae: 0.3391\n",
      "Epoch 78/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.4306 - mae: 0.3706\n",
      "Epoch 78: val_loss improved from 3.66932 to 3.46778, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 78: train_loss=2.4498, val_loss=2.9514\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.4306 - mae: 0.3706 - val_loss: 3.4678 - val_mae: 0.3325\n",
      "Epoch 79/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.3956 - mae: 0.3696\n",
      "Epoch 79: val_loss improved from 3.46778 to 3.34479, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 79: train_loss=2.1806, val_loss=2.4484\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 3.3956 - mae: 0.3696 - val_loss: 3.3448 - val_mae: 0.3242\n",
      "Epoch 80/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.3096 - mae: 0.3671\n",
      "Epoch 80: val_loss did not improve from 3.34479\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 80: train_loss=2.0236, val_loss=3.4869\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 3.3096 - mae: 0.3671 - val_loss: 3.5388 - val_mae: 0.3172\n",
      "Epoch 81/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.2554 - mae: 0.3594\n",
      "Epoch 81: val_loss did not improve from 3.34479\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 81: train_loss=1.9008, val_loss=5.9036\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 3.2554 - mae: 0.3594 - val_loss: 3.5574 - val_mae: 0.3124\n",
      "Epoch 82/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.2340 - mae: 0.3550\n",
      "Epoch 82: val_loss did not improve from 3.34479\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 82: train_loss=1.4230, val_loss=6.1697\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 3.2340 - mae: 0.3550 - val_loss: 3.3848 - val_mae: 0.3060\n",
      "Epoch 83/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.1497 - mae: 0.3518\n",
      "Epoch 83: val_loss improved from 3.34479 to 3.16984, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 83: train_loss=1.4170, val_loss=2.3015\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 3.1497 - mae: 0.3518 - val_loss: 3.1698 - val_mae: 0.3008\n",
      "Epoch 84/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.1080 - mae: 0.3473\n",
      "Epoch 84: val_loss improved from 3.16984 to 3.08992, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 84: train_loss=1.4547, val_loss=2.7599\n",
      "67/67 [==============================] - 12s 169ms/step - loss: 3.1080 - mae: 0.3473 - val_loss: 3.0899 - val_mae: 0.2992\n",
      "Epoch 85/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.0797 - mae: 0.3448\n",
      "Epoch 85: val_loss improved from 3.08992 to 3.08032, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 85: train_loss=1.4750, val_loss=7.1037\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.0797 - mae: 0.3448 - val_loss: 3.0803 - val_mae: 0.2949\n",
      "Epoch 86/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.1168 - mae: 0.3440\n",
      "Epoch 86: val_loss did not improve from 3.08032\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 86: train_loss=2.1491, val_loss=2.9418\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 3.1168 - mae: 0.3440 - val_loss: 3.4446 - val_mae: 0.2938\n",
      "Epoch 87/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.0100 - mae: 0.3370\n",
      "Epoch 87: val_loss did not improve from 3.08032\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 87: train_loss=1.7068, val_loss=2.4183\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 3.0100 - mae: 0.3370 - val_loss: 3.0993 - val_mae: 0.2897\n",
      "Epoch 88/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9978 - mae: 0.3359\n",
      "Epoch 88: val_loss improved from 3.08032 to 3.06959, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "Epoch 88: train_loss=2.3566, val_loss=2.4203\n",
      "67/67 [==============================] - 11s 171ms/step - loss: 2.9978 - mae: 0.3359 - val_loss: 3.0696 - val_mae: 0.2874\n",
      "Epoch 89/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8629 - mae: 0.3300\n",
      "Epoch 89: val_loss improved from 3.06959 to 3.06884, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 89: train_loss=1.8133, val_loss=9.1709\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.8629 - mae: 0.3300 - val_loss: 3.0688 - val_mae: 0.2733\n",
      "Epoch 90/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8702 - mae: 0.3260\n",
      "Epoch 90: val_loss improved from 3.06884 to 2.98388, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "Epoch 90: train_loss=1.4625, val_loss=2.4100\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 2.8702 - mae: 0.3260 - val_loss: 2.9839 - val_mae: 0.2731\n",
      "Epoch 91/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9168 - mae: 0.3274\n",
      "Epoch 91: val_loss improved from 2.98388 to 2.76184, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 91: train_loss=1.5106, val_loss=4.1919\n",
      "67/67 [==============================] - 12s 170ms/step - loss: 2.9168 - mae: 0.3274 - val_loss: 2.7618 - val_mae: 0.2695\n",
      "Epoch 92/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8273 - mae: 0.3250\n",
      "Epoch 92: val_loss improved from 2.76184 to 2.61216, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "Epoch 92: train_loss=1.3626, val_loss=2.5985\n",
      "67/67 [==============================] - 11s 159ms/step - loss: 2.8273 - mae: 0.3250 - val_loss: 2.6122 - val_mae: 0.2658\n",
      "Epoch 93/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7459 - mae: 0.3213\n",
      "Epoch 93: val_loss improved from 2.61216 to 2.46595, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "Epoch 93: train_loss=1.4087, val_loss=2.4558\n",
      "67/67 [==============================] - 11s 169ms/step - loss: 2.7459 - mae: 0.3213 - val_loss: 2.4659 - val_mae: 0.2587\n",
      "Epoch 94/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6684 - mae: 0.3147\n",
      "Epoch 94: val_loss did not improve from 2.46595\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 94: train_loss=1.0434, val_loss=2.2833\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 2.6684 - mae: 0.3147 - val_loss: 2.5769 - val_mae: 0.2611\n",
      "Epoch 95/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7025 - mae: 0.3128\n",
      "Epoch 95: val_loss did not improve from 2.46595\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 95: train_loss=1.5347, val_loss=2.0781\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.7025 - mae: 0.3128 - val_loss: 2.5746 - val_mae: 0.2509\n",
      "Epoch 96/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6379 - mae: 0.3086\n",
      "Epoch 96: val_loss improved from 2.46595 to 2.39575, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 50ms/step\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "Epoch 96: train_loss=1.2877, val_loss=1.8616\n",
      "67/67 [==============================] - 12s 174ms/step - loss: 2.6379 - mae: 0.3086 - val_loss: 2.3957 - val_mae: 0.2521\n",
      "Epoch 97/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6086 - mae: 0.3091\n",
      "Epoch 97: val_loss improved from 2.39575 to 2.29528, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 97: train_loss=1.0916, val_loss=2.4860\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 2.6086 - mae: 0.3091 - val_loss: 2.2953 - val_mae: 0.2474\n",
      "Epoch 98/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.7069 - mae: 0.3091\n",
      "Epoch 98: val_loss did not improve from 2.29528\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 98: train_loss=1.2105, val_loss=1.9557\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 2.7069 - mae: 0.3091 - val_loss: 4.1593 - val_mae: 0.2408\n",
      "Epoch 99/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.9503 - mae: 0.3081\n",
      "Epoch 99: val_loss did not improve from 2.29528\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 99: train_loss=1.4647, val_loss=4.6506\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.9503 - mae: 0.3081 - val_loss: 2.7422 - val_mae: 0.2495\n",
      "Epoch 100/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.5947 - mae: 0.3014\n",
      "Epoch 100: val_loss improved from 2.29528 to 2.21767, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 100: train_loss=0.9885, val_loss=1.7576\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 2.5947 - mae: 0.3014 - val_loss: 2.2177 - val_mae: 0.2372\n",
      "Epoch 101/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.5446 - mae: 0.2984\n",
      "Epoch 101: val_loss did not improve from 2.21767\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 101: train_loss=1.3258, val_loss=2.2323\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.5446 - mae: 0.2984 - val_loss: 2.2463 - val_mae: 0.2416\n",
      "Epoch 102/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.5349 - mae: 0.2957\n",
      "Epoch 102: val_loss improved from 2.21767 to 2.10961, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 102: train_loss=0.8834, val_loss=3.3149\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.5349 - mae: 0.2957 - val_loss: 2.1096 - val_mae: 0.2339\n",
      "Epoch 103/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.4411 - mae: 0.2893\n",
      "Epoch 103: val_loss improved from 2.10961 to 2.06368, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 103: train_loss=1.0759, val_loss=2.2138\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 2.4411 - mae: 0.2893 - val_loss: 2.0637 - val_mae: 0.2355\n",
      "Epoch 104/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.4523 - mae: 0.2906\n",
      "Epoch 104: val_loss improved from 2.06368 to 2.05959, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 104: train_loss=0.9589, val_loss=1.7897\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 2.4523 - mae: 0.2906 - val_loss: 2.0596 - val_mae: 0.2249\n",
      "Epoch 105/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.4067 - mae: 0.2880\n",
      "Epoch 105: val_loss did not improve from 2.05959\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 105: train_loss=1.1655, val_loss=1.4685\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.4067 - mae: 0.2880 - val_loss: 2.1859 - val_mae: 0.2259\n",
      "Epoch 106/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.3967 - mae: 0.2859\n",
      "Epoch 106: val_loss improved from 2.05959 to 2.04672, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "Epoch 106: train_loss=0.6811, val_loss=6.4468\n",
      "67/67 [==============================] - 11s 160ms/step - loss: 2.3967 - mae: 0.2859 - val_loss: 2.0467 - val_mae: 0.2241\n",
      "Epoch 107/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.3149 - mae: 0.2796\n",
      "Epoch 107: val_loss improved from 2.04672 to 1.99972, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 107: train_loss=0.9546, val_loss=1.7590\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 2.3149 - mae: 0.2796 - val_loss: 1.9997 - val_mae: 0.2122\n",
      "Epoch 108/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.3640 - mae: 0.2816\n",
      "Epoch 108: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 108: train_loss=0.8069, val_loss=5.3827\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.3640 - mae: 0.2816 - val_loss: 2.0345 - val_mae: 0.2093\n",
      "Epoch 109/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2345 - mae: 0.2790\n",
      "Epoch 109: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 109: train_loss=1.1999, val_loss=2.0681\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 2.2345 - mae: 0.2790 - val_loss: 2.3418 - val_mae: 0.2192\n",
      "Epoch 110/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.3068 - mae: 0.2780\n",
      "Epoch 110: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 110: train_loss=0.9207, val_loss=1.9162\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.3068 - mae: 0.2780 - val_loss: 2.3338 - val_mae: 0.2131\n",
      "Epoch 111/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2658 - mae: 0.2748\n",
      "Epoch 111: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 111: train_loss=1.0153, val_loss=1.6847\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.2658 - mae: 0.2748 - val_loss: 2.1901 - val_mae: 0.2080\n",
      "Epoch 112/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2056 - mae: 0.2708\n",
      "Epoch 112: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 112: train_loss=0.8122, val_loss=1.2708\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.2056 - mae: 0.2708 - val_loss: 2.0030 - val_mae: 0.2093\n",
      "Epoch 113/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2371 - mae: 0.2711\n",
      "Epoch 113: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 113: train_loss=1.1463, val_loss=1.2687\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 2.2371 - mae: 0.2711 - val_loss: 2.1897 - val_mae: 0.2018\n",
      "Epoch 114/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2377 - mae: 0.2710\n",
      "Epoch 114: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 114: train_loss=0.8092, val_loss=1.9514\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.2377 - mae: 0.2710 - val_loss: 2.2591 - val_mae: 0.2054\n",
      "Epoch 115/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2092 - mae: 0.2676\n",
      "Epoch 115: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 115: train_loss=0.8794, val_loss=3.6331\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.2092 - mae: 0.2676 - val_loss: 2.1493 - val_mae: 0.2043\n",
      "Epoch 116/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1547 - mae: 0.2656\n",
      "Epoch 116: val_loss did not improve from 1.99972\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 116: train_loss=0.8173, val_loss=5.0820\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.1547 - mae: 0.2656 - val_loss: 2.0042 - val_mae: 0.1991\n",
      "Epoch 117/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0936 - mae: 0.2625\n",
      "Epoch 117: val_loss improved from 1.99972 to 1.96705, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 117: train_loss=0.9301, val_loss=1.4262\n",
      "67/67 [==============================] - 12s 173ms/step - loss: 2.0936 - mae: 0.2625 - val_loss: 1.9671 - val_mae: 0.2038\n",
      "Epoch 118/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1167 - mae: 0.2615\n",
      "Epoch 118: val_loss improved from 1.96705 to 1.88649, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 118: train_loss=0.6961, val_loss=1.5830\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 2.1167 - mae: 0.2615 - val_loss: 1.8865 - val_mae: 0.1972\n",
      "Epoch 119/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1190 - mae: 0.2630\n",
      "Epoch 119: val_loss did not improve from 1.88649\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 119: train_loss=0.6791, val_loss=2.3743\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 2.1190 - mae: 0.2630 - val_loss: 2.0186 - val_mae: 0.1966\n",
      "Epoch 120/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1440 - mae: 0.2630\n",
      "Epoch 120: val_loss improved from 1.88649 to 1.83272, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 120: train_loss=0.7540, val_loss=1.1591\n",
      "67/67 [==============================] - 11s 169ms/step - loss: 2.1440 - mae: 0.2630 - val_loss: 1.8327 - val_mae: 0.1856\n",
      "Epoch 121/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1623 - mae: 0.2598\n",
      "Epoch 121: val_loss did not improve from 1.83272\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 121: train_loss=0.7533, val_loss=1.6612\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.1623 - mae: 0.2598 - val_loss: 3.1628 - val_mae: 0.2025\n",
      "Epoch 122/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.2738 - mae: 0.2704\n",
      "Epoch 122: val_loss did not improve from 1.83272\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 122: train_loss=0.9510, val_loss=2.2202\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 3.2738 - mae: 0.2704 - val_loss: 2.9130 - val_mae: 0.2184\n",
      "Epoch 123/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6804 - mae: 0.2732\n",
      "Epoch 123: val_loss did not improve from 1.83272\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 123: train_loss=0.8043, val_loss=6.4016\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.6804 - mae: 0.2732 - val_loss: 1.9674 - val_mae: 0.2038\n",
      "Epoch 124/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.2232 - mae: 0.2658\n",
      "Epoch 124: val_loss improved from 1.83272 to 1.74420, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 124: train_loss=0.7835, val_loss=1.3780\n",
      "67/67 [==============================] - 11s 168ms/step - loss: 2.2232 - mae: 0.2658 - val_loss: 1.7442 - val_mae: 0.1951\n",
      "Epoch 125/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.1447 - mae: 0.2645\n",
      "Epoch 125: val_loss improved from 1.74420 to 1.62176, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 125: train_loss=0.6938, val_loss=1.3509\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 2.1447 - mae: 0.2645 - val_loss: 1.6218 - val_mae: 0.1945\n",
      "Epoch 126/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0326 - mae: 0.2566\n",
      "Epoch 126: val_loss improved from 1.62176 to 1.50574, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 126: train_loss=0.7569, val_loss=1.5926\n",
      "67/67 [==============================] - 12s 172ms/step - loss: 2.0326 - mae: 0.2566 - val_loss: 1.5057 - val_mae: 0.1839\n",
      "Epoch 127/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9740 - mae: 0.2539\n",
      "Epoch 127: val_loss did not improve from 1.50574\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 127: train_loss=0.6373, val_loss=1.5086\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.9740 - mae: 0.2539 - val_loss: 1.7433 - val_mae: 0.1822\n",
      "Epoch 128/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.0010 - mae: 0.2552\n",
      "Epoch 128: val_loss did not improve from 1.50574\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 128: train_loss=0.6419, val_loss=1.4195\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.0010 - mae: 0.2552 - val_loss: 1.6169 - val_mae: 0.1834\n",
      "Epoch 129/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9279 - mae: 0.2499\n",
      "Epoch 129: val_loss did not improve from 1.50574\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 129: train_loss=0.7590, val_loss=1.3585\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.9279 - mae: 0.2499 - val_loss: 1.7247 - val_mae: 0.1788\n",
      "Epoch 130/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9950 - mae: 0.2542\n",
      "Epoch 130: val_loss did not improve from 1.50574\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 130: train_loss=0.5310, val_loss=1.1055\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.9950 - mae: 0.2542 - val_loss: 1.5904 - val_mae: 0.1775\n",
      "Epoch 131/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9146 - mae: 0.2482\n",
      "Epoch 131: val_loss did not improve from 1.50574\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 131: train_loss=0.5920, val_loss=1.2475\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.9146 - mae: 0.2482 - val_loss: 1.6231 - val_mae: 0.1770\n",
      "Epoch 132/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9178 - mae: 0.2471\n",
      "Epoch 132: val_loss improved from 1.50574 to 1.49730, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 132: train_loss=0.6403, val_loss=1.5282\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 1.9178 - mae: 0.2471 - val_loss: 1.4973 - val_mae: 0.1836\n",
      "Epoch 133/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9002 - mae: 0.2477\n",
      "Epoch 133: val_loss improved from 1.49730 to 1.48831, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "Epoch 133: train_loss=0.6108, val_loss=1.6013\n",
      "67/67 [==============================] - 11s 169ms/step - loss: 1.9002 - mae: 0.2477 - val_loss: 1.4883 - val_mae: 0.1722\n",
      "Epoch 134/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8728 - mae: 0.2445\n",
      "Epoch 134: val_loss did not improve from 1.48831\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 134: train_loss=1.9891, val_loss=1.1231\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8728 - mae: 0.2445 - val_loss: 1.5819 - val_mae: 0.1713\n",
      "Epoch 135/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8983 - mae: 0.2454\n",
      "Epoch 135: val_loss improved from 1.48831 to 1.40887, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "Epoch 135: train_loss=0.5602, val_loss=1.0511\n",
      "67/67 [==============================] - 12s 174ms/step - loss: 1.8983 - mae: 0.2454 - val_loss: 1.4089 - val_mae: 0.1733\n",
      "Epoch 136/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8616 - mae: 0.2460\n",
      "Epoch 136: val_loss did not improve from 1.40887\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 136: train_loss=0.6440, val_loss=2.6564\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8616 - mae: 0.2460 - val_loss: 1.5372 - val_mae: 0.1757\n",
      "Epoch 137/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8305 - mae: 0.2422\n",
      "Epoch 137: val_loss improved from 1.40887 to 1.39783, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 47ms/step\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "Epoch 137: train_loss=0.4578, val_loss=0.9533\n",
      "67/67 [==============================] - 12s 174ms/step - loss: 1.8305 - mae: 0.2422 - val_loss: 1.3978 - val_mae: 0.1653\n",
      "Epoch 138/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8123 - mae: 0.2406\n",
      "Epoch 138: val_loss improved from 1.39783 to 1.36347, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Epoch 138: train_loss=0.5759, val_loss=0.9681\n",
      "67/67 [==============================] - 12s 171ms/step - loss: 1.8123 - mae: 0.2406 - val_loss: 1.3635 - val_mae: 0.1666\n",
      "Epoch 139/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7689 - mae: 0.2396\n",
      "Epoch 139: val_loss improved from 1.36347 to 1.34996, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 139: train_loss=0.4584, val_loss=0.8361\n",
      "67/67 [==============================] - 11s 169ms/step - loss: 1.7689 - mae: 0.2396 - val_loss: 1.3500 - val_mae: 0.1627\n",
      "Epoch 140/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7933 - mae: 0.2367\n",
      "Epoch 140: val_loss did not improve from 1.34996\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 140: train_loss=0.5900, val_loss=5.7891\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7933 - mae: 0.2367 - val_loss: 1.4424 - val_mae: 0.1614\n",
      "Epoch 141/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8101 - mae: 0.2409\n",
      "Epoch 141: val_loss did not improve from 1.34996\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 141: train_loss=0.5572, val_loss=1.0752\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8101 - mae: 0.2409 - val_loss: 1.4523 - val_mae: 0.1661\n",
      "Epoch 142/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8550 - mae: 0.2417\n",
      "Epoch 142: val_loss did not improve from 1.34996\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 142: train_loss=0.4206, val_loss=1.3498\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8550 - mae: 0.2417 - val_loss: 1.4307 - val_mae: 0.1598\n",
      "Epoch 143/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8057 - mae: 0.2385\n",
      "Epoch 143: val_loss did not improve from 1.34996\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 143: train_loss=0.4024, val_loss=1.1731\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8057 - mae: 0.2385 - val_loss: 1.4250 - val_mae: 0.1602\n",
      "Epoch 144/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8093 - mae: 0.2400\n",
      "Epoch 144: val_loss improved from 1.34996 to 1.27744, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 144: train_loss=0.4485, val_loss=0.8989\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 1.8093 - mae: 0.2400 - val_loss: 1.2774 - val_mae: 0.1586\n",
      "Epoch 145/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.8014 - mae: 0.2388\n",
      "Epoch 145: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 145: train_loss=0.5351, val_loss=1.2209\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.8014 - mae: 0.2388 - val_loss: 1.4449 - val_mae: 0.1633\n",
      "Epoch 146/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7438 - mae: 0.2353\n",
      "Epoch 146: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 146: train_loss=0.5757, val_loss=0.9632\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7438 - mae: 0.2353 - val_loss: 1.3505 - val_mae: 0.1565\n",
      "Epoch 147/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7094 - mae: 0.2348\n",
      "Epoch 147: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 147: train_loss=0.5053, val_loss=0.9700\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7094 - mae: 0.2348 - val_loss: 1.3192 - val_mae: 0.1594\n",
      "Epoch 148/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7293 - mae: 0.2342\n",
      "Epoch 148: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 148: train_loss=0.4247, val_loss=1.0823\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7293 - mae: 0.2342 - val_loss: 1.3181 - val_mae: 0.1635\n",
      "Epoch 149/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7006 - mae: 0.2345\n",
      "Epoch 149: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 149: train_loss=0.4223, val_loss=2.5830\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7006 - mae: 0.2345 - val_loss: 1.3565 - val_mae: 0.1549\n",
      "Epoch 150/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6713 - mae: 0.2311\n",
      "Epoch 150: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 150: train_loss=0.3382, val_loss=1.8557\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6713 - mae: 0.2311 - val_loss: 1.3494 - val_mae: 0.1574\n",
      "Epoch 151/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6904 - mae: 0.2350\n",
      "Epoch 151: val_loss did not improve from 1.27744\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 151: train_loss=0.5640, val_loss=0.8940\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6904 - mae: 0.2350 - val_loss: 1.5045 - val_mae: 0.1548\n",
      "Epoch 152/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6629 - mae: 0.2318\n",
      "Epoch 152: val_loss improved from 1.27744 to 1.22243, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "Epoch 152: train_loss=0.4194, val_loss=0.8795\n",
      "67/67 [==============================] - 12s 179ms/step - loss: 1.6629 - mae: 0.2318 - val_loss: 1.2224 - val_mae: 0.1586\n",
      "Epoch 153/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6630 - mae: 0.2309\n",
      "Epoch 153: val_loss did not improve from 1.22243\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 153: train_loss=0.6099, val_loss=1.4189\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6630 - mae: 0.2309 - val_loss: 1.2225 - val_mae: 0.1512\n",
      "Epoch 154/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6441 - mae: 0.2272\n",
      "Epoch 154: val_loss did not improve from 1.22243\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 154: train_loss=0.5722, val_loss=0.9648\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6441 - mae: 0.2272 - val_loss: 1.3131 - val_mae: 0.1577\n",
      "Epoch 155/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6223 - mae: 0.2283\n",
      "Epoch 155: val_loss improved from 1.22243 to 1.10185, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 155: train_loss=0.3160, val_loss=0.7493\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.6223 - mae: 0.2283 - val_loss: 1.1018 - val_mae: 0.1437\n",
      "Epoch 156/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6770 - mae: 0.2279\n",
      "Epoch 156: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 156: train_loss=0.4443, val_loss=0.7383\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6770 - mae: 0.2279 - val_loss: 1.2611 - val_mae: 0.1490\n",
      "Epoch 157/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6032 - mae: 0.2244\n",
      "Epoch 157: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 157: train_loss=0.3616, val_loss=5.6904\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6032 - mae: 0.2244 - val_loss: 1.2558 - val_mae: 0.1490\n",
      "Epoch 158/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6151 - mae: 0.2258\n",
      "Epoch 158: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 158: train_loss=0.3608, val_loss=0.9875\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6151 - mae: 0.2258 - val_loss: 1.4216 - val_mae: 0.1470\n",
      "Epoch 159/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6002 - mae: 0.2259\n",
      "Epoch 159: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 159: train_loss=0.4525, val_loss=2.5134\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6002 - mae: 0.2259 - val_loss: 1.2749 - val_mae: 0.1570\n",
      "Epoch 160/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6752 - mae: 0.2267\n",
      "Epoch 160: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 160: train_loss=0.4639, val_loss=1.0401\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6752 - mae: 0.2267 - val_loss: 1.2891 - val_mae: 0.1558\n",
      "Epoch 161/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6356 - mae: 0.2263\n",
      "Epoch 161: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 161: train_loss=0.3451, val_loss=0.7002\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6356 - mae: 0.2263 - val_loss: 1.1734 - val_mae: 0.1446\n",
      "Epoch 162/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5687 - mae: 0.2239\n",
      "Epoch 162: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 162: train_loss=0.4805, val_loss=0.8314\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5687 - mae: 0.2239 - val_loss: 1.3476 - val_mae: 0.1395\n",
      "Epoch 163/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6301 - mae: 0.2238\n",
      "Epoch 163: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 163: train_loss=0.4021, val_loss=0.8039\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6301 - mae: 0.2238 - val_loss: 1.2744 - val_mae: 0.1440\n",
      "Epoch 164/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5574 - mae: 0.2228\n",
      "Epoch 164: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 164: train_loss=0.4217, val_loss=0.8608\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5574 - mae: 0.2228 - val_loss: 1.1852 - val_mae: 0.1495\n",
      "Epoch 165/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6229 - mae: 0.2242\n",
      "Epoch 165: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 165: train_loss=0.4032, val_loss=0.8856\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.6229 - mae: 0.2242 - val_loss: 1.1406 - val_mae: 0.1477\n",
      "Epoch 166/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5706 - mae: 0.2241\n",
      "Epoch 166: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 166: train_loss=0.4077, val_loss=1.8572\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5706 - mae: 0.2241 - val_loss: 1.1443 - val_mae: 0.1504\n",
      "Epoch 167/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5694 - mae: 0.2244\n",
      "Epoch 167: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 167: train_loss=0.4397, val_loss=1.7473\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5694 - mae: 0.2244 - val_loss: 1.3522 - val_mae: 0.1453\n",
      "Epoch 168/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5611 - mae: 0.2221\n",
      "Epoch 168: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 168: train_loss=0.3039, val_loss=0.7519\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5611 - mae: 0.2221 - val_loss: 1.2342 - val_mae: 0.1456\n",
      "Epoch 169/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5162 - mae: 0.2211\n",
      "Epoch 169: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 169: train_loss=0.3558, val_loss=0.8813\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5162 - mae: 0.2211 - val_loss: 1.1991 - val_mae: 0.1412\n",
      "Epoch 170/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5698 - mae: 0.2210\n",
      "Epoch 170: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 170: train_loss=0.3720, val_loss=3.2564\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 1.5698 - mae: 0.2210 - val_loss: 1.3191 - val_mae: 0.1477\n",
      "Epoch 171/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5348 - mae: 0.2208\n",
      "Epoch 171: val_loss did not improve from 1.10185\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 171: train_loss=0.4339, val_loss=0.7556\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5348 - mae: 0.2208 - val_loss: 1.2176 - val_mae: 0.1473\n",
      "Epoch 172/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5105 - mae: 0.2171\n",
      "Epoch 172: val_loss improved from 1.10185 to 1.09746, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 46ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "Epoch 172: train_loss=0.2925, val_loss=0.6854\n",
      "67/67 [==============================] - 12s 172ms/step - loss: 1.5105 - mae: 0.2171 - val_loss: 1.0975 - val_mae: 0.1412\n",
      "Epoch 173/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5265 - mae: 0.2212\n",
      "Epoch 173: val_loss did not improve from 1.09746\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 173: train_loss=0.4121, val_loss=0.8148\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5265 - mae: 0.2212 - val_loss: 1.3085 - val_mae: 0.1504\n",
      "Epoch 174/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5137 - mae: 0.2208\n",
      "Epoch 174: val_loss did not improve from 1.09746\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 174: train_loss=0.3989, val_loss=4.9979\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.5137 - mae: 0.2208 - val_loss: 1.2262 - val_mae: 0.1403\n",
      "Epoch 175/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5314 - mae: 0.2209\n",
      "Epoch 175: val_loss improved from 1.09746 to 1.06909, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 175: train_loss=0.3887, val_loss=0.9732\n",
      "67/67 [==============================] - 11s 166ms/step - loss: 1.5314 - mae: 0.2209 - val_loss: 1.0691 - val_mae: 0.1393\n",
      "Epoch 176/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5086 - mae: 0.2174\n",
      "Epoch 176: val_loss improved from 1.06909 to 1.04965, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 176: train_loss=0.4360, val_loss=2.8031\n",
      "67/67 [==============================] - 11s 170ms/step - loss: 1.5086 - mae: 0.2174 - val_loss: 1.0496 - val_mae: 0.1458\n",
      "Epoch 177/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4922 - mae: 0.2194\n",
      "Epoch 177: val_loss did not improve from 1.04965\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 177: train_loss=0.3900, val_loss=0.8593\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4922 - mae: 0.2194 - val_loss: 1.0655 - val_mae: 0.1412\n",
      "Epoch 178/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4586 - mae: 0.2162\n",
      "Epoch 178: val_loss did not improve from 1.04965\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 178: train_loss=0.4585, val_loss=0.8553\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4586 - mae: 0.2162 - val_loss: 1.0566 - val_mae: 0.1374\n",
      "Epoch 179/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4856 - mae: 0.2145\n",
      "Epoch 179: val_loss did not improve from 1.04965\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 179: train_loss=0.3274, val_loss=0.7913\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4856 - mae: 0.2145 - val_loss: 1.0777 - val_mae: 0.1387\n",
      "Epoch 180/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4672 - mae: 0.2159\n",
      "Epoch 180: val_loss did not improve from 1.04965\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 180: train_loss=0.3325, val_loss=0.6037\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4672 - mae: 0.2159 - val_loss: 1.1919 - val_mae: 0.1338\n",
      "Epoch 181/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4590 - mae: 0.2140\n",
      "Epoch 181: val_loss did not improve from 1.04965\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 181: train_loss=0.3855, val_loss=0.6893\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.4590 - mae: 0.2140 - val_loss: 1.1946 - val_mae: 0.1399\n",
      "Epoch 182/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4770 - mae: 0.2148\n",
      "Epoch 182: val_loss improved from 1.04965 to 0.96062, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "Epoch 182: train_loss=0.3500, val_loss=0.7352\n",
      "67/67 [==============================] - 11s 167ms/step - loss: 1.4770 - mae: 0.2148 - val_loss: 0.9606 - val_mae: 0.1392\n",
      "Epoch 183/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4404 - mae: 0.2138\n",
      "Epoch 183: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "Epoch 183: train_loss=0.4516, val_loss=2.5169\n",
      "67/67 [==============================] - 9s 135ms/step - loss: 1.4404 - mae: 0.2138 - val_loss: 1.0250 - val_mae: 0.1376\n",
      "Epoch 184/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4391 - mae: 0.2155\n",
      "Epoch 184: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 184: train_loss=0.3007, val_loss=1.6128\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4391 - mae: 0.2155 - val_loss: 1.1370 - val_mae: 0.1400\n",
      "Epoch 185/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4450 - mae: 0.2142\n",
      "Epoch 185: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 185: train_loss=0.3055, val_loss=0.5486\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.4450 - mae: 0.2142 - val_loss: 1.0509 - val_mae: 0.1283\n",
      "Epoch 186/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4345 - mae: 0.2129\n",
      "Epoch 186: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 186: train_loss=0.3099, val_loss=0.6649\n",
      "67/67 [==============================] - 9s 133ms/step - loss: 1.4345 - mae: 0.2129 - val_loss: 1.1913 - val_mae: 0.1341\n",
      "Epoch 187/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4469 - mae: 0.2136\n",
      "Epoch 187: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 187: train_loss=0.3068, val_loss=3.7562\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.4469 - mae: 0.2136 - val_loss: 1.1889 - val_mae: 0.1451\n",
      "Epoch 188/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4014 - mae: 0.2123\n",
      "Epoch 188: val_loss did not improve from 0.96062\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 188: train_loss=0.2394, val_loss=0.5527\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4014 - mae: 0.2123 - val_loss: 0.9643 - val_mae: 0.1282\n",
      "Epoch 189/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4347 - mae: 0.2120\n",
      "Epoch 189: val_loss improved from 0.96062 to 0.90226, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "Epoch 189: train_loss=0.2974, val_loss=0.6143\n",
      "67/67 [==============================] - 11s 169ms/step - loss: 1.4347 - mae: 0.2120 - val_loss: 0.9023 - val_mae: 0.1373\n",
      "Epoch 190/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4258 - mae: 0.2130\n",
      "Epoch 190: val_loss did not improve from 0.90226\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 190: train_loss=0.2935, val_loss=0.5813\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.4258 - mae: 0.2130 - val_loss: 0.9200 - val_mae: 0.1397\n",
      "Epoch 191/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4270 - mae: 0.2119\n",
      "Epoch 191: val_loss did not improve from 0.90226\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 191: train_loss=0.3046, val_loss=3.9053\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.4270 - mae: 0.2119 - val_loss: 1.1057 - val_mae: 0.1272\n",
      "Epoch 192/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4185 - mae: 0.2120\n",
      "Epoch 192: val_loss did not improve from 0.90226\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 192: train_loss=0.2794, val_loss=0.6127\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4185 - mae: 0.2120 - val_loss: 0.9695 - val_mae: 0.1325\n",
      "Epoch 193/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4299 - mae: 0.2083\n",
      "Epoch 193: val_loss did not improve from 0.90226\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 193: train_loss=0.2304, val_loss=0.9750\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4299 - mae: 0.2083 - val_loss: 1.2022 - val_mae: 0.1317\n",
      "Epoch 194/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4212 - mae: 0.2116\n",
      "Epoch 194: val_loss improved from 0.90226 to 0.80731, saving model to best_model_mobile_val.h5\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "Epoch 194: train_loss=0.3405, val_loss=0.6399\n",
      "67/67 [==============================] - 11s 160ms/step - loss: 1.4212 - mae: 0.2116 - val_loss: 0.8073 - val_mae: 0.1270\n",
      "Epoch 195/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.3917 - mae: 0.2081\n",
      "Epoch 195: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 195: train_loss=0.3320, val_loss=0.6448\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 1.3917 - mae: 0.2081 - val_loss: 1.0346 - val_mae: 0.1342\n",
      "Epoch 196/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.4951 - mae: 0.2299\n",
      "Epoch 196: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 196: train_loss=0.3657, val_loss=1.4856\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.4951 - mae: 0.2299 - val_loss: 1.6714 - val_mae: 0.1603\n",
      "Epoch 197/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.4277 - mae: 0.2359\n",
      "Epoch 197: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 197: train_loss=0.3760, val_loss=0.8683\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 2.4277 - mae: 0.2359 - val_loss: 1.7851 - val_mae: 0.1534\n",
      "Epoch 198/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7385 - mae: 0.2246\n",
      "Epoch 198: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 198: train_loss=0.4473, val_loss=0.8037\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.7385 - mae: 0.2246 - val_loss: 1.7672 - val_mae: 0.1524\n",
      "Epoch 199/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4613 - mae: 0.2175\n",
      "Epoch 199: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 199: train_loss=0.3669, val_loss=0.7130\n",
      "67/67 [==============================] - 9s 131ms/step - loss: 1.4613 - mae: 0.2175 - val_loss: 1.3910 - val_mae: 0.1408\n",
      "Epoch 200/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.7890 - mae: 0.2188\n",
      "Epoch 200: val_loss did not improve from 0.80731\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 200: train_loss=2.6058, val_loss=2.5966\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 1.7890 - mae: 0.2188 - val_loss: 1.4286 - val_mae: 0.1472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "# ----------- GPU Configuration -----------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# ----------- 1. Load all data for training -----------\n",
    "train_df = pd.read_csv('./csbf3nvm/NVM23_train_split.txt', delimiter=' ',\n",
    "                       names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "val_df = pd.read_csv('./csbf3nvm/NVM23_val_split.txt', delimiter=' ',\n",
    "                     names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ----------- 2. Define custom loss function -----------\n",
    "def custom_loss(y_true, y_pred):\n",
    "    beta = 400.0\n",
    "    x_pred = y_pred[:, :3]\n",
    "    q_pred = y_pred[:, 3:]\n",
    "    x_true = y_true[:, :3]\n",
    "    q_true = y_true[:, 3:]\n",
    "\n",
    "    q_norm = K.sqrt(K.sum(K.square(q_true), axis=-1, keepdims=True)) + K.epsilon()\n",
    "    q_true_normed = q_true / q_norm\n",
    "\n",
    "    position_loss = K.mean(K.square(x_pred - x_true), axis=-1)\n",
    "    quaternion_loss = K.mean(K.square(q_pred - q_true_normed), axis=-1)\n",
    "\n",
    "    return position_loss + beta * quaternion_loss\n",
    "\n",
    "# ----------- 3. Build model with MobileNetV3 -----------\n",
    "base_model = MobileNetV3Large(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = True  # Set to True later for fine-tuning\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=True)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(7)(x)  # Output: [X, Y, Z, W, P, Q, R]\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# ----------- 4. Compile model -----------\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=custom_loss,\n",
    "              metrics=['mae'])\n",
    "model.summary()\n",
    "# ----------- 5. Custom callback (updated to log both train and val loss) -----------\n",
    "class TrainValLossLogger(Callback):\n",
    "    def __init__(self, train_gen, val_gen, output_file='loss_mobile.csv'):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.output_file = output_file\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        def compute_losses(gen):\n",
    "            x, y_true = next(iter(gen))\n",
    "            y_pred = self.model.predict(x)\n",
    "            x_pred, q_pred = y_pred[:, :3], y_pred[:, 3:]\n",
    "            x_true, q_true = y_true[:, :3], y_true[:, 3:]\n",
    "            q_true_normed = q_true / (np.linalg.norm(q_true, axis=1, keepdims=True) + 1e-7)\n",
    "            pos_loss = np.mean(np.square(x_pred - x_true))\n",
    "            quat_loss = np.mean(np.square(q_pred - q_true_normed))\n",
    "            total_loss = pos_loss + 400. * quat_loss\n",
    "            return total_loss, pos_loss, quat_loss\n",
    "\n",
    "        train_total, train_pos, train_quat = compute_losses(self.train_gen)\n",
    "        val_total, val_pos, val_quat = compute_losses(self.val_gen)\n",
    "\n",
    "        self.logs.append({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Train_loss': train_total,\n",
    "            'Train_pos_loss': train_pos,\n",
    "            'Train_quat_loss': train_quat,\n",
    "            'Val_loss': val_total,\n",
    "            'Val_pos_loss': val_pos,\n",
    "            'Val_quat_loss': val_quat\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(self.logs).to_csv(self.output_file, index=False)\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_total:.4f}, val_loss={val_total:.4f}\")\n",
    "\n",
    "# ----------- 6. Callbacks -----------\n",
    "loss_logger = TrainValLossLogger(train_generator, val_generator)\n",
    "# ----------- 7. Train with validation -----------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=200,\n",
    "    callbacks=[loss_logger]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) detected: ['/physical_device:GPU:0']\n",
      "Found 4262 validated image filenames.\n",
      "Found 1066 validated image filenames.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 224, 224, 3)  0           ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['normalization[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  4608        ['stem_activation[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_activation (Ac  (None, 112, 112, 16  0          ['block1a_project_bn[0][0]']     \n",
      " tivation)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 56, 56, 64)   9216        ['block1a_project_activation[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 56, 56, 64)  256         ['block2a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 56, 56, 64)  0           ['block2a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 32)   2048        ['block2a_expand_activation[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 32)  128         ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 128)  36864       ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 128)  512        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 128)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 32)   4096        ['block2b_expand_activation[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 32)  128         ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 32)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 32)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 28, 28, 128)  36864       ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 28, 28, 128)  512        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 28, 28, 128)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 48)   6144        ['block3a_expand_activation[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 48)  192         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 192)  82944       ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 192)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 48)   9216        ['block3b_expand_activation[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 48)  192         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 48)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 48)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 192)  9216        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 192)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv2 (DepthwiseConv  (None, 14, 14, 192)  1728       ['block4a_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 192)  768        ['block4a_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 192)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 192)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 12)     2316        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 192)    2496        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 192)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 96)   18432       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 96)  384         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 384)  36864       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 384)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv2 (DepthwiseConv  (None, 14, 14, 384)  3456       ['block4b_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 384)  1536       ['block4b_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 384)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 384)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 384)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 96)   36864       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 96)  384         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 96)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 96)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 384)  36864       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 384)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv2 (DepthwiseConv  (None, 14, 14, 384)  3456       ['block4c_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 384)  1536       ['block4c_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 384)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 384)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 384)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 96)   36864       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 96)  384         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 96)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 96)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 576)  55296       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 576)  2304       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 576)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv2 (DepthwiseConv  (None, 14, 14, 576)  5184       ['block5a_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 576)  2304       ['block5a_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 576)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 576)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 576)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  64512       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv2 (DepthwiseConv  (None, 14, 14, 672)  6048       ['block5b_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv2 (DepthwiseConv  (None, 14, 14, 672)  6048       ['block5c_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_expand_activation (Act  (None, 14, 14, 672)  0          ['block5d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_dwconv2 (DepthwiseConv  (None, 14, 14, 672)  6048       ['block5d_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5d_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5d_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_activation (Activation  (None, 14, 14, 672)  0          ['block5d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_se_squeeze (GlobalAver  (None, 672)         0           ['block5d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5d_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5d_activation[0][0]',     \n",
      "                                                                  'block5d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5d_drop (Dropout)         (None, 14, 14, 112)  0           ['block5d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_add (Add)              (None, 14, 14, 112)  0           ['block5d_drop[0][0]',           \n",
      "                                                                  'block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_expand_activation (Act  (None, 14, 14, 672)  0          ['block5e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_dwconv2 (DepthwiseConv  (None, 14, 14, 672)  6048       ['block5e_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block5e_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5e_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_activation (Activation  (None, 14, 14, 672)  0          ['block5e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_se_squeeze (GlobalAver  (None, 672)         0           ['block5e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5e_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5e_activation[0][0]',     \n",
      "                                                                  'block5e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5e_drop (Dropout)         (None, 14, 14, 112)  0           ['block5e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_add (Add)              (None, 14, 14, 112)  0           ['block5e_drop[0][0]',           \n",
      "                                                                  'block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv2 (DepthwiseConv  (None, 7, 7, 672)   6048        ['block6a_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6b_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6c_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6d_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6e_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6e_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6e_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6e_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6e_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6e_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6e_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6e_activation (Activation  (None, 7, 7, 1152)  0           ['block6e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6e_se_squeeze (GlobalAver  (None, 1152)        0           ['block6e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6e_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6e_activation[0][0]',     \n",
      "                                                                  'block6e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6e_drop (Dropout)         (None, 7, 7, 192)    0           ['block6e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_add (Add)              (None, 7, 7, 192)    0           ['block6e_drop[0][0]',           \n",
      "                                                                  'block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6f_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6f_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6f_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6f_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6f_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6f_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6f_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6f_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6f_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6f_activation (Activation  (None, 7, 7, 1152)  0           ['block6f_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6f_se_squeeze (GlobalAver  (None, 1152)        0           ['block6f_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6f_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6f_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6f_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6f_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6f_activation[0][0]',     \n",
      "                                                                  'block6f_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6f_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6f_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6f_drop (Dropout)         (None, 7, 7, 192)    0           ['block6f_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_add (Add)              (None, 7, 7, 192)    0           ['block6f_drop[0][0]',           \n",
      "                                                                  'block6e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6g_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6f_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6g_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6g_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6g_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6g_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6g_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6g_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6g_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6g_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6g_activation (Activation  (None, 7, 7, 1152)  0           ['block6g_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6g_se_squeeze (GlobalAver  (None, 1152)        0           ['block6g_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6g_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6g_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6g_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6g_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6g_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6g_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6g_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6g_activation[0][0]',     \n",
      "                                                                  'block6g_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6g_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6g_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6g_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6g_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6g_drop (Dropout)         (None, 7, 7, 192)    0           ['block6g_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6g_add (Add)              (None, 7, 7, 192)    0           ['block6g_drop[0][0]',           \n",
      "                                                                  'block6f_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6h_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6g_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6h_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6h_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6h_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6h_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6h_dwconv2 (DepthwiseConv  (None, 7, 7, 1152)  10368       ['block6h_expand_activation[0][0]\n",
      " 2D)                                                             ']                               \n",
      "                                                                                                  \n",
      " block6h_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6h_dwconv2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6h_activation (Activation  (None, 7, 7, 1152)  0           ['block6h_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6h_se_squeeze (GlobalAver  (None, 1152)        0           ['block6h_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6h_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6h_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6h_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6h_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6h_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6h_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6h_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6h_activation[0][0]',     \n",
      "                                                                  'block6h_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6h_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6h_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6h_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6h_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6h_drop (Dropout)         (None, 7, 7, 192)    0           ['block6h_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6h_add (Add)              (None, 7, 7, 192)    0           ['block6h_drop[0][0]',           \n",
      "                                                                  'block6g_add[0][0]']            \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   245760      ['block6h_add[0][0]']            \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['top_activation[0][0]']         \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1311744     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7)            3591        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,759,447\n",
      "Trainable params: 7,698,839\n",
      "Non-trainable params: 60,608\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 11ms/steploss: 77.7126 - mae: 0.89\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 1: train_loss=57.5413, val_loss=52.8027\n",
      "67/67 [==============================] - 22s 199ms/step - loss: 77.7126 - mae: 0.8947 - val_loss: 58.0154 - val_mae: 0.8564\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 48.4073 - mae: 0.84\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 2: train_loss=38.0562, val_loss=45.6269\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 48.4073 - mae: 0.8446 - val_loss: 37.3963 - val_mae: 0.8193\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 37.1607 - mae: 0.82\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "Epoch 3: train_loss=46.8200, val_loss=42.5203\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 37.1607 - mae: 0.8230 - val_loss: 47.0268 - val_mae: 0.8335\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 29.6271 - mae: 0.80\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 4: train_loss=32.4541, val_loss=37.0962\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 29.6271 - mae: 0.8074 - val_loss: 35.5989 - val_mae: 0.8129\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 24.5100 - mae: 0.79\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 5: train_loss=21.5546, val_loss=23.6673\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 24.5100 - mae: 0.7938 - val_loss: 24.7273 - val_mae: 0.7870\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 21.7725 - mae: 0.78\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 6: train_loss=17.1410, val_loss=24.7358\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 21.7725 - mae: 0.7862 - val_loss: 21.1463 - val_mae: 0.7785\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 18.8555 - mae: 0.77\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 7: train_loss=17.0220, val_loss=22.7361\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 18.8555 - mae: 0.7792 - val_loss: 21.1168 - val_mae: 0.7783\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 17.6486 - mae: 0.77\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 8: train_loss=24.2587, val_loss=25.3874\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 17.6486 - mae: 0.7737 - val_loss: 26.3908 - val_mae: 0.7892\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 15.8658 - mae: 0.76\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 9: train_loss=29.9151, val_loss=27.9279\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 15.8658 - mae: 0.7682 - val_loss: 27.0091 - val_mae: 0.7889\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 14.7552 - mae: 0.76\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 10: train_loss=11.4537, val_loss=15.9787\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 14.7552 - mae: 0.7646 - val_loss: 15.9870 - val_mae: 0.7592\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 13.5992 - mae: 0.75\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 11: train_loss=11.2465, val_loss=12.7027\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 13.5992 - mae: 0.7594 - val_loss: 15.9951 - val_mae: 0.7614\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 13.0063 - mae: 0.75\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 12: train_loss=7.6374, val_loss=10.5970\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 13.0063 - mae: 0.7578 - val_loss: 11.8424 - val_mae: 0.7458\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 12.3067 - mae: 0.75\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 13: train_loss=8.6453, val_loss=15.7236\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 12.3067 - mae: 0.7545 - val_loss: 13.5217 - val_mae: 0.7520\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 11.8215 - mae: 0.75\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 14: train_loss=9.6800, val_loss=11.4681\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 11.8215 - mae: 0.7506 - val_loss: 12.6134 - val_mae: 0.7470\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 11.5571 - mae: 0.74\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 15: train_loss=25.3053, val_loss=19.7676\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 11.5571 - mae: 0.7467 - val_loss: 24.1466 - val_mae: 0.7749\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 11.0500 - mae: 0.74\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 16: train_loss=6.7592, val_loss=9.0168\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 11.0500 - mae: 0.7440 - val_loss: 10.6987 - val_mae: 0.7332\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 10.7060 - mae: 0.73\n",
      "2/2 [==============================] - 1s 34ms/step\n",
      "Epoch 17: train_loss=6.5327, val_loss=13.0111\n",
      "67/67 [==============================] - 12s 176ms/step - loss: 10.7060 - mae: 0.7390 - val_loss: 9.9769 - val_mae: 0.7273\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 10.4897 - mae: 0.73\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 18: train_loss=15.8280, val_loss=18.8625\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 10.4897 - mae: 0.7367 - val_loss: 21.0508 - val_mae: 0.7664\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 10.3414 - mae: 0.73\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 19: train_loss=7.3185, val_loss=17.1118\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 10.3414 - mae: 0.7352 - val_loss: 11.5804 - val_mae: 0.7285\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 10.1391 - mae: 0.73\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 20: train_loss=12.9417, val_loss=15.3166\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 10.1391 - mae: 0.7311 - val_loss: 19.1262 - val_mae: 0.7527\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 9.6294 - mae: 0.72\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 21: train_loss=11.3601, val_loss=16.9601\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 9.6294 - mae: 0.7265 - val_loss: 15.3256 - val_mae: 0.7458\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 9.5897 - mae: 0.72\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 22: train_loss=10.2788, val_loss=12.4088\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 9.5897 - mae: 0.7223 - val_loss: 12.4265 - val_mae: 0.7257\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 9.2314 - mae: 0.71\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 23: train_loss=13.7497, val_loss=20.6952\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 9.2314 - mae: 0.7164 - val_loss: 16.0286 - val_mae: 0.7310\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 9.0654 - mae: 0.71\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 24: train_loss=10.1328, val_loss=16.8152\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 9.0654 - mae: 0.7124 - val_loss: 12.4816 - val_mae: 0.7193\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 8.8906 - mae: 0.70\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 25: train_loss=8.1813, val_loss=16.3834\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.8906 - mae: 0.7072 - val_loss: 12.2235 - val_mae: 0.7092\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 8.5449 - mae: 0.70\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 26: train_loss=7.0887, val_loss=8.6161\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.5449 - mae: 0.7021 - val_loss: 10.1874 - val_mae: 0.6963\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 8.5212 - mae: 0.69\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 27: train_loss=5.7146, val_loss=8.8032\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.5212 - mae: 0.6967 - val_loss: 8.0505 - val_mae: 0.6736\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 8.2725 - mae: 0.68\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 28: train_loss=10.7824, val_loss=12.3971\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 8.2725 - mae: 0.6869 - val_loss: 13.5968 - val_mae: 0.6944\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 8.3769 - mae: 0.68\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 29: train_loss=16.3004, val_loss=15.9255\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.3769 - mae: 0.6813 - val_loss: 16.0259 - val_mae: 0.7183\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 8.1986 - mae: 0.67\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 30: train_loss=9.5282, val_loss=12.8998\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 8.1986 - mae: 0.6757 - val_loss: 12.0754 - val_mae: 0.6731\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 7.9121 - mae: 0.66\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 31: train_loss=7.6367, val_loss=9.2912\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 7.9121 - mae: 0.6651 - val_loss: 9.6770 - val_mae: 0.6590\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 7.5952 - mae: 0.65\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 32: train_loss=6.3905, val_loss=6.2629\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 7.5952 - mae: 0.6554 - val_loss: 8.9191 - val_mae: 0.6403\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 7.5596 - mae: 0.64\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 33: train_loss=6.6392, val_loss=8.8728\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 7.5596 - mae: 0.6459 - val_loss: 10.4803 - val_mae: 0.6425\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 7.3557 - mae: 0.63\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 34: train_loss=10.7119, val_loss=13.4203\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 7.3557 - mae: 0.6376 - val_loss: 14.7090 - val_mae: 0.6623\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 7.1935 - mae: 0.62\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 35: train_loss=5.7079, val_loss=6.4902\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 7.1935 - mae: 0.6258 - val_loss: 8.2135 - val_mae: 0.6072\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 7.0598 - mae: 0.61\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 36: train_loss=6.3859, val_loss=7.9175\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 7.0598 - mae: 0.6163 - val_loss: 8.3948 - val_mae: 0.5964\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 23ms/steploss: 6.8820 - mae: 0.60\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 37: train_loss=9.8312, val_loss=11.5375\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 6.8820 - mae: 0.6057 - val_loss: 13.6529 - val_mae: 0.6151\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 6.6499 - mae: 0.59\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 38: train_loss=16.4734, val_loss=19.2155\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 6.6499 - mae: 0.5942 - val_loss: 17.6676 - val_mae: 0.6441\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 6.5940 - mae: 0.58\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 39: train_loss=11.9217, val_loss=15.2971\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 6.5940 - mae: 0.5828 - val_loss: 16.3745 - val_mae: 0.6176\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 6.3300 - mae: 0.57\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 40: train_loss=11.6779, val_loss=16.5488\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 6.3300 - mae: 0.5735 - val_loss: 12.5900 - val_mae: 0.5919\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 6.2224 - mae: 0.56\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 41: train_loss=10.3899, val_loss=17.7874\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 6.2224 - mae: 0.5613 - val_loss: 16.0511 - val_mae: 0.5898\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 6.0676 - mae: 0.55\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 42: train_loss=5.0702, val_loss=6.5557\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 6.0676 - mae: 0.5518 - val_loss: 7.0802 - val_mae: 0.5239\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 6.0684 - mae: 0.53\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 43: train_loss=4.0720, val_loss=5.7396\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 6.0684 - mae: 0.5396 - val_loss: 6.3002 - val_mae: 0.5027\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 5.7416 - mae: 0.52\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 44: train_loss=4.7446, val_loss=9.6199\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 5.7416 - mae: 0.5267 - val_loss: 7.3679 - val_mae: 0.4935\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 5.7193 - mae: 0.51\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 45: train_loss=25.5898, val_loss=22.9235\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 5.7193 - mae: 0.5146 - val_loss: 27.0064 - val_mae: 0.6338\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 5.5987 - mae: 0.49\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 46: train_loss=9.6910, val_loss=14.4533\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 5.5987 - mae: 0.4990 - val_loss: 14.1838 - val_mae: 0.5392\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 5.3844 - mae: 0.48\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 47: train_loss=3.6327, val_loss=7.9410\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 5.3844 - mae: 0.4899 - val_loss: 6.0047 - val_mae: 0.4490\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 5.2956 - mae: 0.47\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 48: train_loss=12.0464, val_loss=13.7121\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 5.2956 - mae: 0.4788 - val_loss: 12.3152 - val_mae: 0.5243\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 5.1177 - mae: 0.47\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 49: train_loss=6.7521, val_loss=5.7683\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 5.1177 - mae: 0.4709 - val_loss: 9.4402 - val_mae: 0.4493\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.9490 - mae: 0.45\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 50: train_loss=4.4555, val_loss=6.1700\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.9490 - mae: 0.4558 - val_loss: 7.1543 - val_mae: 0.4287\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.7640 - mae: 0.44\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 51: train_loss=3.0691, val_loss=9.6672\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 4.7640 - mae: 0.4415 - val_loss: 5.7587 - val_mae: 0.3986\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 4.6780 - mae: 0.43\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 52: train_loss=6.8766, val_loss=8.0941\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.6780 - mae: 0.4335 - val_loss: 9.9832 - val_mae: 0.4506\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.7674 - mae: 0.42\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 53: train_loss=5.9529, val_loss=7.7638\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 4.7674 - mae: 0.4260 - val_loss: 7.0565 - val_mae: 0.3932\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.6617 - mae: 0.41\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 54: train_loss=4.2487, val_loss=5.6807\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 4.6617 - mae: 0.4183 - val_loss: 6.2227 - val_mae: 0.3952\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 4.5868 - mae: 0.41\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 55: train_loss=71.0252, val_loss=60.2258\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.5868 - mae: 0.4114 - val_loss: 53.4260 - val_mae: 0.8476\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.5287 - mae: 0.40\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 56: train_loss=4.9577, val_loss=7.4185\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 4.5287 - mae: 0.4064 - val_loss: 8.3570 - val_mae: 0.3819\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 4.3166 - mae: 0.39\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 57: train_loss=5.3151, val_loss=14.0522\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 4.3166 - mae: 0.3980 - val_loss: 7.9692 - val_mae: 0.4095\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 4.1603 - mae: 0.39\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 58: train_loss=2.6965, val_loss=5.2073\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.1603 - mae: 0.3913 - val_loss: 4.7385 - val_mae: 0.3277\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 4.1891 - mae: 0.38\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 59: train_loss=6.4670, val_loss=11.9162\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 4.1891 - mae: 0.3882 - val_loss: 9.5320 - val_mae: 0.4184\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 4.1958 - mae: 0.38\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 60: train_loss=2.4262, val_loss=3.2444\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.1958 - mae: 0.3880 - val_loss: 4.5893 - val_mae: 0.3131\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.9458 - mae: 0.37\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 61: train_loss=7.9311, val_loss=15.1498\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.9458 - mae: 0.3772 - val_loss: 13.9302 - val_mae: 0.4253\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 4.1086 - mae: 0.37\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 62: train_loss=15.2306, val_loss=10.7065\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 4.1086 - mae: 0.3739 - val_loss: 13.2646 - val_mae: 0.4204\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.9119 - mae: 0.36\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 63: train_loss=11.0368, val_loss=7.8774\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.9119 - mae: 0.3697 - val_loss: 9.3736 - val_mae: 0.3805\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.7700 - mae: 0.36\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 64: train_loss=3.5904, val_loss=7.4482\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 3.7700 - mae: 0.3683 - val_loss: 5.5067 - val_mae: 0.3298\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.7777 - mae: 0.36\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 65: train_loss=21.5934, val_loss=25.7339\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.7777 - mae: 0.3629 - val_loss: 18.9015 - val_mae: 0.5020\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.6694 - mae: 0.36\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 66: train_loss=16.5636, val_loss=11.7000\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.6694 - mae: 0.3628 - val_loss: 12.8528 - val_mae: 0.3968\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 31ms/steploss: 3.6685 - mae: 0.35\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 67: train_loss=3.0077, val_loss=4.1895\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.6685 - mae: 0.3554 - val_loss: 5.2265 - val_mae: 0.3271\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.6360 - mae: 0.35\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 68: train_loss=9.1710, val_loss=9.8049\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.6360 - mae: 0.3539 - val_loss: 13.1080 - val_mae: 0.3966\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.5028 - mae: 0.34\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 69: train_loss=13.1239, val_loss=11.9382\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.5028 - mae: 0.3493 - val_loss: 14.2907 - val_mae: 0.4224\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 3.4843 - mae: 0.34\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 70: train_loss=2.9830, val_loss=4.6752\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.4843 - mae: 0.3441 - val_loss: 5.6579 - val_mae: 0.3121\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 22ms/steploss: 3.4360 - mae: 0.34\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 71: train_loss=11.6000, val_loss=6.3268\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.4360 - mae: 0.3427 - val_loss: 8.4193 - val_mae: 0.3635\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.3872 - mae: 0.34\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 72: train_loss=11.5780, val_loss=18.0091\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.3872 - mae: 0.3426 - val_loss: 15.8163 - val_mae: 0.4194\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.4378 - mae: 0.33\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 73: train_loss=1.7516, val_loss=2.7737\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.4378 - mae: 0.3372 - val_loss: 3.6595 - val_mae: 0.2810\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 3.3252 - mae: 0.33\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 74: train_loss=2.9456, val_loss=7.2676\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.3252 - mae: 0.3336 - val_loss: 4.6294 - val_mae: 0.2684\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.2713 - mae: 0.32\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 75: train_loss=1.9348, val_loss=2.7514\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.2713 - mae: 0.3289 - val_loss: 3.3581 - val_mae: 0.2549\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.2530 - mae: 0.32\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 76: train_loss=13.9649, val_loss=22.8159\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.2530 - mae: 0.3294 - val_loss: 21.9446 - val_mae: 0.5206\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 3.3269 - mae: 0.32\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 77: train_loss=3.5668, val_loss=4.6711\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.3269 - mae: 0.3251 - val_loss: 6.6715 - val_mae: 0.3294\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.2293 - mae: 0.32\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 78: train_loss=14.0778, val_loss=13.4856\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.2293 - mae: 0.3234 - val_loss: 15.1560 - val_mae: 0.4236\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 3.1091 - mae: 0.32\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 79: train_loss=19.7411, val_loss=11.9321\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.1091 - mae: 0.3228 - val_loss: 15.2117 - val_mae: 0.4020\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.1543 - mae: 0.32\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 80: train_loss=7.5181, val_loss=5.8864\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.1543 - mae: 0.3201 - val_loss: 6.4308 - val_mae: 0.3061\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 3.0487 - mae: 0.31\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 81: train_loss=4.2003, val_loss=5.8854\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 3.0487 - mae: 0.3150 - val_loss: 5.0860 - val_mae: 0.2680\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 3.1537 - mae: 0.30\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 82: train_loss=5.9792, val_loss=15.2794\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.1537 - mae: 0.3098 - val_loss: 8.4894 - val_mae: 0.3589\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.1732 - mae: 0.31\n",
      "2/2 [==============================] - 0s 20ms/step\n",
      "Epoch 83: train_loss=5.8651, val_loss=9.5141\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 3.1732 - mae: 0.3162 - val_loss: 14.1845 - val_mae: 0.3563\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.2000 - mae: 0.31\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 84: train_loss=37.6206, val_loss=45.3551\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.2000 - mae: 0.3169 - val_loss: 43.0719 - val_mae: 0.5322\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 3.4738 - mae: 0.31\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 85: train_loss=2.6916, val_loss=5.8122\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.4738 - mae: 0.3186 - val_loss: 6.2805 - val_mae: 0.2841\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.1590 - mae: 0.31\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 86: train_loss=2.5225, val_loss=2.4661\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.1590 - mae: 0.3143 - val_loss: 3.9334 - val_mae: 0.2450\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 3.1275 - mae: 0.30\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 87: train_loss=39.0725, val_loss=60.6503\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.1275 - mae: 0.3070 - val_loss: 51.6382 - val_mae: 0.7003\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 23ms/steploss: 3.0295 - mae: 0.31\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 88: train_loss=9.0328, val_loss=10.5391\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 3.0295 - mae: 0.3106 - val_loss: 10.0744 - val_mae: 0.3683\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.9015 - mae: 0.30\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 89: train_loss=1.8415, val_loss=5.5303\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.9015 - mae: 0.3099 - val_loss: 3.6570 - val_mae: 0.2403\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 3.0370 - mae: 0.30\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 90: train_loss=2.9574, val_loss=3.9826\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 3.0370 - mae: 0.3045 - val_loss: 5.3310 - val_mae: 0.2984\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.8664 - mae: 0.30\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 91: train_loss=5.7946, val_loss=11.4331\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.8664 - mae: 0.3043 - val_loss: 6.6416 - val_mae: 0.3058\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.7821 - mae: 0.29\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 92: train_loss=21.7111, val_loss=30.9251\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7821 - mae: 0.2994 - val_loss: 31.1717 - val_mae: 0.5936\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.7297 - mae: 0.29\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 93: train_loss=1.3617, val_loss=1.7287\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7297 - mae: 0.2984 - val_loss: 2.8772 - val_mae: 0.2282\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.7444 - mae: 0.29\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 94: train_loss=75.0478, val_loss=68.4110\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7444 - mae: 0.2914 - val_loss: 76.9986 - val_mae: 0.8360\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.8231 - mae: 0.29\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 95: train_loss=3.4184, val_loss=3.9094\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.8231 - mae: 0.2930 - val_loss: 5.6803 - val_mae: 0.2953\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.7202 - mae: 0.29\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 96: train_loss=4.8289, val_loss=3.4828\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.7202 - mae: 0.2902 - val_loss: 4.8049 - val_mae: 0.2787\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 2.7978 - mae: 0.29\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 97: train_loss=3.3587, val_loss=4.8171\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7978 - mae: 0.2909 - val_loss: 5.3666 - val_mae: 0.2883\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.7195 - mae: 0.29\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 98: train_loss=5.5764, val_loss=6.4747\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7195 - mae: 0.2911 - val_loss: 7.6324 - val_mae: 0.3074\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.6911 - mae: 0.29\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 99: train_loss=1.4261, val_loss=13.8044\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.6911 - mae: 0.2906 - val_loss: 4.1063 - val_mae: 0.2302\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.6149 - mae: 0.28\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 100: train_loss=1.1707, val_loss=1.6874\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.6149 - mae: 0.2865 - val_loss: 2.8443 - val_mae: 0.2093\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.6633 - mae: 0.28\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 101: train_loss=7.1850, val_loss=6.1670\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.6633 - mae: 0.2848 - val_loss: 7.8538 - val_mae: 0.2539\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.6312 - mae: 0.28\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 102: train_loss=3.4062, val_loss=6.7014\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.6312 - mae: 0.2843 - val_loss: 5.3090 - val_mae: 0.2766\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.6040 - mae: 0.28\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 103: train_loss=1.0633, val_loss=1.2879\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.6040 - mae: 0.2832 - val_loss: 2.3910 - val_mae: 0.1860\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.5522 - mae: 0.27\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 104: train_loss=1.0662, val_loss=1.5732\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.5522 - mae: 0.2784 - val_loss: 2.5473 - val_mae: 0.1841\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.5405 - mae: 0.28\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 105: train_loss=1.2238, val_loss=1.4519\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.5405 - mae: 0.2822 - val_loss: 2.5863 - val_mae: 0.2079\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.5659 - mae: 0.28\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 106: train_loss=10.5848, val_loss=11.4696\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.5659 - mae: 0.2814 - val_loss: 12.3310 - val_mae: 0.3794\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.4824 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 107: train_loss=1.0900, val_loss=1.5766\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.4824 - mae: 0.2751 - val_loss: 2.1491 - val_mae: 0.1911\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.4470 - mae: 0.27\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 108: train_loss=3.9191, val_loss=11.5222\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.4470 - mae: 0.2789 - val_loss: 7.4150 - val_mae: 0.2706\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 2.4737 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 109: train_loss=5.2646, val_loss=13.9486\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.4737 - mae: 0.2774 - val_loss: 9.6046 - val_mae: 0.2974\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.4091 - mae: 0.27\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 110: train_loss=6.7071, val_loss=10.9177\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.4091 - mae: 0.2757 - val_loss: 9.6948 - val_mae: 0.3163\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 2.7400 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 111: train_loss=78.9762, val_loss=64.4501\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.7400 - mae: 0.2739 - val_loss: 73.0998 - val_mae: 0.7909\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 2.8487 - mae: 0.28\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 112: train_loss=7.6628, val_loss=6.0115\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.8487 - mae: 0.2802 - val_loss: 11.0865 - val_mae: 0.3463\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.8340 - mae: 0.28\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 113: train_loss=24.2561, val_loss=22.2514\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 2.8340 - mae: 0.2812 - val_loss: 28.5755 - val_mae: 0.5647\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.7656 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 114: train_loss=30.1319, val_loss=42.4803\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.7656 - mae: 0.2765 - val_loss: 36.7867 - val_mae: 0.6595\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 2.6819 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 115: train_loss=0.7480, val_loss=6.3155\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.6819 - mae: 0.2756 - val_loss: 2.3159 - val_mae: 0.1815\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.3961 - mae: 0.27\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 116: train_loss=8.1566, val_loss=15.3820\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.3961 - mae: 0.2740 - val_loss: 10.0340 - val_mae: 0.3556\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.4098 - mae: 0.27\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 117: train_loss=2.0970, val_loss=2.5732\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.4098 - mae: 0.2720 - val_loss: 3.7264 - val_mae: 0.2355\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.3488 - mae: 0.26\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 118: train_loss=14.4585, val_loss=15.7204\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.3488 - mae: 0.2672 - val_loss: 15.8792 - val_mae: 0.3902\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.3427 - mae: 0.26\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 119: train_loss=1.2809, val_loss=9.0537\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.3427 - mae: 0.2697 - val_loss: 3.1525 - val_mae: 0.2253\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 2.2533 - mae: 0.26\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 120: train_loss=0.7188, val_loss=1.1199\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.2533 - mae: 0.2632 - val_loss: 2.3419 - val_mae: 0.1708\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.2358 - mae: 0.26\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 121: train_loss=1.3462, val_loss=1.7197\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.2358 - mae: 0.2614 - val_loss: 4.0549 - val_mae: 0.2103\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.2396 - mae: 0.26\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 122: train_loss=3.2635, val_loss=4.6763\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.2396 - mae: 0.2625 - val_loss: 6.4401 - val_mae: 0.2946\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.2236 - mae: 0.26\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 123: train_loss=25.4394, val_loss=28.6368\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.2236 - mae: 0.2607 - val_loss: 25.4697 - val_mae: 0.5497\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.3383 - mae: 0.26\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 124: train_loss=18.9139, val_loss=20.5655\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.3383 - mae: 0.2644 - val_loss: 23.1270 - val_mae: 0.5398\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 2.6922 - mae: 0.26\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 125: train_loss=17.6196, val_loss=38.2178\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.6922 - mae: 0.2682 - val_loss: 22.8525 - val_mae: 0.4005\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.4485 - mae: 0.26\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 126: train_loss=28.3191, val_loss=21.6396\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.4485 - mae: 0.2658 - val_loss: 32.5847 - val_mae: 0.5715\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.2635 - mae: 0.26\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 127: train_loss=1.2059, val_loss=3.8166\n",
      "67/67 [==============================] - 11s 161ms/step - loss: 2.2635 - mae: 0.2639 - val_loss: 2.5063 - val_mae: 0.1946\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.2117 - mae: 0.26\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 128: train_loss=10.2136, val_loss=7.9655\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.2117 - mae: 0.2622 - val_loss: 10.7354 - val_mae: 0.3111\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.2054 - mae: 0.26\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 129: train_loss=2.0623, val_loss=8.9478\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.2054 - mae: 0.2607 - val_loss: 3.3329 - val_mae: 0.2257\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.0933 - mae: 0.25\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 130: train_loss=6.0572, val_loss=6.2518\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.0933 - mae: 0.2528 - val_loss: 12.8015 - val_mae: 0.4128\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.0724 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 131: train_loss=1.3562, val_loss=1.7118\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.0724 - mae: 0.2573 - val_loss: 2.8613 - val_mae: 0.2122\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.0869 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 132: train_loss=1.6986, val_loss=3.4030\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.0869 - mae: 0.2559 - val_loss: 3.8700 - val_mae: 0.2154\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 21ms/steploss: 2.0280 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 133: train_loss=0.7449, val_loss=5.7538\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.0280 - mae: 0.2546 - val_loss: 2.1767 - val_mae: 0.1718\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 21ms/steploss: 2.0520 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 134: train_loss=4.6866, val_loss=5.9433\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.0520 - mae: 0.2539 - val_loss: 9.4648 - val_mae: 0.2900\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 2.1016 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 135: train_loss=2.4955, val_loss=7.3538\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.1016 - mae: 0.2527 - val_loss: 4.8852 - val_mae: 0.2609\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.1499 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 136: train_loss=1.1987, val_loss=6.5239\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.1499 - mae: 0.2543 - val_loss: 3.2970 - val_mae: 0.1964\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.1397 - mae: 0.25\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 137: train_loss=76.3701, val_loss=65.4995\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.1397 - mae: 0.2554 - val_loss: 57.5272 - val_mae: 0.6579\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 2.1475 - mae: 0.25\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 138: train_loss=18.9166, val_loss=25.4025\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.1475 - mae: 0.2532 - val_loss: 16.7470 - val_mae: 0.3656\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.0173 - mae: 0.25\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 139: train_loss=1.0182, val_loss=1.0339\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.0173 - mae: 0.2533 - val_loss: 4.9251 - val_mae: 0.1911\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 2.0725 - mae: 0.25\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 140: train_loss=1.1246, val_loss=6.5825\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 2.0725 - mae: 0.2517 - val_loss: 2.8669 - val_mae: 0.2120\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.1319 - mae: 0.25\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 141: train_loss=52.1730, val_loss=34.5034\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 2.1319 - mae: 0.2511 - val_loss: 41.1300 - val_mae: 0.5582\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.0616 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 142: train_loss=3.1588, val_loss=8.3375\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.0616 - mae: 0.2488 - val_loss: 5.4807 - val_mae: 0.2974\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.9554 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 143: train_loss=0.5566, val_loss=0.8454\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.9554 - mae: 0.2492 - val_loss: 1.6889 - val_mae: 0.1613\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.9711 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 144: train_loss=1.0186, val_loss=1.3808\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.9711 - mae: 0.2482 - val_loss: 2.1194 - val_mae: 0.1791\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 1.9649 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 145: train_loss=3.8229, val_loss=1.5488\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.9649 - mae: 0.2460 - val_loss: 4.4879 - val_mae: 0.2201\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.9047 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 146: train_loss=4.7188, val_loss=4.4422\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.9047 - mae: 0.2456 - val_loss: 5.9853 - val_mae: 0.3110\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.9145 - mae: 0.24\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 147: train_loss=2.1364, val_loss=3.2682\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.9145 - mae: 0.2453 - val_loss: 4.3319 - val_mae: 0.2374\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8720 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 148: train_loss=0.8773, val_loss=1.4711\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8720 - mae: 0.2433 - val_loss: 2.3637 - val_mae: 0.1829\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.9553 - mae: 0.24\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 149: train_loss=2.4339, val_loss=7.7658\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.9553 - mae: 0.2473 - val_loss: 4.3100 - val_mae: 0.2759\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.8827 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 150: train_loss=1.2107, val_loss=13.1931\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.8827 - mae: 0.2430 - val_loss: 3.8185 - val_mae: 0.2092\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.8595 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 151: train_loss=4.6977, val_loss=1.4851\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.8595 - mae: 0.2421 - val_loss: 3.8347 - val_mae: 0.1845\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.8947 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 152: train_loss=40.9236, val_loss=45.4021\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8947 - mae: 0.2440 - val_loss: 46.3708 - val_mae: 0.6941\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.8727 - mae: 0.24\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 153: train_loss=1.1848, val_loss=1.6420\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8727 - mae: 0.2440 - val_loss: 3.1548 - val_mae: 0.2106\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.8907 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 154: train_loss=13.7773, val_loss=21.2238\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8907 - mae: 0.2421 - val_loss: 23.3274 - val_mae: 0.4138\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8067 - mae: 0.24\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 155: train_loss=0.8112, val_loss=1.2350\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.8067 - mae: 0.2410 - val_loss: 2.2331 - val_mae: 0.1827\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8040 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 156: train_loss=0.4064, val_loss=0.6379\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 1.8040 - mae: 0.2401 - val_loss: 1.8115 - val_mae: 0.1376\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.7694 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 157: train_loss=2.4805, val_loss=8.6982\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.7694 - mae: 0.2358 - val_loss: 5.4824 - val_mae: 0.2548\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 22ms/steploss: 1.8230 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 158: train_loss=2.9625, val_loss=3.6964\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8230 - mae: 0.2394 - val_loss: 6.1276 - val_mae: 0.2568\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.7964 - mae: 0.23\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 159: train_loss=106.8409, val_loss=128.0173\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.7964 - mae: 0.2373 - val_loss: 117.9583 - val_mae: 0.9434\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8341 - mae: 0.23\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 160: train_loss=153.4494, val_loss=133.4026\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.8341 - mae: 0.2370 - val_loss: 150.4886 - val_mae: 0.9926\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 2.5172 - mae: 0.25\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 161: train_loss=7.5911, val_loss=14.9506\n",
      "67/67 [==============================] - 11s 165ms/step - loss: 2.5172 - mae: 0.2500 - val_loss: 12.8867 - val_mae: 0.3899\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 2.0699 - mae: 0.24\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 162: train_loss=7.9418, val_loss=13.3322\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.0699 - mae: 0.2458 - val_loss: 15.6943 - val_mae: 0.3877\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 21ms/steploss: 2.1079 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 163: train_loss=1.5108, val_loss=9.6402\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 2.1079 - mae: 0.2442 - val_loss: 3.9424 - val_mae: 0.1983\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8699 - mae: 0.24\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 164: train_loss=10.7906, val_loss=4.0391\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.8699 - mae: 0.2422 - val_loss: 7.0472 - val_mae: 0.2846\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.7924 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 165: train_loss=1.0203, val_loss=1.5970\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.7924 - mae: 0.2385 - val_loss: 4.6319 - val_mae: 0.1936\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.7864 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 166: train_loss=0.4843, val_loss=3.2821\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.7864 - mae: 0.2369 - val_loss: 1.3571 - val_mae: 0.1355\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.8048 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 167: train_loss=18.7333, val_loss=12.8405\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8048 - mae: 0.2347 - val_loss: 14.3561 - val_mae: 0.3780\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 23ms/steploss: 1.8080 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 168: train_loss=6.3391, val_loss=5.1852\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.8080 - mae: 0.2362 - val_loss: 9.0932 - val_mae: 0.2799\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.7622 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 169: train_loss=0.4901, val_loss=0.8170\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.7622 - mae: 0.2388 - val_loss: 1.3213 - val_mae: 0.1554\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 1.7100 - mae: 0.23\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 170: train_loss=0.7148, val_loss=3.9865\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.7100 - mae: 0.2336 - val_loss: 1.9763 - val_mae: 0.1759\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.6968 - mae: 0.23\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 171: train_loss=0.5737, val_loss=0.7767\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.6968 - mae: 0.2325 - val_loss: 1.4778 - val_mae: 0.1480\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.6676 - mae: 0.23\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 172: train_loss=0.3456, val_loss=0.7130\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.6676 - mae: 0.2305 - val_loss: 1.5717 - val_mae: 0.1482\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 19ms/steploss: 1.6809 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 173: train_loss=1.1602, val_loss=1.9464\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.6809 - mae: 0.2297 - val_loss: 2.5918 - val_mae: 0.1889\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.6413 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 174: train_loss=0.5584, val_loss=1.6218\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.6413 - mae: 0.2280 - val_loss: 1.5815 - val_mae: 0.1622\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.6157 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 175: train_loss=0.6219, val_loss=1.3478\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.6157 - mae: 0.2288 - val_loss: 1.7951 - val_mae: 0.1591\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.6030 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 176: train_loss=0.9116, val_loss=4.3135\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.6030 - mae: 0.2288 - val_loss: 2.5435 - val_mae: 0.1759\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 13ms/steploss: 1.6360 - mae: 0.22\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 177: train_loss=1.2343, val_loss=2.7413\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.6360 - mae: 0.2276 - val_loss: 2.4307 - val_mae: 0.2056\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.6093 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 178: train_loss=0.3519, val_loss=0.6675\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.6093 - mae: 0.2285 - val_loss: 1.3497 - val_mae: 0.1488\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.6128 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 179: train_loss=1.3326, val_loss=1.6134\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.6128 - mae: 0.2268 - val_loss: 3.8545 - val_mae: 0.2160\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.6022 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 180: train_loss=0.4223, val_loss=0.6775\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.6022 - mae: 0.2267 - val_loss: 1.0729 - val_mae: 0.1342\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5587 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 181: train_loss=0.6109, val_loss=0.8139\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5587 - mae: 0.2242 - val_loss: 2.4037 - val_mae: 0.1732\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5660 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 182: train_loss=0.3214, val_loss=0.5356\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5660 - mae: 0.2272 - val_loss: 1.4106 - val_mae: 0.1389\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.5308 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 183: train_loss=4.1604, val_loss=3.6026\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5308 - mae: 0.2217 - val_loss: 5.6083 - val_mae: 0.2503\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5561 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 184: train_loss=0.4329, val_loss=5.5291\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5561 - mae: 0.2256 - val_loss: 1.4070 - val_mae: 0.1341\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 20ms/steploss: 1.5435 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 185: train_loss=0.3161, val_loss=0.4920\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5435 - mae: 0.2222 - val_loss: 1.6199 - val_mae: 0.1368\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 15ms/steploss: 1.5330 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 186: train_loss=0.7864, val_loss=1.3355\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.5330 - mae: 0.2244 - val_loss: 1.7447 - val_mae: 0.1706\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 25ms/steploss: 1.6441 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 187: train_loss=1.4222, val_loss=5.4264\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.6441 - mae: 0.2258 - val_loss: 5.4040 - val_mae: 0.2252\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5797 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 188: train_loss=0.4063, val_loss=0.6714\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5797 - mae: 0.2251 - val_loss: 2.0307 - val_mae: 0.1534\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5407 - mae: 0.22\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 189: train_loss=1.8082, val_loss=3.3226\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5407 - mae: 0.2224 - val_loss: 3.5859 - val_mae: 0.2258\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 16ms/steploss: 1.5700 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 190: train_loss=0.5984, val_loss=0.6100\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5700 - mae: 0.2247 - val_loss: 2.1256 - val_mae: 0.1507\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5384 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 191: train_loss=0.4235, val_loss=1.4867\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5384 - mae: 0.2235 - val_loss: 1.4950 - val_mae: 0.1456\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.5335 - mae: 0.22\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 192: train_loss=1.3543, val_loss=4.2431\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5335 - mae: 0.2242 - val_loss: 4.8019 - val_mae: 0.2231\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.5290 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 193: train_loss=0.4091, val_loss=1.5489\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.5290 - mae: 0.2213 - val_loss: 1.3458 - val_mae: 0.1339\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.5053 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 194: train_loss=11.9610, val_loss=11.3010\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.5053 - mae: 0.2219 - val_loss: 13.9242 - val_mae: 0.3695\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.4786 - mae: 0.22\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 195: train_loss=1.0238, val_loss=4.4666\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.4786 - mae: 0.2213 - val_loss: 3.0062 - val_mae: 0.1946\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.4637 - mae: 0.21\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 196: train_loss=0.6785, val_loss=0.9767\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.4637 - mae: 0.2173 - val_loss: 1.5208 - val_mae: 0.1500\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.4913 - mae: 0.21\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "Epoch 197: train_loss=6.0517, val_loss=8.6085\n",
      "67/67 [==============================] - 11s 162ms/step - loss: 1.4913 - mae: 0.2195 - val_loss: 11.0572 - val_mae: 0.3762\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.4408 - mae: 0.21\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "Epoch 198: train_loss=0.5440, val_loss=0.7144\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.4408 - mae: 0.2196 - val_loss: 1.8890 - val_mae: 0.1499\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 18ms/steploss: 1.4715 - mae: 0.21\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Epoch 199: train_loss=0.5355, val_loss=0.6993\n",
      "67/67 [==============================] - 11s 164ms/step - loss: 1.4715 - mae: 0.2183 - val_loss: 1.7765 - val_mae: 0.1542\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 17ms/steploss: 1.4290 - mae: 0.21\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "Epoch 200: train_loss=3.7102, val_loss=7.5672\n",
      "67/67 [==============================] - 11s 163ms/step - loss: 1.4290 - mae: 0.2141 - val_loss: 6.6197 - val_mae: 0.2413\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "# ----------- GPU Configuration -----------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# ----------- 1. Load all data for training -----------\n",
    "train_df = pd.read_csv('./csbf3nvm/NVM23_train_split.txt', delimiter=' ',\n",
    "                       names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "val_df = pd.read_csv('./csbf3nvm/NVM23_val_split.txt', delimiter=' ',\n",
    "                     names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ----------- 2. Define custom loss function -----------\n",
    "def custom_loss(y_true, y_pred):\n",
    "    beta = 400.0\n",
    "    x_pred = y_pred[:, :3]\n",
    "    q_pred = y_pred[:, 3:]\n",
    "    x_true = y_true[:, :3]\n",
    "    q_true = y_true[:, 3:]\n",
    "\n",
    "    q_norm = K.sqrt(K.sum(K.square(q_true), axis=-1, keepdims=True)) + K.epsilon()\n",
    "    q_true_normed = q_true / q_norm\n",
    "\n",
    "    position_loss = K.mean(K.square(x_pred - x_true), axis=-1)\n",
    "    quaternion_loss = K.mean(K.square(q_pred - q_true_normed), axis=-1)\n",
    "\n",
    "    return position_loss + beta * quaternion_loss\n",
    "\n",
    "# ----------- 3. Build EfficientNetV2B0 model -----------\n",
    "base_model = EfficientNetV2B0(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(7)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# ----------- 4. Compile model -----------\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=custom_loss,\n",
    "              metrics=['mae'])\n",
    "model.summary()\n",
    "# ----------- 5. Custom callback (updated to log both train and val loss) -----------\n",
    "class TrainValLossLogger(Callback):\n",
    "    def __init__(self, train_gen, val_gen, output_file='loss_EfficientNetV2B0.csv'):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.output_file = output_file\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        def compute_losses(gen):\n",
    "            x, y_true = next(iter(gen))\n",
    "            y_pred = self.model.predict(x)\n",
    "            x_pred, q_pred = y_pred[:, :3], y_pred[:, 3:]\n",
    "            x_true, q_true = y_true[:, :3], y_true[:, 3:]\n",
    "            q_true_normed = q_true / (np.linalg.norm(q_true, axis=1, keepdims=True) + 1e-7)\n",
    "            pos_loss = np.mean(np.square(x_pred - x_true))\n",
    "            quat_loss = np.mean(np.square(q_pred - q_true_normed))\n",
    "            total_loss = pos_loss + 400. * quat_loss\n",
    "            return total_loss, pos_loss, quat_loss\n",
    "\n",
    "        train_total, train_pos, train_quat = compute_losses(self.train_gen)\n",
    "        val_total, val_pos, val_quat = compute_losses(self.val_gen)\n",
    "\n",
    "        self.logs.append({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Train_loss': train_total,\n",
    "            'Train_pos_loss': train_pos,\n",
    "            'Train_quat_loss': train_quat,\n",
    "            'Val_loss': val_total,\n",
    "            'Val_pos_loss': val_pos,\n",
    "            'Val_quat_loss': val_quat\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(self.logs).to_csv(self.output_file, index=False)\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_total:.4f}, val_loss={val_total:.4f}\")\n",
    "\n",
    "# ----------- 6. Callbacks (Remove checkpoint) -----------\n",
    "loss_logger = TrainValLossLogger(train_generator, val_generator)\n",
    "\n",
    "# ----------- 7. Train with validation -----------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=200,\n",
    "    callbacks=[loss_logger]\n",
    ")\n",
    "\n",
    "# ----------- 8. Save final model (last epoch) -----------\n",
    "model.save('final_model_EfficientNetV2B0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "# ----------- GPU Configuration -----------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# ----------- 1. Load all data for training -----------\n",
    "train_df = pd.read_csv('./csbf3nvm/NVM23_train_split.txt', delimiter=' ',\n",
    "                       names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "val_df = pd.read_csv('./csbf3nvm/NVM23_val_split.txt', delimiter=' ',\n",
    "                     names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ----------- 2. Define custom loss function -----------\n",
    "def custom_loss(y_true, y_pred):\n",
    "    beta = 400.0\n",
    "    x_pred = y_pred[:, :3]\n",
    "    q_pred = y_pred[:, 3:]\n",
    "    x_true = y_true[:, :3]\n",
    "    q_true = y_true[:, 3:]\n",
    "\n",
    "    q_norm = K.sqrt(K.sum(K.square(q_true), axis=-1, keepdims=True)) + K.epsilon()\n",
    "    q_true_normed = q_true / q_norm\n",
    "\n",
    "    position_loss = K.mean(K.square(x_pred - x_true), axis=-1)\n",
    "    quaternion_loss = K.mean(K.square(q_pred - q_true_normed), axis=-1)\n",
    "\n",
    "    return position_loss + beta * quaternion_loss\n",
    "\n",
    "# ----------- 3. Build InceptionV3 model -----------\n",
    "base_model = InceptionV3(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x) \n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(7)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# ----------- 4. Compile model -----------\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=custom_loss,\n",
    "              metrics=['mae'])\n",
    "model.summary()\n",
    "class TrainValLossLogger(Callback):\n",
    "    def __init__(self, train_gen, val_gen, output_file='loss_InceptionV3_.csv'):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.output_file = output_file\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        def compute_losses(gen):\n",
    "            x, y_true = next(iter(gen))\n",
    "            y_pred = self.model.predict(x)\n",
    "            x_pred, q_pred = y_pred[:, :3], y_pred[:, 3:]\n",
    "            x_true, q_true = y_true[:, :3], y_true[:, 3:]\n",
    "            q_true_normed = q_true / (np.linalg.norm(q_true, axis=1, keepdims=True) + 1e-7)\n",
    "            pos_loss = np.mean(np.square(x_pred - x_true))\n",
    "            quat_loss = np.mean(np.square(q_pred - q_true_normed))\n",
    "            total_loss = pos_loss + 400. * quat_loss\n",
    "            return total_loss, pos_loss, quat_loss\n",
    "\n",
    "        train_total, train_pos, train_quat = compute_losses(self.train_gen)\n",
    "        val_total, val_pos, val_quat = compute_losses(self.val_gen)\n",
    "\n",
    "        self.logs.append({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Train_loss': train_total,\n",
    "            'Train_pos_loss': train_pos,\n",
    "            'Train_quat_loss': train_quat,\n",
    "            'Val_loss': val_total,\n",
    "            'Val_pos_loss': val_pos,\n",
    "            'Val_quat_loss': val_quat\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(self.logs).to_csv(self.output_file, index=False)\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_total:.4f}, val_loss={val_total:.4f}\")\n",
    "\n",
    "# ----------- 6. Callbacks -----------\n",
    "loss_logger = TrainValLossLogger(train_generator, val_generator)\n",
    "\n",
    "# ----------- 7. Train with validation -----------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=200,\n",
    "    callbacks=[loss_logger]\n",
    ")\n",
    "\n",
    "model.save('final_model_InceptionV3_.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    beta = 400\n",
    "    x_pred = y_pred[:, :3]\n",
    "    q_pred = y_pred[:, 3:]\n",
    "    x_true = y_true[:, :3]\n",
    "    q_true = y_true[:, 3:]\n",
    "    \n",
    "    q_norm = K.sqrt(K.sum(K.square(q_true), axis=-1, keepdims=True))\n",
    "    q_true_normed = q_true / q_norm\n",
    "    \n",
    "    return K.mean(K.square(x_pred - x_true), axis=-1) + beta * K.mean(K.square(q_pred - q_true_normed), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best_CNN_FIX_model.h5', custom_objects={'custom_loss': custom_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 validated image filenames.\n",
      " 19/592 [..............................] - ETA: 27sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 592 batches). You may need to use the repeat() function when building your dataset.\n",
      "592/592 [==============================] - 4s 1ms/step\n",
      "Mean Squared Error: 0.06549714834953776\n",
      "Image: seq_val/S_2_3_116_0.jpg\n",
      "Actual   : [0.0212611942306 -0.150198364279 -4.02815241156 0.992784597423\n",
      " -0.0231174873332 0.116162444327 0.0187246209027]\n",
      "Predicted: [-0.35580921173095703 -0.11551351845264435 -4.122383117675781\n",
      " 0.9832386374473572 -0.006837533786892891 0.07170430570840836\n",
      " 0.022308899089694023]\n",
      "\n",
      "Image: seq_val/IMG_9804_0.jpg\n",
      "Actual   : [-3.66592251994 1.02639878824 2.82205127708 0.949369820332 0.0210068530886\n",
      " -0.313457549781 -0.000144397914525]\n",
      "Predicted: [-3.2417845726013184 0.7269228100776672 2.040851593017578\n",
      " 0.930647075176239 -0.0058696214109659195 -0.28249043226242065\n",
      " -7.445248775184155e-05]\n",
      "\n",
      "Image: seq_val/IMG_9862_0.jpg\n",
      "Actual   : [4.38230986374 0.392425569864 3.84121422039 0.452808276381\n",
      " -0.0231972306729 -0.89107694329 -0.0202097616994]\n",
      "Predicted: [4.204751014709473 0.4122766852378845 3.631880044937134 0.4574793875217438\n",
      " -0.03873911499977112 -0.8691929578781128 -0.022504543885588646]\n",
      "\n",
      "Image: seq_val/IMG_9827_0.jpg\n",
      "Actual   : [3.0828962791 0.60075780485 0.33826264933 0.999538954796 -0.0140699460639\n",
      " -0.0253371083637 0.00905236999898]\n",
      "Predicted: [2.115058660507202 0.5810368657112122 0.7288863658905029\n",
      " 0.9784722924232483 -0.011927993968129158 -0.06244418025016785\n",
      " 0.01759786158800125]\n",
      "\n",
      "Image: seq_val/S_2_3_198_0.jpg\n",
      "Actual   : [-0.496417170274 -1.4184200361 0.709599448889 0.188041811234\n",
      " -0.0600656968683 -0.978605969768 -0.0579891819365]\n",
      "Predicted: [-0.11273252964019775 -1.4581542015075684 0.8074906468391418\n",
      " 0.15930968523025513 -0.05236569419503212 -0.9715533256530762\n",
      " -0.0665726363658905]\n",
      "\n",
      "Image: seq_val/IMG_9634_0.jpg\n",
      "Actual   : [-1.91519146717 -1.55575775226 4.23703409571 0.603672325209\n",
      " 0.0510572790965 0.794517456643 0.0414112197018]\n",
      "Predicted: [-1.9600703716278076 -1.3020566701889038 3.777669668197632\n",
      " 0.6020363569259644 0.03943202272057533 0.7740797996520996\n",
      " 0.054430410265922546]\n",
      "\n",
      "Image: seq_val/S_2_3_169_0.jpg\n",
      "Actual   : [-1.14781489122 -0.477238706695 -3.08355672794 0.947708215966\n",
      " -0.0311319744049 -0.317446894702 -0.0103637157561]\n",
      "Predicted: [-0.8990582227706909 -0.6648090481758118 -2.923234224319458\n",
      " 0.9450299739837646 -0.0276840478181839 -0.27152544260025024\n",
      " -0.0009465855546295643]\n",
      "\n",
      "Image: seq_val/S_2_3_5_0.jpg\n",
      "Actual   : [-0.510490613809 0.876725337194 0.395557902776 0.177835653554\n",
      " -0.0399711646963 -0.97885646312 -0.0928267791443]\n",
      "Predicted: [-0.2625269293785095 0.7840589880943298 0.22845952212810516\n",
      " 0.1679438054561615 -0.05062023550271988 -0.9505447149276733\n",
      " -0.08063367009162903]\n",
      "\n",
      "Image: seq_val/IMG_9814_0.jpg\n",
      "Actual   : [1.81018634316 0.586061998868 3.23340083713 0.839918794843\n",
      " -0.00483072971626 0.541428020542 0.0369970362544]\n",
      "Predicted: [1.9945718050003052 0.2846086621284485 2.833380937576294\n",
      " 0.8301340341567993 0.013362225145101547 0.5299533009529114\n",
      " 0.037554334849119186]\n",
      "\n",
      "Image: seq_val/IMG_9775_0.jpg\n",
      "Actual   : [-2.14340287057 0.879711340404 2.21354610062 0.397283973506\n",
      " 0.0668856937512 0.914026614717 0.0474035437077]\n",
      "Predicted: [-2.049077272415161 0.4388118088245392 1.596517562866211\n",
      " 0.37179359793663025 0.046386998146772385 0.9178024530410767\n",
      " 0.05192897841334343]\n",
      "\n",
      "Image: seq_val/IMG_9685_0.jpg\n",
      "Actual   : [4.16636056918 -1.93567930711 3.98734207315 0.709268446003 -0.040054683821\n",
      " -0.70375929149 0.00753348859393]\n",
      "Predicted: [5.018376350402832 -1.936968445777893 3.18991756439209 0.6659562587738037\n",
      " -0.04207552224397659 -0.7242457866668701 -0.00774947227910161]\n",
      "\n",
      "Image: seq_val/S_2_3_127_0.jpg\n",
      "Actual   : [-1.09957714831 -0.0317228450817 -4.57686053363 0.965030325631\n",
      " -0.00984509436166 -0.261823715267 -0.00823934787124]\n",
      "Predicted: [-0.9436596632003784 -0.17905521392822266 -4.103917598724365\n",
      " 0.9537066221237183 -0.01789848506450653 -0.2546781599521637\n",
      " -0.0031297518871724606]\n",
      "\n",
      "Image: seq_val/IMG_8286_0.jpg\n",
      "Actual   : [-3.60394879129 0.965084079734 2.8927888109 0.917751971686\n",
      " -0.0144595424988 -0.396427830137 -0.0191628700806]\n",
      "Predicted: [-3.4693422317504883 0.9645028710365295 2.7410709857940674\n",
      " 0.9222240447998047 -0.017440371215343475 -0.374636173248291\n",
      " -0.002706747967749834]\n",
      "\n",
      "Image: seq_val/IMG_9649_0.jpg\n",
      "Actual   : [-3.89830997701 -1.23502479787 1.61499463854 0.954345139083\n",
      " -0.0244685297591 -0.297169586484 0.0178012198489]\n",
      "Predicted: [-2.9315154552459717 -0.8435413837432861 1.5806983709335327\n",
      " 0.9477019309997559 -0.031199121847748756 -0.2840128242969513\n",
      " 0.009825596585869789]\n",
      "\n",
      "Image: seq_val/S_2_3_89_0.jpg\n",
      "Actual   : [-1.94255304234 -0.0311520447354 -3.89483037345 0.778404784428\n",
      " -0.0335912722881 -0.62669333267 -0.0145974241612]\n",
      "Predicted: [-1.851130723953247 0.02719593420624733 -4.048027038574219\n",
      " 0.7901349663734436 -0.02849438786506653 -0.6374297142028809\n",
      " -0.031784191727638245]\n",
      "\n",
      "Image: seq_val/S_2_3_186_0.jpg\n",
      "Actual   : [-1.07858062459 -1.15300048202 -1.67533926505 0.0339831229078\n",
      " 0.0727941855046 0.97056759429 0.227034576284]\n",
      "Predicted: [-0.6358551979064941 -0.38461169600486755 -0.9973335862159729\n",
      " 0.050311535596847534 0.06836974620819092 0.988425612449646\n",
      " 0.16304372251033783]\n",
      "\n",
      "Image: seq_val/IMG_9841_0.jpg\n",
      "Actual   : [4.14947011962 0.568610583713 0.466170664136 0.94552032317\n",
      " -0.0183018155769 -0.325000951974 0.00554465822187]\n",
      "Predicted: [3.7848923206329346 0.3687722682952881 0.9465956687927246\n",
      " 0.9376265406608582 -0.025594940409064293 -0.28642037510871887\n",
      " 0.007919940166175365]\n",
      "\n",
      "Image: seq_val/IMG_9700_0.jpg\n",
      "Actual   : [4.34257757811 -1.7706054152 1.37869346324 0.302865521552 -0.0451585902125\n",
      " -0.951477254002 -0.0304008667905]\n",
      "Predicted: [4.667614459991455 -1.7327567338943481 1.3727436065673828\n",
      " 0.3122255206108093 -0.04162711277604103 -0.9286540746688843\n",
      " -0.03893834352493286]\n",
      "\n",
      "Image: seq_val/S_2_3_8_0.jpg\n",
      "Actual   : [-1.5068412855 1.01851224464 -1.22594836855 0.821916091312\n",
      " 0.00132457818746 -0.568961808603 -0.0271043296658]\n",
      "Predicted: [-1.2082492113113403 0.7115166187286377 -1.2980319261550903\n",
      " 0.7524394989013672 -0.016820091754198074 -0.6458172798156738\n",
      " -0.034892018884420395]\n",
      "\n",
      "Image: seq_val/IMG_9789_0.jpg\n",
      "Actual   : [-2.16474010095 0.797089892177 4.56998964968 0.486982489392\n",
      " 0.0671011422662 0.869438235974 0.0492203774757]\n",
      "Predicted: [-2.4643120765686035 0.37084701657295227 4.607917308807373\n",
      " 0.48691272735595703 0.05525444075465202 0.868146538734436\n",
      " 0.048732828348875046]\n",
      "\n",
      "Image: seq_val/S_2_3_21_0.jpg\n",
      "Actual   : [-0.253506810199 0.781624044337 -2.32620373833 0.982087489882\n",
      " 0.133529065066 -0.132926758194 0.00215126862766]\n",
      "Predicted: [-0.4932468831539154 0.4920714795589447 -2.6279356479644775\n",
      " 0.9736269116401672 0.10222983360290527 -0.11998040974140167\n",
      " 0.0012388568138703704]\n",
      "\n",
      "Image: seq_val/IMG_9938_0.jpg\n",
      "Actual   : [-1.37995888431 0.975289808322 -0.736188374151 0.586871901926\n",
      " -0.0466550983558 -0.808224352301 -0.013344245016]\n",
      "Predicted: [-1.2034924030303955 0.302209734916687 -1.0869872570037842\n",
      " 0.5880324840545654 -0.03352351486682892 -0.7919321060180664\n",
      " -0.03868168219923973]\n",
      "\n",
      "Image: seq_val/IMG_9920_0.jpg\n",
      "Actual   : [-3.19510113503 1.02774944724 0.781364712221 0.550452738397\n",
      " -0.029153406225 -0.83268768784 -0.052754869113]\n",
      "Predicted: [-2.3855767250061035 0.522794246673584 0.6880303025245667\n",
      " 0.532425582408905 -0.03439369425177574 -0.854328989982605\n",
      " -0.04238336160778999]\n",
      "\n",
      "Image: seq_val/S_2_3_206_0.jpg\n",
      "Actual   : [-0.744700300717 -1.2394995306 -1.76846055347 0.886879366397\n",
      " -0.0428492955941 -0.459991234952 0.0041220254223]\n",
      "Predicted: [-0.4632534682750702 -0.7314977645874023 -1.6860682964324951\n",
      " 0.8792927265167236 -0.03268579766154289 -0.477792888879776\n",
      " -0.011242588981986046]\n",
      "\n",
      "Image: seq_val/S_2_3_3_0.jpg\n",
      "Actual   : [-0.673251548629 0.826031287291 1.39826676531 0.101119646178\n",
      " -0.0613112062123 -0.990993040214 -0.0628374680989]\n",
      "Predicted: [-0.5728052258491516 0.26168665289878845 1.1538994312286377\n",
      " 0.12650719285011292 -0.05092749744653702 -0.9590498805046082\n",
      " -0.0703331008553505]\n",
      "\n",
      "Image: seq_val/S_2_3_22_0.jpg\n",
      "Actual   : [-0.280607759715 0.780865207861 -2.31047214564 0.989859667295\n",
      " 0.135806185598 -0.0378122514681 0.0174571663119]\n",
      "Predicted: [-0.32103464007377625 0.5599619746208191 -2.467864513397217\n",
      " 0.981471598148346 0.10887978971004486 -0.03564537689089775\n",
      " 0.011792274191975594]\n",
      "\n",
      "Image: seq_val/IMG_9657_0.jpg\n",
      "Actual   : [-0.418141688878 -1.47691264929 1.31510740249 0.97245769735\n",
      " -0.0497125000527 -0.227248784137 0.0145837001971]\n",
      "Predicted: [0.03740396350622177 -1.1305341720581055 0.7751545906066895\n",
      " 0.9134894013404846 -0.03576704487204552 -0.3624497950077057\n",
      " 0.005348637234419584]\n",
      "\n",
      "Image: seq_val/IMG_8197_0.jpg\n",
      "Actual   : [-1.47214775975 0.802394559695 2.33159090011 0.756954075048\n",
      " 0.0160865311381 0.65273391732 0.0264610083732]\n",
      "Predicted: [-1.5857549905776978 0.7219444513320923 2.1383867263793945\n",
      " 0.7709044814109802 0.03576860949397087 0.6206554770469666\n",
      " 0.048634301871061325]\n",
      "\n",
      "Image: seq_val/IMG_9952_0.jpg\n",
      "Actual   : [0.0195657940994 0.914865269013 -0.979648743959 0.840985205688\n",
      " 0.0213800510943 0.539675244068 0.0322088212933]\n",
      "Predicted: [-0.3324483633041382 0.8435623645782471 -1.2050833702087402\n",
      " 0.8563234806060791 0.03401147946715355 0.49605870246887207\n",
      " 0.04215046390891075]\n",
      "\n",
      "Image: seq_val/S_2_3_125_0.jpg\n",
      "Actual   : [-1.10797173883 -0.0349634846172 -4.55704507237 0.995747564125\n",
      " -0.000438610364909 -0.0921225515537 0.000177920613322]\n",
      "Predicted: [-1.0703719854354858 -0.10144272446632385 -4.665101528167725\n",
      " 0.9866907000541687 -0.010873889550566673 -0.0839124321937561\n",
      " 0.009231629781425]\n",
      "\n",
      "Image: seq_val/IMG_9942_0.jpg\n",
      "Actual   : [-1.47734697763 1.00647064713 -0.782510331399 0.105108694503\n",
      " -0.052626345674 -0.992305059036 -0.0389011554015]\n",
      "Predicted: [-0.9041566252708435 0.04261190816760063 -0.7895791530609131\n",
      " 0.0588114857673645 -0.061085045337677 -0.9875892400741577\n",
      " -0.0355868823826313]\n",
      "\n",
      "Image: seq_val/IMG_9770_0.jpg\n",
      "Actual   : [-2.41433077482 0.977855177004 0.826177627001 0.414754153793\n",
      " 0.0545290842024 0.906059490732 0.0637320181939]\n",
      "Predicted: [-1.9725178480148315 0.5050532221794128 0.9564263820648193\n",
      " 0.3988652229309082 0.048323553055524826 0.9172247052192688\n",
      " 0.05724895000457764]\n",
      "\n",
      "Image: seq_val/IMG_9845_0.jpg\n",
      "Actual   : [5.61558004078 0.52767886119 2.1533531535 0.950510608323 -0.019651639349\n",
      " -0.31005924316 0.00258113698621]\n",
      "Predicted: [5.794930458068848 0.30754008889198303 1.360011100769043\n",
      " 0.8850177526473999 -0.02589045837521553 -0.3668797016143799\n",
      " 0.007053384557366371]\n",
      "\n",
      "Image: seq_val/S_2_3_113_0.jpg\n",
      "Actual   : [0.0313771509168 -0.152612885121 -4.03490916421 0.931790603106\n",
      " -0.0061606580116 0.361068128993 0.0368527404898]\n",
      "Predicted: [-0.3324921429157257 -0.19617068767547607 -3.573421001434326\n",
      " 0.9213880300521851 0.00472332164645195 0.3716922998428345\n",
      " 0.03653758391737938]\n",
      "\n",
      "Image: seq_val/IMG_9623_0.jpg\n",
      "Actual   : [-2.16499368785 -1.41412889082 2.14551367591 0.504641217606 0.03976413419\n",
      " 0.860519524278 0.0571157024116]\n",
      "Predicted: [-2.018537998199463 -1.3649258613586426 1.8858671188354492\n",
      " 0.5309303998947144 0.03165656700730324 0.8402794599533081\n",
      " 0.05988254398107529]\n",
      "\n",
      "Image: seq_val/IMG_9877_0.jpg\n",
      "Actual   : [6.72299610876 0.487007308847 3.55757136775 0.453344970672\n",
      " -0.0196725179302 -0.890584299063 -0.0308372480382]\n",
      "Predicted: [6.875962257385254 0.4741932451725006 2.6637940406799316 0.45015749335289\n",
      " -0.03575882315635681 -0.8758097290992737 -0.03036509081721306]\n",
      "\n",
      "Image: seq_val/IMG_9906_0.jpg\n",
      "Actual   : [4.29776706786 0.83121297481 -3.31685672532 0.973479094482 0.0995756226112\n",
      " 0.20320122574 0.033651297833]\n",
      "Predicted: [3.6590301990509033 0.44908562302589417 -2.608491897583008\n",
      " 0.9633428454399109 0.07652439177036285 0.2000582218170166\n",
      " 0.03567850962281227]\n",
      "\n",
      "Image: seq_val/S_2_3_171_0.jpg\n",
      "Actual   : [-0.153207600242 -1.33135664072 -0.774706840455 0.420949287132\n",
      " -0.0268230155562 -0.902842936037 -0.0834077715114]\n",
      "Predicted: [-0.1039004996418953 -0.20028479397296906 -0.08230873942375183\n",
      " 0.4126114249229431 -0.04352898895740509 -0.9144031405448914\n",
      " -0.04728148505091667]\n",
      "\n",
      "Image: seq_val/S_2_3_47_0.jpg\n",
      "Actual   : [-0.184497268888 0.938285524204 -0.915528467025 0.0402928455957\n",
      " -0.0637785844068 -0.993893804609 -0.0805225677961]\n",
      "Predicted: [-0.10546364635229111 0.8484721779823303 -1.1249175071716309\n",
      " 0.062462180852890015 -0.061534833163022995 -0.9667086005210876\n",
      " -0.04632563143968582]\n",
      "\n",
      "Image: seq_val/IMG_9686_0.jpg\n",
      "Actual   : [4.12549863537 -1.85691751318 2.58724407482 0.762997203121\n",
      " -0.0480516941409 -0.644596873832 -0.00460140919175]\n",
      "Predicted: [4.0295515060424805 -1.8754011392593384 1.9057438373565674\n",
      " 0.7777423858642578 -0.0395493283867836 -0.6191926002502441\n",
      " -0.004391845315694809]\n",
      "\n",
      "Image: seq_val/IMG_9693_0.jpg\n",
      "Actual   : [6.15478104879 -1.8865239176 1.29579126412 0.206876892049 -0.0401438140522\n",
      " -0.976687690514 -0.0408849719061]\n",
      "Predicted: [6.359489917755127 -1.5714632272720337 2.2719812393188477\n",
      " 0.17366328835487366 -0.04639219492673874 -1.0195921659469604\n",
      " -0.04679827019572258]\n",
      "\n",
      "Image: seq_val/IMG_9679_0.jpg\n",
      "Actual   : [2.75122163801 -1.68768454591 1.78011207721 0.883392813423\n",
      " -0.0509161466564 -0.465599470519 0.0155504422522]\n",
      "Predicted: [3.237112045288086 -1.7593376636505127 1.2045722007751465\n",
      " 0.8941588997840881 -0.0370115265250206 -0.44669073820114136\n",
      " 0.005728984251618385]\n",
      "\n",
      "Image: seq_val/IMG_9820_0.jpg\n",
      "Actual   : [1.84133657181 0.676596600285 0.662007321287 0.99519442653\n",
      " -0.00209331681811 0.097752223094 0.00530794768106]\n",
      "Predicted: [1.6097192764282227 0.6792832016944885 0.6187132000923157\n",
      " 0.9907125234603882 0.0022936630994081497 0.10769333690404892\n",
      " 0.0244480911642313]\n",
      "\n",
      "Image: seq_val/S_2_3_17_0.jpg\n",
      "Actual   : [-1.46543031551 1.02134060492 -1.33067224321 0.887169453835\n",
      " 0.000289789620826 -0.461423319878 -0.00433544412431]\n",
      "Predicted: [-1.5616462230682373 0.348506361246109 -1.5557795763015747\n",
      " 0.899750292301178 -0.01860622502863407 -0.45800885558128357\n",
      " -0.01602383889257908]\n",
      "\n",
      "Image: seq_val/IMG_9633_0.jpg\n",
      "Actual   : [-1.46455478411 -1.52718165336 3.04996494413 0.764949444469\n",
      " 0.0221429195933 0.64151804155 0.0530720348663]\n",
      "Predicted: [-0.9732996225357056 -1.4760929346084595 3.1121251583099365\n",
      " 0.7337009906768799 0.026768561452627182 0.6793750524520874\n",
      " 0.052814070135354996]\n",
      "\n",
      "Image: seq_val/IMG_9747_0.jpg\n",
      "Actual   : [1.36763993282 -1.6444881714 2.37972166782 0.276207649398 0.0171280851576\n",
      " 0.959240428121 0.0572168172024]\n",
      "Predicted: [1.4790865182876587 -1.3343507051467896 2.000373125076294\n",
      " 0.2497202455997467 0.028789401054382324 0.9500772953033447\n",
      " 0.056384775787591934]\n",
      "\n",
      "Image: seq_val/S_2_3_183_0.jpg\n",
      "Actual   : [-1.01157435418 -1.14511005459 -1.55023506871 0.0182192605524\n",
      " 0.0686869816248 0.953860809891 0.291718550067]\n",
      "Predicted: [-0.8904401063919067 -0.5101584792137146 -1.409538984298706\n",
      " 0.0359654426574707 0.08608885109424591 0.9761475324630737\n",
      " 0.22031311690807343]\n",
      "\n",
      "Image: seq_val/IMG_8193_0.jpg\n",
      "Actual   : [-2.48266569214 1.00542559707 0.534804079153 0.943628324489\n",
      " 0.0113766974379 0.327384778209 0.0474906619837]\n",
      "Predicted: [-2.52298903465271 0.812036395072937 0.5398099422454834 0.9263865947723389\n",
      " 0.016267389059066772 0.3545008599758148 0.03777815029025078]\n",
      "\n",
      "Image: seq_val/IMG_9743_0.jpg\n",
      "Actual   : [-2.51980951578 -1.41079295692 2.41339157855 0.286050835238\n",
      " -0.055966533036 -0.956430925687 -0.0168092602116]\n",
      "Predicted: [-2.014568567276001 -1.1245421171188354 1.9936678409576416\n",
      " 0.2657029926776886 -0.054624900221824646 -0.9508147239685059\n",
      " -0.03680507093667984]\n",
      "\n",
      "Image: seq_val/S_2_3_211_0.jpg\n",
      "Actual   : [-0.599768152692 -1.23811075554 -2.09689853505 0.771306500623\n",
      " 0.0349622507317 0.633101763311 0.0551913074016]\n",
      "Predicted: [-0.43688729405403137 -0.23222368955612183 -1.4249123334884644\n",
      " 0.6992294788360596 0.026066694408655167 0.722529947757721\n",
      " 0.05711640045046806]\n",
      "\n",
      "Image: seq_val/S_2_3_174_0.jpg\n",
      "Actual   : [-0.142236512093 -1.32864924999 -0.693328436032 0.0538011157781\n",
      " -0.0656588960058 -0.990900954183 -0.10444926192]\n",
      "Predicted: [-0.3073793351650238 -1.0255653858184814 -0.43409019708633423\n",
      " 0.017256557941436768 -0.05394292622804642 -0.9352689981460571\n",
      " -0.08720461279153824]\n",
      "\n",
      "Image: seq_val/IMG_9777_0.jpg\n",
      "Actual   : [-3.49461073508 0.949719903937 2.86494113981 0.228544342999\n",
      " 0.0484410195884 0.970043194542 0.0666119480732]\n",
      "Predicted: [-3.187124729156494 -0.0902758538722992 3.2820746898651123\n",
      " 0.20847457647323608 0.04214583709836006 0.9530383348464966\n",
      " 0.048947110772132874]\n",
      "\n",
      "Image: seq_val/IMG_8290_0.jpg\n",
      "Actual   : [-2.35692398693 0.907850677115 1.2950882776 0.977077472183\n",
      " -0.0297653079281 -0.210024345221 0.0179837206889]\n",
      "Predicted: [-2.3169212341308594 0.5944928526878357 0.9247860908508301\n",
      " 0.9598943591117859 -0.0159793458878994 -0.2290465235710144\n",
      " 0.004848245531320572]\n",
      "\n",
      "Image: seq_val/S_2_3_170_0.jpg\n",
      "Actual   : [-0.175698241934 -1.32257353071 -0.755023869857 0.505857339563\n",
      " -0.0405791754581 -0.859803402178 -0.0565667051597]\n",
      "Predicted: [-0.5839894413948059 -0.4077604413032532 -1.0145177841186523\n",
      " 0.44034522771835327 -0.041488952934741974 -0.8814256191253662\n",
      " -0.04802853986620903]\n",
      "\n",
      "Image: seq_val/IMG_9650_0.jpg\n",
      "Actual   : [-2.83976809657 -1.30330573666 1.31913661062 0.971062836793\n",
      " -0.0149662052819 -0.236494265578 0.0297227530265]\n",
      "Predicted: [-1.9848889112472534 -0.7463828325271606 1.2685272693634033\n",
      " 0.9572705626487732 -0.023970264941453934 -0.21762323379516602\n",
      " 0.014174839481711388]\n",
      "\n",
      "Image: seq_val/S_2_3_133_0.jpg\n",
      "Actual   : [-1.37693956227 -0.226339033398 -3.34033074822 0.992341526084\n",
      " -0.0374587347516 -0.11521671634 0.0240883182629]\n",
      "Predicted: [-1.139804720878601 -0.09270448982715607 -2.9236621856689453\n",
      " 0.9800624847412109 -0.017018526792526245 -0.1423136591911316\n",
      " 0.006970347836613655]\n",
      "\n",
      "Image: seq_val/S_2_3_166_0.jpg\n",
      "Actual   : [-1.20610056161 -0.471602001751 -3.0520422615 0.992462788533\n",
      " -0.0146223650031 -0.121597304412 -0.00423029299019]\n",
      "Predicted: [-1.1465861797332764 -0.20745858550071716 -3.399272918701172\n",
      " 0.9715901613235474 -0.01985790580511093 -0.13059502840042114\n",
      " 0.007256206125020981]\n",
      "\n",
      "Image: seq_val/S_2_3_61_0.jpg\n",
      "Actual   : [0.483181070597 0.894059276877 -1.09908068564 0.312339767704\n",
      " 0.033250983384 0.947884141884 0.0534218605067]\n",
      "Predicted: [0.3457452356815338 0.4813016951084137 -0.9455313086509705\n",
      " 0.29079869389533997 0.033027131110429764 0.937713623046875\n",
      " 0.047987185418605804]\n",
      "\n",
      "Image: seq_val/IMG_9676_0.jpg\n",
      "Actual   : [6.07019229827 -1.86651333663 0.53729828235 0.991006739201\n",
      " -0.0156250367403 0.129326958531 0.0305947525157]\n",
      "Predicted: [5.14860725402832 -1.6792937517166138 0.4686012268066406\n",
      " 0.9906297922134399 -0.010757885873317719 0.09429765492677689\n",
      " 0.03439462557435036]\n",
      "\n",
      "Image: seq_val/IMG_9745_0.jpg\n",
      "Actual   : [-0.994864001695 -1.47519461972 2.43602782824 0.117646837574\n",
      " -0.0432268686208 -0.990890180439 -0.0492677353681]\n",
      "Predicted: [-0.6752320528030396 -1.2289998531341553 2.18782639503479\n",
      " 0.07152722775936127 -0.05670779570937157 -0.9824513792991638\n",
      " -0.04538688808679581]\n",
      "\n",
      "Image: seq_val/IMG_8206_0.jpg\n",
      "Actual   : [-3.57366066563 0.980119520357 2.34811424391 0.998634173965 0.029696425691\n",
      " 0.0424918782553 0.00650762413719]\n",
      "Predicted: [-3.7855019569396973 0.8666347861289978 2.521212339401245\n",
      " 0.9939559698104858 0.00934503972530365 -0.03629454970359802\n",
      " 0.015289189293980598]\n",
      "\n",
      "Image: seq_val/S_2_3_11_0.jpg\n",
      "Actual   : [-1.39605041661 1.02781167165 -1.4409839291 0.643372247825\n",
      " -0.0114104952816 -0.764012145911 -0.0471952563897]\n",
      "Predicted: [-1.1229329109191895 0.22548258304595947 -0.7283136248588562\n",
      " 0.6145905256271362 -0.039345283061265945 -0.7991049885749817\n",
      " -0.03296239301562309]\n",
      "\n",
      "Image: seq_val/S_2_3_44_0.jpg\n",
      "Actual   : [-0.169018384055 0.976758703478 -0.879293575561 0.0402596598107\n",
      " -0.0695086635343 -0.981737639292 -0.172449740744]\n",
      "Predicted: [-0.30928727984428406 0.5433732867240906 -1.1628220081329346\n",
      " 0.03960506618022919 -0.0578358955681324 -0.9869157075881958\n",
      " -0.18293431401252747]\n",
      "\n",
      "Image: seq_val/IMG_9636_0.jpg\n",
      "Actual   : [-2.43310391289 -1.47191294886 4.77005689516 0.536158917889\n",
      " 0.0774551052032 0.839667611808 0.0386344810647]\n",
      "Predicted: [-2.4959163665771484 -0.9385054707527161 5.026488304138184\n",
      " 0.5370512008666992 0.05389682576060295 0.8365941643714905\n",
      " 0.055822115391492844]\n",
      "\n",
      "Image: seq_val/IMG_9722_0.jpg\n",
      "Actual   : [1.09979110886 -1.63406674978 2.46224446441 0.200058831184 0.029113828391\n",
      " 0.9786854568 0.0361057573018]\n",
      "Predicted: [0.6739203929901123 -1.1721025705337524 2.2066495418548584\n",
      " 0.18375082314014435 0.03338354080915451 0.9906219244003296\n",
      " 0.05595479533076286]\n",
      "\n",
      "Image: seq_val/IMG_9884_0.jpg\n",
      "Actual   : [5.6109525072 0.597948479677 -1.63848705313 0.406776612464 0.0241958653159\n",
      " 0.911757541729 0.0514347426817]\n",
      "Predicted: [5.1594743728637695 0.6998966336250305 -1.3065170049667358\n",
      " 0.4246501326560974 0.038596831262111664 0.8947999477386475\n",
      " 0.05707258731126785]\n",
      "\n",
      "Image: seq_val/IMG_9899_0.jpg\n",
      "Actual   : [2.39697736234 0.851829672806 -0.00963098001104 0.389503604733\n",
      " -0.0312780344095 -0.917139337872 -0.0785115366804]\n",
      "Predicted: [3.1138827800750732 0.4546109735965729 0.19379012286663055\n",
      " 0.40946686267852783 -0.027179770171642303 -0.8821860551834106\n",
      " -0.0695367380976677]\n",
      "\n",
      "Image: seq_val/IMG_9885_0.jpg\n",
      "Actual   : [4.82640819456 0.577225987687 -0.662245350518 0.324400698512\n",
      " 0.0155266590888 0.945641604274 0.0168838955621]\n",
      "Predicted: [4.2269439697265625 0.6108018159866333 -0.6414635181427002\n",
      " 0.2925769090652466 0.036668140441179276 0.9409471750259399\n",
      " 0.047118186950683594]\n",
      "\n",
      "Image: seq_val/S_2_3_196_0.jpg\n",
      "Actual   : [-1.32038586459 -1.37202602295 0.86336373614 0.268371200796\n",
      " -0.0536348679026 -0.959878213165 -0.0611065906368]\n",
      "Predicted: [-1.0209001302719116 -1.0055783987045288 1.097883701324463\n",
      " 0.24500307440757751 -0.05155474320054054 -0.9503931999206543\n",
      " -0.05073445662856102]\n",
      "\n",
      "Image: seq_val/IMG_8191_0.jpg\n",
      "Actual   : [-3.47402508873 1.09448198192 0.0223381735087 0.976044700632\n",
      " 0.00840693914353 0.215956480909 0.0250771628326]\n",
      "Predicted: [-3.3822479248046875 0.6055191159248352 0.4175037741661072\n",
      " 0.956501841545105 0.01773976907134056 0.23179244995117188\n",
      " 0.03072706237435341]\n",
      "\n",
      "Image: seq_val/IMG_9951_0.jpg\n",
      "Actual   : [-0.104330411968 0.946115476766 -1.44791204256 0.879058843463\n",
      " 0.014759761022 0.475913872024 0.0233170666881]\n",
      "Predicted: [-0.08234643191099167 0.9394170641899109 -1.1413640975952148\n",
      " 0.8923066258430481 0.03754230588674545 0.43750959634780884\n",
      " 0.04338063672184944]\n",
      "\n",
      "Image: seq_val/IMG_9850_0.jpg\n",
      "Actual   : [4.6505001767 0.462958637403 1.97495496112 0.655459244954 -0.0531682185702\n",
      " -0.753282031686 -0.0106065771098]\n",
      "Predicted: [5.077690601348877 0.4245111346244812 2.2115261554718018\n",
      " 0.6577073931694031 -0.03396443650126457 -0.7398205995559692\n",
      " -0.014387542381882668]\n",
      "\n",
      "Image: seq_val/IMG_8272_0.jpg\n",
      "Actual   : [-3.63934762833 0.969651073425 2.59682155193 0.968857227192\n",
      " -0.0143751966661 -0.246608287842 0.017128321775]\n",
      "Predicted: [-3.268667697906494 0.8850377798080444 2.3554584980010986\n",
      " 0.948698103427887 -0.011846872977912426 -0.2682008445262909\n",
      " 0.0038609181065112352]\n",
      "\n",
      "Image: seq_val/IMG_9925_0.jpg\n",
      "Actual   : [-0.488683292049 0.81760184178 2.25536197212 0.0155011723158\n",
      " -0.0427776593914 -0.998311457864 -0.0361111978669]\n",
      "Predicted: [-0.28946998715400696 0.1953546702861786 1.3055604696273804\n",
      " 0.04241617023944855 -0.05702975392341614 -0.9976608753204346\n",
      " -0.06510008871555328]\n",
      "\n",
      "Image: seq_val/IMG_9802_0.jpg\n",
      "Actual   : [-1.22717727713 0.825725787699 2.35374999867 0.92359357608 0.0129986096581\n",
      " 0.382053729383 0.0289981074398]\n",
      "Predicted: [-1.170299768447876 0.6555590033531189 2.0541937351226807\n",
      " 0.9417534470558167 0.011891815811395645 0.3278346061706543\n",
      " 0.03804480656981468]\n",
      "\n",
      "Image: seq_val/IMG_8294_0.jpg\n",
      "Actual   : [0.147512409724 0.809083667222 0.962438217199 0.971197366418\n",
      " -0.00888195103477 0.237468756462 0.0174750139462]\n",
      "Predicted: [0.02211976796388626 0.7706617712974548 0.8711399435997009\n",
      " 0.9685846567153931 0.008116470649838448 0.2200586199760437\n",
      " 0.02866826020181179]\n",
      "\n",
      "Image: seq_val/IMG_9936_0.jpg\n",
      "Actual   : [-0.844864148849 1.02529365263 -1.88317766602 0.834233088596\n",
      " 0.000271638590577 -0.551104142239 -0.0184202201686]\n",
      "Predicted: [-0.8034407496452332 0.6610971093177795 -1.614302396774292\n",
      " 0.8226007223129272 -0.0030263811349868774 -0.5282059907913208\n",
      " -0.027821525931358337]\n",
      "\n",
      "Image: seq_val/IMG_9810_0.jpg\n",
      "Actual   : [0.584601690388 0.755344241231 1.3478812431 0.958428663229\n",
      " 0.00157963972773 0.284100958996 0.0264319378188]\n",
      "Predicted: [0.6549770832061768 0.7442885041236877 1.5076031684875488\n",
      " 0.9365485906600952 0.011823061853647232 0.34766340255737305\n",
      " 0.03229061886668205]\n",
      "\n",
      "Image: seq_val/S_2_3_77_0.jpg\n",
      "Actual   : [0.0353821829069 0.373531080106 -3.16223267275 0.409597212703\n",
      " 0.0216564857127 0.911684375696 0.0243458227061]\n",
      "Predicted: [-0.009831972420215607 0.34722355008125305 -2.968066692352295\n",
      " 0.3951467275619507 0.028327807784080505 0.8984127640724182\n",
      " 0.025902487337589264]\n",
      "\n",
      "Image: seq_val/IMG_9918_0.jpg\n",
      "Actual   : [-1.91763251255 0.915420085486 1.39031750903 0.272168671412\n",
      " -0.054964539219 -0.958172271652 -0.0693470372006]\n",
      "Predicted: [-1.6704049110412598 0.2284592092037201 1.2300639152526855\n",
      " 0.27249792218208313 -0.044157497584819794 -0.9352743029594421\n",
      " -0.058795612305402756]\n",
      "\n",
      "Image: seq_val/IMG_9797_0.jpg\n",
      "Actual   : [-1.49467324549 0.863976588037 2.12645784473 0.936295986494\n",
      " 0.0352437572721 0.344295585303 0.0597348574036]\n",
      "Predicted: [-1.6794556379318237 0.6613950133323669 2.016277313232422\n",
      " 0.9255703687667847 0.022243682295084 0.36842095851898193\n",
      " 0.03822268545627594]\n",
      "\n",
      "Image: seq_val/S_2_3_157_0.jpg\n",
      "Actual   : [-2.12164593843 0.0537947654046 -4.40082337891 0.915065343295\n",
      " 0.0435479863465 -0.397092309843 -0.0554678992739]\n",
      "Predicted: [-1.9264047145843506 0.006759811192750931 -4.486382007598877\n",
      " 0.9168097376823425 0.02938910201191902 -0.40287503600120544\n",
      " -0.05514577403664589]\n",
      "\n",
      "Image: seq_val/IMG_9618_0.jpg\n",
      "Actual   : [-2.79757425048 -1.25729067441 0.455843049759 0.422627038901\n",
      " 0.0416195120747 0.90423192321 0.0449314060779]\n",
      "Predicted: [-1.920097827911377 -0.35551202297210693 1.0973896980285645\n",
      " 0.3666113317012787 0.03888533636927605 0.924281120300293\n",
      " 0.05707884579896927]\n",
      "\n",
      "Image: seq_val/S_2_3_104_0.jpg\n",
      "Actual   : [0.0174226110712 0.0361248171689 -3.80914922715 0.885348965382\n",
      " 0.463141542797 0.0110917623064 -0.0391675074815]\n",
      "Predicted: [-0.4301851689815521 -0.14766214787960052 -4.7044453620910645\n",
      " 0.9124943017959595 0.42566752433776855 0.0267372727394104\n",
      " -0.04793338477611542]\n",
      "\n",
      "Image: seq_val/S_2_3_13_0.jpg\n",
      "Actual   : [-1.50119821892 1.03979834245 -1.46847324851 0.430944637336\n",
      " -0.0455825197867 -0.900873530368 -0.0252157832388]\n",
      "Predicted: [-1.2457034587860107 0.7640690803527832 -1.1298786401748657\n",
      " 0.41828399896621704 -0.04431954026222229 -0.8977949023246765\n",
      " -0.04937061294913292]\n",
      "\n",
      "Image: seq_val/IMG_9957_0.jpg\n",
      "Actual   : [-0.907326255318 0.853194859947 1.24251311561 0.13808001861\n",
      " 0.0285898451879 0.988787568976 0.0491494928873]\n",
      "Predicted: [-0.5723138451576233 0.17610758543014526 1.2601816654205322\n",
      " 0.13193385303020477 0.04090515896677971 0.9801241159439087\n",
      " 0.06533298641443253]\n",
      "\n",
      "Image: seq_val/IMG_9944_0.jpg\n",
      "Actual   : [-1.1047352337 1.03056984534 -0.886659507769 0.0337219001382\n",
      " 0.0479815045751 0.980149052894 0.189389658592]\n",
      "Predicted: [-0.3150108754634857 -0.194636732339859 -0.9857776165008545\n",
      " 0.032040148973464966 0.06481964886188507 1.0006413459777832\n",
      " 0.13561758399009705]\n",
      "\n",
      "Image: seq_val/IMG_9627_0.jpg\n",
      "Actual   : [-1.22917397113 -1.38784247085 0.563256330819 0.863648526745\n",
      " 0.0052946934323 0.500324656351 0.0613060088415]\n",
      "Predicted: [-1.056277871131897 -0.7420254945755005 0.19794590771198273\n",
      " 0.8602957129478455 0.012309491634368896 0.5049158930778503\n",
      " 0.04604025557637215]\n",
      "\n",
      "Image: seq_val/IMG_8208_0.jpg\n",
      "Actual   : [-2.88814785495 0.931208984479 2.45273936701 0.994874572885\n",
      " 0.0394016285914 0.0926757582325 0.00912686837942]\n",
      "Predicted: [-2.791830539703369 0.8107931017875671 2.197791576385498\n",
      " 0.9812719821929932 0.010590482503175735 0.05044005811214447\n",
      " 0.019839925691485405]\n",
      "\n",
      "Image: seq_val/IMG_9684_0.jpg\n",
      "Actual   : [6.22746970425 -1.81631148945 -0.572865483077 0.997676048578\n",
      " -0.00726549432702 -0.0667164456101 0.0117741484499]\n",
      "Predicted: [6.237989902496338 -1.9134331941604614 -0.679318368434906\n",
      " 0.9899536967277527 -0.00874314084649086 -0.10613183677196503\n",
      " 0.027851317077875137]\n",
      "\n",
      "Image: seq_val/IMG_9828_0.jpg\n",
      "Actual   : [4.83461302904 0.474550138126 0.673331740909 0.974613246298\n",
      " -0.0113296055874 0.222203823492 0.0250224100404]\n",
      "Predicted: [4.182015419006348 0.3659387230873108 0.5491291284561157\n",
      " 0.9548011422157288 0.0006719864904880524 0.22474512457847595\n",
      " 0.027948888018727303]\n",
      "\n",
      "Image: seq_val/IMG_9785_0.jpg\n",
      "Actual   : [-2.22076996687 1.02523997621 -0.219995590368 0.93475949581\n",
      " -0.0031272877149 0.352504105156 0.044223985725]\n",
      "Predicted: [-2.2756383419036865 0.6614405512809753 0.16165421903133392\n",
      " 0.9246288537979126 0.012138787657022476 0.356198787689209\n",
      " 0.038158074021339417]\n",
      "\n",
      "Image: seq_val/IMG_9678_0.jpg\n",
      "Actual   : [6.72584031343 -1.96609643399 2.32728704009 0.94620305259 0.0603183903557\n",
      " 0.312999842388 0.0556109136788]\n",
      "Predicted: [6.965328216552734 -1.693091869354248 2.196181297302246 0.9167397022247314\n",
      " 0.010542847216129303 0.34023767709732056 0.04093484207987785]\n",
      "\n",
      "Image: seq_val/IMG_9666_0.jpg\n",
      "Actual   : [3.04777630213 -1.63432560401 -0.148721762645 0.999375936004\n",
      " -0.0196099065072 -0.00847633766203 0.0281307981214]\n",
      "Predicted: [3.6239235401153564 -1.584351897239685 -0.1415354311466217\n",
      " 0.9918689727783203 -0.018673427402973175 -0.0698661208152771\n",
      " 0.024761883541941643]\n",
      "\n",
      "Image: seq_val/S_2_3_188_0.jpg\n",
      "Actual   : [-0.101915261291 -1.45400740318 0.952185554867 0.299197823051\n",
      " -0.0336955006638 -0.952001666024 -0.055118996766]\n",
      "Predicted: [0.17674753069877625 -1.0268226861953735 1.048549771308899\n",
      " 0.28194043040275574 -0.05067693069577217 -0.9525749683380127\n",
      " -0.04512067511677742]\n",
      "\n",
      "Image: seq_val/IMG_9663_0.jpg\n",
      "Actual   : [-0.782386127425 -1.47696905308 2.01778670718 0.884109500031\n",
      " -0.0243237064178 -0.466318624715 -0.0174839785008]\n",
      "Predicted: [-0.4506467282772064 -1.2419445514678955 1.2178698778152466\n",
      " 0.8911638855934143 -0.035740531980991364 -0.43047937750816345\n",
      " 0.001620202325284481]\n",
      "\n",
      "Image: seq_val/S_2_3_177_0.jpg\n",
      "Actual   : [-0.159245803464 -1.31962038138 -0.714247570103 0.255401783371\n",
      " 0.0581303828381 0.960588397499 0.0930629799237]\n",
      "Predicted: [-0.18532882630825043 -0.8614148497581482 -0.5338357090950012\n",
      " 0.2568883001804352 0.05699886009097099 0.9406774044036865\n",
      " 0.10017640888690948]\n",
      "\n",
      "Image: seq_val/IMG_8279_0.jpg\n",
      "Actual   : [-1.17078173216 0.824674975368 2.3392665309 0.94260602734 0.025293141391\n",
      " 0.331434715063 0.0317043194423]\n",
      "Predicted: [-1.2375096082687378 0.8000531792640686 2.0034661293029785\n",
      " 0.9573659896850586 0.018113084137439728 0.26994824409484863\n",
      " 0.03175203874707222]\n",
      "\n",
      "Image: seq_val/S_2_3_140_0.jpg\n",
      "Actual   : [-1.25174546553 -0.236602992897 -3.35724052974 0.997103037441\n",
      " -0.040598957828 -0.0636395125227 0.0093418303873]\n",
      "Predicted: [-1.2985191345214844 -0.14068464934825897 -3.5413546562194824\n",
      " 0.9810840487480164 -0.01939268223941326 -0.11287782341241837\n",
      " 0.007816031575202942]\n",
      "\n",
      "Image: seq_val/S_2_3_73_0.jpg\n",
      "Actual   : [0.165910801512 0.340744168938 -3.2122327513 0.139894600639\n",
      " -0.0610542902716 -0.988282252405 0.000252849549852]\n",
      "Predicted: [-0.9203149676322937 -0.18086473643779755 -2.422713279724121\n",
      " 0.19294482469558716 -0.05030433461070061 -0.945673942565918\n",
      " -0.04449997842311859]\n",
      "\n",
      "Image: seq_val/IMG_9891_0.jpg\n",
      "Actual   : [3.12726586362 0.733642172835 -1.39289020275 0.235796057835\n",
      " -0.0428690609309 -0.969987201287 -0.0410766608237]\n",
      "Predicted: [3.0558483600616455 0.7554723024368286 -0.6957433223724365\n",
      " 0.19781401753425598 -0.04682709649205208 -0.983884334564209\n",
      " -0.03682294115424156]\n",
      "\n",
      "Image: seq_val/S_2_3_84_0.jpg\n",
      "Actual   : [-1.9916571718 0.00540766386678 -3.99475700967 0.440256308576\n",
      " -0.0528333368916 -0.894818213847 -0.0518014038318]\n",
      "Predicted: [-1.0350929498672485 -0.23075978457927704 -2.962834596633911\n",
      " 0.3808320462703705 -0.04519805312156677 -0.8786758780479431\n",
      " -0.045122917741537094]\n",
      "\n",
      "Image: seq_val/IMG_9711_0.jpg\n",
      "Actual   : [0.272969927433 -1.46837878947 0.894075291709 0.454145681819\n",
      " -0.0516402540755 -0.888715957171 -0.0356220621772]\n",
      "Predicted: [0.513066828250885 -1.5641659498214722 0.7874348163604736\n",
      " 0.43448787927627563 -0.04753958061337471 -0.8921331167221069\n",
      " -0.035421062260866165]\n",
      "\n",
      "Image: seq_val/IMG_9710_0.jpg\n",
      "Actual   : [0.974497579039 -1.552063124 1.60668600606 0.260840353836 -0.0608854134147\n",
      " -0.963130696592 -0.0251900283243]\n",
      "Predicted: [1.401104211807251 -1.2763549089431763 1.6759475469589233\n",
      " 0.24533648788928986 -0.05041317269206047 -0.9555627107620239\n",
      " -0.041118521243333817]\n",
      "\n",
      "Image: seq_val/S_2_3_56_0.jpg\n",
      "Actual   : [0.44890436237 0.896570637609 -1.03060439057 0.617736639129\n",
      " 0.0372121178655 0.783511344226 0.0559166918765]\n",
      "Predicted: [0.3317333459854126 0.8320546746253967 -0.6931101679801941\n",
      " 0.5489515662193298 0.04013964906334877 0.8259336948394775\n",
      " 0.05863343924283981]\n",
      "\n",
      "Image: seq_val/S_2_3_161_0.jpg\n",
      "Actual   : [-2.09240721369 0.136479212513 -4.40092975845 0.876894873782\n",
      " 0.229718973319 -0.395585159457 -0.147637919412]\n",
      "Predicted: [-1.9323052167892456 0.1724279522895813 -4.345515251159668\n",
      " 0.8688762784004211 0.17981140315532684 -0.4120023250579834\n",
      " -0.11167117208242416]\n",
      "\n",
      "Image: seq_val/IMG_9752_0.jpg\n",
      "Actual   : [-0.747557241971 -1.32772222447 -0.473012787845 0.551438278868\n",
      " -0.0505054221163 -0.832325787291 -0.0244706098413]\n",
      "Predicted: [-0.6073788404464722 -1.0603306293487549 -0.3950093984603882\n",
      " 0.5504821538925171 -0.04453827440738678 -0.8369529247283936\n",
      " -0.03457893058657646]\n",
      "\n",
      "Image: seq_val/S_2_3_212_0.jpg\n",
      "Actual   : [-0.607019391964 -1.23697394336 -2.13859114831 0.626007989456\n",
      " 0.0467008268861 0.776140721537 0.0594862192223]\n",
      "Predicted: [-0.7711912989616394 -0.3583008944988251 -1.9806448221206665\n",
      " 0.6232229471206665 0.02996189147233963 0.796207845211029\n",
      " 0.0595971904695034]\n",
      "\n",
      "Image: seq_val/IMG_9717_0.jpg\n",
      "Actual   : [-1.46978668497 -1.43435346319 1.88945267728 0.261272808983\n",
      " -0.0703064735741 -0.962262875354 -0.0290461318497]\n",
      "Predicted: [-0.9330732822418213 -1.0567893981933594 1.9832167625427246\n",
      " 0.18186655640602112 -0.054474908858537674 -0.967160165309906\n",
      " -0.041622262448072433]\n",
      "\n",
      "Image: seq_val/IMG_9756_0.jpg\n",
      "Actual   : [-0.708402585985 -1.38806904031 0.459102627946 0.447256420907\n",
      " -0.0320041446314 -0.893539253081 -0.0229135742972]\n",
      "Predicted: [-0.5053552985191345 -1.3378173112869263 0.11746563017368317\n",
      " 0.3962298035621643 -0.04732624441385269 -0.9017525911331177\n",
      " -0.04430718719959259]\n",
      "\n",
      "Image: seq_val/IMG_9661_0.jpg\n",
      "Actual   : [4.26812710216 -1.86359406812 2.54788209474 0.878327560067\n",
      " 0.000649773822906 0.477401561558 0.0250604078692]\n",
      "Predicted: [4.435657501220703 -1.8327614068984985 2.4897396564483643\n",
      " 0.8790816068649292 0.005473947152495384 0.4587622880935669\n",
      " 0.042712487280368805]\n",
      "\n",
      "Image: seq_val/S_2_3_103_0.jpg\n",
      "Actual   : [0.0592404636461 -0.048210280045 -3.81868942656 0.96232300374\n",
      " 0.251508464875 -0.0987186696548 -0.0305377279715]\n",
      "Predicted: [-0.04733341187238693 0.2588135898113251 -3.8605353832244873\n",
      " 0.9463423490524292 0.1744830161333084 -0.1519738733768463\n",
      " -0.015979448333382607]\n",
      "\n",
      "Image: seq_val/IMG_8300_0.jpg\n",
      "Actual   : [1.15821471563 0.624448879557 3.24975732108 0.84457152572 0.0148421251297\n",
      " 0.534354753363 0.0307188349675]\n",
      "Predicted: [2.0039522647857666 0.47346314787864685 3.236246347427368\n",
      " 0.8359470367431641 0.01804456114768982 0.5306334495544434\n",
      " 0.03730051964521408]\n",
      "\n",
      "Image: seq_val/IMG_9855_0.jpg\n",
      "Actual   : [3.05066306081 0.793980114881 -2.5417651575 0.843836366208\n",
      " -0.0442627400585 -0.534771572152 0.000602100466462]\n",
      "Predicted: [2.9770278930664062 0.29610249400138855 -1.572283148765564\n",
      " 0.8443185687065125 -0.023139314725995064 -0.5255467295646667\n",
      " -0.012984629720449448]\n",
      "\n",
      "Image: seq_val/S_2_3_190_0.jpg\n",
      "Actual   : [-0.180992333659 -1.44313853674 0.950715972752 0.117110434745\n",
      " -0.0403514971435 -0.989249008288 -0.0777386799038]\n",
      "Predicted: [-0.06494618207216263 -1.0276182889938354 0.8118519186973572\n",
      " 0.1048332005739212 -0.056338462978601456 -0.9821156859397888\n",
      " -0.09598875790834427]\n",
      "\n",
      "Image: seq_val/IMG_8194_0.jpg\n",
      "Actual   : [-2.15635844836 0.949115291518 0.961639716211 0.905629317554\n",
      " 0.00970393385832 0.420135282993 0.0568129988537]\n",
      "Predicted: [-2.025355100631714 0.7724671959877014 1.2578638792037964\n",
      " 0.8681427240371704 0.024876493960618973 0.48230063915252686\n",
      " 0.04378250613808632]\n",
      "\n",
      "Image: seq_val/IMG_9887_0.jpg\n",
      "Actual   : [3.30523347591 0.631426509559 0.151043591316 0.178053273531\n",
      " 0.0240791685262 0.982591778034 0.0472294735004]\n",
      "Predicted: [2.969179630279541 0.45616236329078674 0.0272204726934433\n",
      " 0.20517617464065552 0.0377994067966938 0.9615460634231567\n",
      " 0.053077418357133865]\n",
      "\n",
      "Image: seq_val/S_2_3_142_0.jpg\n",
      "Actual   : [-1.27186911517 -0.235550724605 -3.37433422858 0.961571458627\n",
      " -0.0369710927485 0.270940851285 0.0245870567409]\n",
      "Predicted: [-0.9071664214134216 -0.21819965541362762 -2.983083963394165\n",
      " 0.9561692476272583 -0.0028148479759693146 0.2718968689441681\n",
      " 0.031924378126859665]\n",
      "\n",
      "Image: seq_val/IMG_9946_0.jpg\n",
      "Actual   : [-0.435699920992 0.892000302263 0.456241609148 0.0164295103416\n",
      " 0.0372871990483 0.996189370722 0.077113381708]\n",
      "Predicted: [-0.23824119567871094 0.4595719277858734 0.28343433141708374\n",
      " 0.01717078685760498 0.03876691311597824 0.9973024129867554\n",
      " 0.07857552915811539]\n",
      "\n",
      "Image: seq_val/IMG_9746_0.jpg\n",
      "Actual   : [0.363335081766 -1.59654161777 2.66639345122 0.0441637364382\n",
      " 0.0325489557776 0.998003267481 0.0312986893971]\n",
      "Predicted: [0.5995165109634399 -0.6873945593833923 2.004215717315674\n",
      " 0.09144334495067596 0.033094413578510284 0.9639697074890137\n",
      " 0.06211936101317406]\n",
      "\n",
      "Image: seq_val/IMG_9644_0.jpg\n",
      "Actual   : [-2.00561237252 -1.38956070573 2.00763166604 0.990270243696\n",
      " 0.0133003146949 0.137097789718 0.0198025789389]\n",
      "Predicted: [-2.0370700359344482 -1.2442890405654907 1.9971797466278076\n",
      " 0.9961898326873779 0.0006778016686439514 0.08488877862691879\n",
      " 0.034009285271167755]\n",
      "\n",
      "Image: seq_val/IMG_9629_0.jpg\n",
      "Actual   : [-2.24369647381 -1.51907809837 4.1038686144 0.417537308704 0.0586960912919\n",
      " 0.905084001894 0.0551390444507]\n",
      "Predicted: [-2.036761999130249 -0.9892004132270813 3.0252833366394043\n",
      " 0.378939688205719 0.0396893210709095 0.9081109762191772\n",
      " 0.05523444712162018]\n",
      "\n",
      "Image: seq_val/IMG_9894_0.jpg\n",
      "Actual   : [4.20683251281 0.750552641363 -2.02637464801 0.243806560662\n",
      " -0.0459320395533 -0.967036138622 -0.0573560399559]\n",
      "Predicted: [3.116056203842163 0.496794730424881 -0.8068850040435791\n",
      " 0.26555365324020386 -0.04039108753204346 -0.9402034878730774\n",
      " -0.04470441862940788]\n",
      "\n",
      "Image: seq_val/IMG_9665_0.jpg\n",
      "Actual   : [1.61352929193 -1.5545815375 0.103735283858 0.981194773575\n",
      " -0.0319412803228 -0.188398969218 0.0272470056824]\n",
      "Predicted: [2.0941293239593506 -1.4502859115600586 0.07525236904621124\n",
      " 0.9664317965507507 -0.024946406483650208 -0.22078454494476318\n",
      " 0.015683060511946678]\n",
      "\n",
      "Image: seq_val/S_2_3_208_0.jpg\n",
      "Actual   : [-0.721670892947 -1.22113771663 -1.8709271786 0.705651095431\n",
      " -0.042013921487 -0.706579888151 -0.0321904268336]\n",
      "Predicted: [-0.8211108446121216 -0.06940118968486786 -2.546257734298706\n",
      " 0.3931242525577545 -0.027739528566598892 -0.5371331572532654\n",
      " -0.02044318988919258]\n",
      "\n",
      "Image: seq_val/IMG_9822_0.jpg\n",
      "Actual   : [4.3825281077 0.483401046441 1.21262363957 0.956477441346 0.00152743296373\n",
      " 0.29071811704 0.0251306102193]\n",
      "Predicted: [4.650435447692871 0.07919257879257202 1.2409073114395142\n",
      " 0.9304662346839905 0.0022839419543743134 0.2822781205177307\n",
      " 0.03108201175928116]\n",
      "\n",
      "Image: seq_val/IMG_9866_0.jpg\n",
      "Actual   : [5.38172754549 0.613082219211 0.828478205156 0.336961568626\n",
      " -0.0106846064412 -0.940304016457 -0.046595032882]\n",
      "Predicted: [5.461556434631348 0.5892130136489868 1.1089307069778442 0.2874775826931\n",
      " -0.038508594036102295 -0.9479188919067383 -0.043488550931215286]\n",
      "\n",
      "Image: seq_val/IMG_9800_0.jpg\n",
      "Actual   : [-2.58048596603 0.914760540249 2.31346024383 0.987293406359\n",
      " 0.0133104592846 0.1565113574 0.0240573572465]\n",
      "Predicted: [-2.503540515899658 0.6857258081436157 2.039293050765991\n",
      " 0.9815364480018616 0.008759677410125732 0.14008483290672302\n",
      " 0.027156371623277664]\n",
      "\n",
      "Image: seq_val/S_2_3_30_0.jpg\n",
      "Actual   : [-0.555451070889 0.923293932997 -0.36701508688 0.38456611287\n",
      " -0.053623146598 -0.919075299973 -0.0673353990121]\n",
      "Predicted: [-0.7895947694778442 0.1979077011346817 -0.014694079756736755\n",
      " 0.43327710032463074 -0.04103860631585121 -0.8925004005432129\n",
      " -0.046980734914541245]\n",
      "\n",
      "Image: seq_val/IMG_9701_0.jpg\n",
      "Actual   : [3.77147237743 -1.7084223016 1.00034337665 0.35730320461 -0.04584399906\n",
      " -0.93239051485 -0.0296761780564]\n",
      "Predicted: [3.5987257957458496 -0.987349808216095 0.35202670097351074\n",
      " 0.3501424789428711 -0.04071014001965523 -0.9099382758140564\n",
      " -0.03772328048944473]\n",
      "\n",
      "Image: seq_val/S_2_3_27_0.jpg\n",
      "Actual   : [-0.283717939605 0.770787000706 -2.33278446802 0.993285975993\n",
      " 0.108264519739 0.0387702285037 0.0125949610118]\n",
      "Predicted: [-0.41097578406333923 0.7188345789909363 -2.6461706161499023\n",
      " 0.9811533093452454 0.0901821106672287 -0.006398908793926239\n",
      " 0.013536855578422546]\n",
      "\n",
      "Image: seq_val/S_2_3_9_0.jpg\n",
      "Actual   : [-1.40801461878 1.03206829565 -1.39045940892 0.769715520318\n",
      " 0.00465939207021 -0.636371100998 -0.0504790021784]\n",
      "Predicted: [-1.1058231592178345 0.6320287585258484 -1.482466697692871\n",
      " 0.7448656558990479 -0.017061183229088783 -0.6397750377655029\n",
      " -0.03600682318210602]\n",
      "\n",
      "Image: seq_val/IMG_8280_0.jpg\n",
      "Actual   : [-0.803465218505 0.789828051064 2.62705198655 0.938337007089\n",
      " 0.0338861689158 0.342195759844 0.0357414413323]\n",
      "Predicted: [-0.9972270727157593 0.6760422587394714 2.5807406902313232\n",
      " 0.9376713037490845 0.022081434726715088 0.32704150676727295\n",
      " 0.034073859453201294]\n",
      "\n",
      "Image: seq_val/IMG_9753_0.jpg\n",
      "Actual   : [-0.71186877962 -1.31686394543 -0.724278203963 0.59603248129\n",
      " -0.0429568784009 -0.801526678883 -0.0213300465143]\n",
      "Predicted: [-0.2997395396232605 -1.052193522453308 -0.45711344480514526\n",
      " 0.5336579084396362 -0.04443939030170441 -0.8399417400360107\n",
      " -0.03513072803616524]\n",
      "\n",
      "Image: seq_val/S_2_3_55_0.jpg\n",
      "Actual   : [0.438390823619 0.897721984075 -1.02568113492 0.675787720802\n",
      " 0.0383780222953 0.733835319157 0.0576524776201]\n",
      "Predicted: [0.598596453666687 0.792620837688446 -0.5612196326255798\n",
      " 0.6688931584358215 0.035466331988573074 0.7276721596717834\n",
      " 0.05375015363097191]\n",
      "\n",
      "Image: seq_val/IMG_9849_0.jpg\n",
      "Actual   : [4.65341065399 0.457216134112 2.0224131957 0.748589661694 -0.0492119701314\n",
      " -0.661174895275 -0.00628158089423]\n",
      "Predicted: [3.897678852081299 0.4003952741622925 1.4619331359863281\n",
      " 0.7462518811225891 -0.03453751280903816 -0.6550643444061279\n",
      " -0.008435389026999474]\n",
      "\n",
      "Image: seq_val/IMG_9699_0.jpg\n",
      "Actual   : [4.93970894101 -1.82409641989 1.63133054569 0.267262200121\n",
      " -0.0472861519599 -0.961768882815 -0.0365452085613]\n",
      "Predicted: [5.288483142852783 -1.714617133140564 1.6220084428787231\n",
      " 0.2825208306312561 -0.041306719183921814 -0.9471048712730408\n",
      " -0.04097309708595276]\n",
      "\n",
      "Image: seq_val/IMG_9886_0.jpg\n",
      "Actual   : [4.14341237677 0.604447408147 -0.169516362397 0.238413846003\n",
      " 0.0278527338235 0.970260711741 0.0312604303232]\n",
      "Predicted: [4.031630516052246 0.5798868536949158 -0.5303489565849304\n",
      " 0.26686927676200867 0.036567218601703644 0.941838264465332\n",
      " 0.04812758415937424]\n",
      "\n",
      "Image: seq_val/IMG_9863_0.jpg\n",
      "Actual   : [4.33156062845 0.401365333602 3.79119635133 0.354077835334\n",
      " -0.0364519771323 -0.934026555715 -0.0299087463284]\n",
      "Predicted: [4.4068803787231445 0.31481292843818665 3.107607841491699\n",
      " 0.3556779623031616 -0.039719581604003906 -0.917472243309021\n",
      " -0.03070085495710373]\n",
      "\n",
      "Image: seq_val/IMG_8207_0.jpg\n",
      "Actual   : [-3.25878543266 0.953634094568 2.44254969353 0.999342379168\n",
      " 0.0222222085884 0.0272737638877 0.00878205254451]\n",
      "Predicted: [-3.1257259845733643 0.8218801617622375 2.2924647331237793\n",
      " 0.9844769835472107 0.007290281355381012 -7.386505603790283e-05\n",
      " 0.018304936587810516]\n",
      "\n",
      "Image: seq_val/IMG_9624_0.jpg\n",
      "Actual   : [-3.06293720193 -1.38896811374 2.99221987009 0.305328739315 0.029437442694\n",
      " 0.949818374432 0.0612605379355]\n",
      "Predicted: [-2.381208896636963 -0.7993989586830139 3.2321078777313232\n",
      " 0.2886647582054138 0.039158258587121964 0.9418435096740723\n",
      " 0.05145929753780365]\n",
      "\n",
      "Image: seq_val/IMG_8283_0.jpg\n",
      "Actual   : [-0.408605258784 0.674363674305 3.9699413847 0.78211467907 0.035535591848\n",
      " 0.619986837358 0.0514798212668]\n",
      "Predicted: [-0.4538657069206238 0.48370829224586487 3.857133626937866\n",
      " 0.8096479177474976 0.029596704989671707 0.5753973126411438\n",
      " 0.04476045444607735]\n",
      "\n",
      "Image: seq_val/IMG_9910_0.jpg\n",
      "Actual   : [4.0928790344 0.55742779301 0.752047394382 0.494688392969 0.0367341840318\n",
      " 0.866048803842 0.0623976197376]\n",
      "Predicted: [3.357311487197876 0.3778410851955414 0.9647853374481201\n",
      " 0.49871885776519775 0.03732194006443024 0.8824900984764099\n",
      " 0.05768105387687683]\n",
      "\n",
      "Image: seq_val/IMG_9662_0.jpg\n",
      "Actual   : [4.50704452719 -1.95132794895 3.69198454766 0.797324057807\n",
      " 0.00169801569027 0.602763683536 0.0307799511832]\n",
      "Predicted: [3.643526792526245 -1.6833062171936035 3.702057123184204\n",
      " 0.7874281406402588 0.017490480095148087 0.5830544233322144\n",
      " 0.042336877435445786]\n",
      "\n",
      "Image: seq_val/S_2_3_111_0.jpg\n",
      "Actual   : [0.0046730305371 -0.147979898138 -4.04487309689 0.879721476127\n",
      " 0.00965049075405 0.47335050274 0.0440033410587]\n",
      "Predicted: [-0.30719318985939026 -0.043039627373218536 -3.8299567699432373\n",
      " 0.8835506439208984 0.0038134753704071045 0.4448045492172241\n",
      " 0.040469370782375336]\n",
      "\n",
      "Image: seq_val/S_2_3_172_0.jpg\n",
      "Actual   : [-0.147590274947 -1.32959240701 -0.802496008953 0.322869243918\n",
      " -0.0243636318725 -0.941145304945 -0.0969916478545]\n",
      "Predicted: [-0.2998639643192291 -1.1703602075576782 -0.6833232045173645\n",
      " 0.3011763393878937 -0.047600157558918 -0.9482053518295288\n",
      " -0.06525854021310806]\n",
      "\n",
      "Image: seq_val/S_2_3_210_0.jpg\n",
      "Actual   : [-0.790086991917 -1.28837715981 -1.8743273282 0.851689555654\n",
      " 0.0321020004346 0.521573079875 0.0394447044387]\n",
      "Predicted: [-1.1405798196792603 -0.19941775500774384 -2.1529760360717773\n",
      " 0.8682549595832825 0.022790715098381042 0.49755263328552246\n",
      " 0.04683196172118187]\n",
      "\n",
      "Image: seq_val/IMG_9673_0.jpg\n",
      "Actual   : [2.35765309916 -1.67913575153 1.43784015661 0.914486782576\n",
      " -0.0454602787309 -0.401919403212 0.0103961951748]\n",
      "Predicted: [2.907855272293091 -1.9678133726119995 0.9385981559753418\n",
      " 0.9016969799995422 -0.03913098946213722 -0.42362871766090393\n",
      " 0.006883219815790653]\n",
      "\n",
      "Image: seq_val/IMG_9674_0.jpg\n",
      "Actual   : [3.71368503725 -1.72508945 0.734590410564 0.980537658702 -0.0163670107024\n",
      " -0.195136817108 0.0141295235506]\n",
      "Predicted: [3.7441766262054443 -1.9284591674804688 0.8578104972839355\n",
      " 0.9706516265869141 -0.02651245705783367 -0.1987285315990448\n",
      " 0.02039279229938984]\n",
      "\n",
      "Image: seq_val/IMG_8200_0.jpg\n",
      "Actual   : [-2.09700292831 0.849682314113 2.86314934479 0.765630214538\n",
      " 0.0568365151337 0.638285650963 0.0563152990575]\n",
      "Predicted: [-2.02997088432312 0.7142734527587891 3.092257022857666 0.745919942855835\n",
      " 0.04193094000220299 0.6507248878479004 0.04935610294342041]\n",
      "\n",
      "Image: seq_val/IMG_9733_0.jpg\n",
      "Actual   : [-1.49179798798 -1.19840737139 -1.14511274514 0.34750166559\n",
      " -0.0438756836629 -0.924487837048 -0.150465131994]\n",
      "Predicted: [-1.3784630298614502 -1.2548640966415405 -0.7156327962875366\n",
      " 0.32254844903945923 -0.043169617652893066 -0.9255602359771729\n",
      " -0.1107398122549057]\n",
      "\n",
      "Image: seq_val/IMG_9903_0.jpg\n",
      "Actual   : [5.41265145854 0.688019238248 -0.774528618369 0.534952069862\n",
      " 0.0398926114292 0.841615956968 0.0625894837888]\n",
      "Predicted: [4.8755340576171875 0.36166688799858093 -0.7438646554946899\n",
      " 0.5396820306777954 0.04059198498725891 0.8388040065765381\n",
      " 0.06394656002521515]\n",
      "\n",
      "Image: seq_val/IMG_9901_0.jpg\n",
      "Actual   : [4.00595349733 0.70269769975 0.492648168795 0.000616665212414\n",
      " 0.0298531002111 0.996718414116 0.0752383889906]\n",
      "Predicted: [4.1290717124938965 0.5377011299133301 0.33968091011047363\n",
      " 0.041327208280563354 0.0361805222928524 1.0230202674865723\n",
      " 0.05254122242331505]\n",
      "\n",
      "Image: seq_val/IMG_9740_0.jpg\n",
      "Actual   : [-0.448764256783 -1.3273246356 0.0521048349457 0.0182283908393\n",
      " -0.0557011640079 -0.989907610818 -0.129027237975]\n",
      "Predicted: [-0.2685007154941559 -0.8288516402244568 0.10822945833206177\n",
      " 0.038736969232559204 -0.053549665957689285 -0.9899033904075623\n",
      " -0.14225129783153534]\n",
      "\n",
      "Image: seq_val/IMG_9738_0.jpg\n",
      "Actual   : [-1.55045008086 -1.22136071046 -1.16999557308 0.0862438155126\n",
      " -0.0566675509462 -0.993178525769 -0.0542882022765]\n",
      "Predicted: [-0.8376744985580444 -0.6652823686599731 -1.1865465641021729\n",
      " 0.07966317236423492 -0.05994055047631264 -0.9807091355323792\n",
      " -0.04311614856123924]\n",
      "\n",
      "Image: seq_val/S_2_3_58_0.jpg\n",
      "Actual   : [0.436589073027 0.900186335786 -1.06277332107 0.511673614547\n",
      " 0.0371501953974 0.856538020349 0.0561479728502]\n",
      "Predicted: [0.4484551250934601 0.783269464969635 -0.491698682308197\n",
      " 0.4384600520133972 0.04092328995466232 0.8948269486427307\n",
      " 0.060963794589042664]\n",
      "\n",
      "Image: seq_val/IMG_9621_0.jpg\n",
      "Actual   : [-2.39194308953 -1.22658898155 -0.879682184069 0.843882891924\n",
      " -0.00507585145185 0.533152932428 0.0598652745219]\n",
      "Predicted: [-2.38232159614563 -0.7328326106071472 -0.2051617056131363\n",
      " 0.9040685892105103 0.006826460361480713 0.374044269323349\n",
      " 0.043770402669906616]\n",
      "\n",
      "Image: seq_val/IMG_9889_0.jpg\n",
      "Actual   : [1.59555534917 0.730009293659 0.123255895621 0.115004994765\n",
      " -0.0425063757967 -0.991993337006 -0.0302700930283]\n",
      "Predicted: [1.9403005838394165 0.5771128535270691 -0.19133508205413818\n",
      " 0.06636062264442444 -0.05718991905450821 -0.9956226348876953\n",
      " -0.036803361028432846]\n",
      "\n",
      "Image: seq_val/S_2_3_23_0.jpg\n",
      "Actual   : [-0.303671590964 0.777821024656 -2.30812613025 0.986343835986\n",
      " 0.135502104736 0.0864692407578 0.0358899321256]\n",
      "Predicted: [-0.45611292123794556 0.7112327814102173 -2.1741795539855957\n",
      " 0.9808570742607117 0.1047484427690506 0.04154522716999054\n",
      " 0.02054481953382492]\n",
      "\n",
      "Image: seq_val/IMG_9911_0.jpg\n",
      "Actual   : [3.4163056516 0.580763991072 1.33392529414 0.37085102841 0.0464391859975\n",
      " 0.925136378247 0.0666002880982]\n",
      "Predicted: [3.1358823776245117 0.060996923595666885 1.6692636013031006\n",
      " 0.3771335482597351 0.035946547985076904 0.9324874877929688\n",
      " 0.05044425651431084]\n",
      "\n",
      "Image: seq_val/S_2_3_93_0.jpg\n",
      "Actual   : [-2.03817212051 0.06920828131 -4.43577792291 0.852942345325\n",
      " 0.0594537448196 -0.509974310095 -0.0942380540011]\n",
      "Predicted: [-2.1452622413635254 0.4033714532852173 -4.552596092224121\n",
      " 0.869227409362793 0.07102334499359131 -0.49805527925491333\n",
      " -0.08959470689296722]\n",
      "\n",
      "Image: seq_val/IMG_9705_0.jpg\n",
      "Actual   : [6.78610892883 -1.64345449417 0.458036714799 0.266625983197\n",
      " 0.0113815786931 0.963275164639 0.0296985174792]\n",
      "Predicted: [5.281023979187012 -0.44301530718803406 0.8232266306877136\n",
      " 0.36127734184265137 0.02422754094004631 0.9017425775527954\n",
      " 0.03628825023770332]\n",
      "\n",
      "Image: seq_val/IMG_9793_0.jpg\n",
      "Actual   : [-1.69680885061 0.910474650084 1.67166721641 0.984740190549\n",
      " 0.00258989458428 0.17176362238 0.0278802365722]\n",
      "Predicted: [-1.5342506170272827 0.8205219507217407 1.2644277811050415\n",
      " 0.9735175371170044 0.01136280968785286 0.18852433562278748\n",
      " 0.029839303344488144]\n",
      "\n",
      "Image: seq_val/IMG_9677_0.jpg\n",
      "Actual   : [6.60879418193 -1.73785994454 1.27477168615 0.954135134908\n",
      " 0.00474898502327 0.284731997929 0.0923649329106]\n",
      "Predicted: [6.650437831878662 -2.109278440475464 1.4041833877563477\n",
      " 0.9405595660209656 0.006445363163948059 0.2812809348106384\n",
      " 0.04494364932179451]\n",
      "\n",
      "Image: seq_val/IMG_9860_0.jpg\n",
      "Actual   : [3.98442227362 0.455902142831 2.90315565556 0.469517497371\n",
      " -0.0444210110135 -0.881798187401 -0.00347133106679]\n",
      "Predicted: [4.477804183959961 0.4225656986236572 2.9285407066345215 0.445941686630249\n",
      " -0.03921039402484894 -0.8790311217308044 -0.02597704529762268]\n",
      "\n",
      "Image: seq_val/S_2_3_76_0.jpg\n",
      "Actual   : [0.0576103388682 0.373870800154 -3.19328208556 0.261900980933\n",
      " 0.0330964942467 0.964428639001 0.0137803675522]\n",
      "Predicted: [-0.18014270067214966 0.24290165305137634 -2.8413965702056885\n",
      " 0.1995190978050232 0.040323588997125626 0.9679367542266846\n",
      " 0.05638373643159866]\n",
      "\n",
      "Image: seq_val/IMG_9844_0.jpg\n",
      "Actual   : [6.43043745671 0.585574904252 -0.399988138845 0.988858932982\n",
      " -0.0016507375238 -0.13696788915 0.0582673413525]\n",
      "Predicted: [6.1918044090271 0.4036223888397217 -0.03178061544895172\n",
      " 0.9593150019645691 -0.008889424614608288 -0.20788118243217468\n",
      " 0.01434258371591568]\n",
      "\n",
      "Image: seq_val/S_2_3_168_0.jpg\n",
      "Actual   : [-1.16819421472 -0.478131514913 -3.06865389756 0.96412757048\n",
      " -0.02942913665 -0.26378174655 -0.00333825444451]\n",
      "Predicted: [-1.1588767766952515 -0.5412338376045227 -3.1444339752197266\n",
      " 0.9521761536598206 -0.030393565073609352 -0.2640305161476135\n",
      " -0.001146458089351654]\n",
      "\n",
      "Image: seq_val/IMG_9791_0.jpg\n",
      "Actual   : [-1.74281418154 0.91094192435 1.64913365234 0.939909070506 0.0101653774517\n",
      " 0.33814825459 0.0460799544334]\n",
      "Predicted: [-1.495043158531189 0.667863667011261 1.632360577583313 0.9445434808731079\n",
      " 0.014250162988901138 0.33223170042037964 0.03805571794509888]\n",
      "\n",
      "Image: seq_val/S_2_3_205_0.jpg\n",
      "Actual   : [-0.821988816034 -1.34003106447 0.0913331834785 0.262676359299\n",
      " 0.0552618501445 0.959311973389 0.0875659517002]\n",
      "Predicted: [-0.680778443813324 -0.8085509538650513 0.1669849008321762\n",
      " 0.25541064143180847 0.04832762852311134 0.9460943937301636\n",
      " 0.08025485277175903]\n",
      "\n",
      "Image: seq_val/IMG_9916_0.jpg\n",
      "Actual   : [-0.775252901683 0.798348170391 1.94535623816 0.196405366998\n",
      " -0.0510141052775 -0.977710366072 -0.0538974299227]\n",
      "Predicted: [-0.3397006690502167 0.47677239775657654 1.4421807527542114\n",
      " 0.14645126461982727 -0.05235406383872032 -0.9642111659049988\n",
      " -0.05011383071541786]\n",
      "\n",
      "Image: seq_val/S_2_3_19_0.jpg\n",
      "Actual   : [-0.209932352868 0.785362100621 -2.34323390526 0.942420285767\n",
      " 0.108134181348 -0.313456334114 -0.0435445794943]\n",
      "Predicted: [-0.5647690892219543 0.8156822919845581 -2.130427122116089\n",
      " 0.928127646446228 0.06919121742248535 -0.3229951858520508\n",
      " -0.026854854077100754]\n",
      "\n",
      "Image: seq_val/IMG_8298_0.jpg\n",
      "Actual   : [0.900699212658 0.694271600251 2.37090652038 0.899686508491\n",
      " 0.0294288103097 0.434077005093 0.0357111357026]\n",
      "Predicted: [0.7741429805755615 0.7026033997535706 2.49336576461792 0.8719195127487183\n",
      " 0.01665457710623741 0.4340111017227173 0.03696110472083092]\n",
      "\n",
      "Image: seq_val/IMG_9947_0.jpg\n",
      "Actual   : [-0.410845360394 0.86939901207 0.947479777563 0.0139090000418\n",
      " 0.0478397174569 0.996757772118 0.063181048372]\n",
      "Predicted: [-0.2579557001590729 0.523057758808136 0.8833921551704407\n",
      " 0.003251373767852783 0.03605510666966438 0.9933422803878784\n",
      " 0.06567566096782684]\n",
      "\n",
      "Image: seq_val/S_2_3_202_0.jpg\n",
      "Actual   : [-0.669741245892 -1.35315511998 0.0642853482056 0.160478382416\n",
      " -0.0562507490479 -0.982233326412 -0.0793740164892]\n",
      "Predicted: [-0.19318512082099915 -1.0725650787353516 0.20381994545459747\n",
      " 0.14365187287330627 -0.051831841468811035 -0.9808763861656189\n",
      " -0.07403925061225891]\n",
      "\n",
      "Image: seq_val/S_2_3_66_0.jpg\n",
      "Actual   : [-0.0782185285316 0.602088104789 -2.77611563977 0.146551358091\n",
      " -0.0703431051018 -0.985857481434 -0.0407378608574]\n",
      "Predicted: [-0.48469212651252747 0.5520695447921753 -1.888623833656311\n",
      " 0.19919250905513763 -0.05431746691465378 -0.9600610733032227\n",
      " -0.0625738799571991]\n",
      "\n",
      "Image: seq_val/IMG_9846_0.jpg\n",
      "Actual   : [5.64209269656 0.535899550583 2.1298962188 0.894261726814 -0.0305392221611\n",
      " -0.44649875724 0.00147636303135]\n",
      "Predicted: [4.854831695556641 0.6935856938362122 1.8763920068740845\n",
      " 0.8582482933998108 -0.03147289156913757 -0.46816903352737427\n",
      " 0.002032824791967869]\n",
      "\n",
      "Image: seq_val/IMG_9761_0.jpg\n",
      "Actual   : [-0.568399539661 -1.22315548519 -1.98478359867 0.90353150829\n",
      " -0.0392304838337 -0.426189282604 -0.0213184910106]\n",
      "Predicted: [-0.7364132404327393 -0.8692887425422668 -1.5415464639663696\n",
      " 0.8818426132202148 -0.036547139286994934 -0.4553026556968689\n",
      " -0.008527448400855064]\n",
      "\n",
      "Image: seq_val/S_2_3_6_0.jpg\n",
      "Actual   : [-0.350953604871 0.8797274059 0.0690835810892 0.175149263407\n",
      " -0.0541832146053 -0.977906938476 -0.100423774393]\n",
      "Predicted: [-0.2267933487892151 0.7929162383079529 0.10507629811763763\n",
      " 0.1536058485507965 -0.0508403480052948 -0.9597251415252686\n",
      " -0.07525664567947388]\n",
      "\n",
      "Image: seq_val/IMG_9765_0.jpg\n",
      "Actual   : [-0.584597947207 -1.34003073986 -0.325114704919 0.00502914456622\n",
      " 0.0384163866558 0.996543329875 0.0734866016761]\n",
      "Predicted: [-0.20608478784561157 -0.4540751874446869 -0.9021791815757751\n",
      " 0.047576069831848145 0.03390982747077942 0.85048508644104\n",
      " 0.07697656005620956]\n",
      "\n",
      "Image: seq_val/S_2_3_98_0.jpg\n",
      "Actual   : [-0.247263761918 -0.015848446451 -4.5097119312 0.973842791912\n",
      " 0.224695795617 0.0335551721008 -0.00400830374551]\n",
      "Predicted: [-0.5689020752906799 0.009519398212432861 -4.023855686187744\n",
      " 0.9684478640556335 0.20303669571876526 -0.010446947067975998\n",
      " 0.0009154397994279861]\n",
      "\n",
      "Image: seq_val/IMG_9771_0.jpg\n",
      "Actual   : [-2.80286555498 0.976484036778 1.57743747365 0.325049863838\n",
      " 0.0407964729579 0.943254130072 0.0543127969719]\n",
      "Predicted: [-2.4479176998138428 0.3298283815383911 2.126835584640503\n",
      " 0.3454304039478302 0.04555023834109306 0.9096563458442688\n",
      " 0.05163231119513512]\n",
      "\n",
      "Image: seq_val/IMG_9875_0.jpg\n",
      "Actual   : [6.27870254743 0.624281956987 0.767836712834 0.912149058956\n",
      " -0.0136103205091 -0.409132658017 -0.0202316970376]\n",
      "Predicted: [5.223615646362305 0.42968592047691345 -0.20067639648914337\n",
      " 0.8826541900634766 -0.024054361507296562 -0.44987839460372925\n",
      " -0.0023056617937982082]\n",
      "\n",
      "Image: seq_val/S_2_3_209_0.jpg\n",
      "Actual   : [-0.74512930895 -1.29151092797 -1.84074391412 0.918188796582\n",
      " 0.0298076596163 0.393959654022 0.0289245269948]\n",
      "Predicted: [-0.9061686396598816 -0.9437974691390991 -1.9340450763702393\n",
      " 0.9142950177192688 0.014591295272111893 0.3771781623363495\n",
      " 0.043134383857250214]\n",
      "\n",
      "Image: seq_val/S_2_3_136_0.jpg\n",
      "Actual   : [-1.20929812355 -0.228866030914 -3.39864469911 0.917111416672\n",
      " -0.0416484489263 -0.396398271946 -0.00636129785511]\n",
      "Predicted: [-1.170011281967163 -0.44011548161506653 -3.553626775741577\n",
      " 0.8956887722015381 -0.03333313763141632 -0.4169209897518158\n",
      " -0.014838432893157005]\n",
      "\n",
      "Image: seq_val/S_2_3_146_0.jpg\n",
      "Actual   : [-1.28891058959 0.0767803969664 -4.6529711293 0.970740828429\n",
      " 0.237031871661 0.0379027822597 -0.00644320848559]\n",
      "Predicted: [-1.087756872177124 -0.06217991188168526 -4.601275444030762\n",
      " 0.9683226346969604 0.21135275065898895 0.03743048012256622\n",
      " -0.007306508254259825]\n",
      "\n",
      "Image: seq_val/IMG_9715_0.jpg\n",
      "Actual   : [1.52986564727 -1.69690648728 3.01141913918 0.0304943537615\n",
      " -0.0494995813779 -0.997566876289 -0.0384735384814]\n",
      "Predicted: [1.429882287979126 -1.0369213819503784 1.7107322216033936\n",
      " 0.11264447867870331 -0.052364178001880646 -0.9506659507751465\n",
      " -0.05147256329655647]\n",
      "\n",
      "Image: seq_val/S_2_3_37_0.jpg\n",
      "Actual   : [-1.03326233282 0.970031381184 -0.901183959663 0.197405616951\n",
      " -0.0744769747748 -0.976928064451 0.0331022584047]\n",
      "Predicted: [-0.5881720781326294 0.8521328568458557 -0.9208008050918579\n",
      " 0.18448686599731445 -0.05638713762164116 -0.9594184756278992\n",
      " -0.015110373497009277]\n",
      "\n",
      "Image: seq_val/S_2_3_201_0.jpg\n",
      "Actual   : [-0.632929816721 -1.34825214985 0.0587148122543 0.289589389235\n",
      " -0.0490799849143 -0.952340378995 -0.0823221917809]\n",
      "Predicted: [-0.3144703805446625 -1.3699164390563965 0.23863057792186737\n",
      " 0.26116904616355896 -0.04659414291381836 -0.9435659646987915\n",
      " -0.07021792978048325]\n",
      "\n",
      "Image: seq_val/S_2_3_181_0.jpg\n",
      "Actual   : [-0.854142086497 -1.16850356564 -1.3394203126 0.151586035686\n",
      " 0.0714086324801 0.948490631298 0.268864284227]\n",
      "Predicted: [-1.0649055242538452 -0.6562480330467224 -1.7415837049484253\n",
      " 0.11040845513343811 0.09788335859775543 0.9551800489425659\n",
      " 0.23970600962638855]\n",
      "\n",
      "Image: seq_val/IMG_9702_0.jpg\n",
      "Actual   : [3.34021634561 -1.64551871007 0.503197808578 0.47023096296\n",
      " -0.0436916894953 -0.880928778792 -0.030632734748]\n",
      "Predicted: [3.648301839828491 -1.2919148206710815 0.5680937170982361\n",
      " 0.4528765082359314 -0.03868354111909866 -0.8636822700500488\n",
      " -0.03352522850036621]\n",
      "\n",
      "Image: seq_val/IMG_9930_0.jpg\n",
      "Actual   : [-0.366292241113 0.858460069364 1.26263005868 0.228513935431\n",
      " -0.0492011540441 -0.971145143737 -0.0473047307389]\n",
      "Predicted: [0.12375982850790024 0.37856629490852356 1.1024690866470337\n",
      " 0.1706583946943283 -0.05048435553908348 -0.9643282890319824\n",
      " -0.05013743415474892]\n",
      "\n",
      "Image: seq_val/S_2_3_35_0.jpg\n",
      "Actual   : [-1.01538358949 0.977897299976 -0.919682777173 0.262677550917\n",
      " -0.0549724061139 -0.963224304785 -0.0133220674869]\n",
      "Predicted: [-0.6925285458564758 0.9956756234169006 -0.8511203527450562\n",
      " 0.25059449672698975 -0.05343395471572876 -0.9485414028167725\n",
      " -0.03020254150032997]\n",
      "\n",
      "Image: seq_val/IMG_9739_0.jpg\n",
      "Actual   : [-0.438048462262 -1.30035704032 -0.328052324319 0.0112752238722\n",
      " -0.0661253536602 -0.985861002597 -0.153552565883]\n",
      "Predicted: [-0.5299914479255676 -0.6214090585708618 -0.508385181427002\n",
      " 0.02440941333770752 -0.05114788934588432 -0.9797910451889038\n",
      " -0.14531651139259338]\n",
      "\n",
      "Image: seq_val/IMG_9898_0.jpg\n",
      "Actual   : [4.19409151706 0.619627967862 -0.129593056872 0.341557187643\n",
      " 0.0686911522149 0.935072435482 0.0652667877153]\n",
      "Predicted: [4.296206474304199 0.40655040740966797 -0.11044114828109741\n",
      " 0.3142709732055664 0.04006163403391838 0.9280358552932739\n",
      " 0.054040562361478806]\n",
      "\n",
      "Image: seq_val/IMG_9772_0.jpg\n",
      "Actual   : [-2.58925829364 1.19366572413 -2.53018487408 0.927053581364 0.030058466868\n",
      " 0.370963528815 0.0453233509182]\n",
      "Predicted: [-2.4238948822021484 0.4763694107532501 -2.3682830333709717\n",
      " 0.9201475381851196 0.015305232256650925 0.3587767779827118\n",
      " 0.039115313440561295]\n",
      "\n",
      "Image: seq_val/IMG_9838_0.jpg\n",
      "Actual   : [7.55154143909 0.419682277841 1.54726575863 0.962922567031\n",
      " -0.00175781430061 0.269688600863 0.00671554591165]\n",
      "Predicted: [6.705284118652344 -0.019291851669549942 1.1166242361068726\n",
      " 0.9314205646514893 0.008151262998580933 0.2980002760887146\n",
      " 0.03081779181957245]\n",
      "\n",
      "Image: seq_val/S_2_3_200_0.jpg\n",
      "Actual   : [-0.613761505675 -1.35352756759 0.0710119381091 0.406725934546\n",
      " -0.0528720280952 -0.908925272282 -0.0750560605171]\n",
      "Predicted: [-1.060673475265503 -0.516569197177887 0.06586359441280365\n",
      " 0.41665375232696533 -0.043375369161367416 -0.9029189348220825\n",
      " -0.047803428024053574]\n",
      "\n",
      "Image: seq_val/IMG_9792_0.jpg\n",
      "Actual   : [-1.71999306475 0.911233104666 1.65710420832 0.964921781859\n",
      " 0.00322501321798 0.260266901806 0.0343029737385]\n",
      "Predicted: [-1.551984190940857 0.6661027073860168 1.7709050178527832\n",
      " 0.9613392353057861 0.011360736563801765 0.2779843807220459\n",
      " 0.03574405983090401]\n",
      "\n",
      "Image: seq_val/S_2_3_213_0.jpg\n",
      "Actual   : [-0.592962759416 -1.24564399615 -2.16108186625 0.535095157943\n",
      " 0.0567819798897 0.84093329413 0.0572727992043]\n",
      "Predicted: [-0.42639556527137756 -0.6580044031143188 -2.038067579269409\n",
      " 0.5209993124008179 0.04374339058995247 0.8549412488937378\n",
      " 0.06520973145961761]\n",
      "\n",
      "Image: seq_val/S_2_3_54_0.jpg\n",
      "Actual   : [0.43216422242 0.897155952557 -1.01939660997 0.71425190648 0.0411130513143\n",
      " 0.696456245322 0.0557012518075]\n",
      "Predicted: [0.3405553996562958 0.6349404454231262 -0.8413394689559937\n",
      " 0.6543851494789124 0.03935129567980766 0.7519346475601196\n",
      " 0.05763602629303932]\n",
      "\n",
      "Image: seq_val/S_2_3_121_0.jpg\n",
      "Actual   : [-1.10389840386 -0.0331896488744 -4.61671571875 0.981275315896\n",
      " 0.00143513073316 0.192100588218 0.0139304996041]\n",
      "Predicted: [-0.9860315322875977 -0.06506489217281342 -4.604232311248779\n",
      " 0.9692434072494507 -0.003944513387978077 0.21169769763946533\n",
      " 0.03003198280930519]\n",
      "\n",
      "Image: seq_val/IMG_9933_0.jpg\n",
      "Actual   : [-0.767390800963 0.965112576743 -0.376456461559 0.609443200452\n",
      " -0.0388737260175 -0.790350689533 -0.0491284683505]\n",
      "Predicted: [-0.9504222869873047 0.6125426888465881 -0.01600106805562973\n",
      " 0.5676717758178711 -0.03278002142906189 -0.8221518993377686\n",
      " -0.039744313806295395]\n",
      "\n",
      "Image: seq_val/S_2_3_90_0.jpg\n",
      "Actual   : [-2.02216179855 0.088813632318 -4.47145152514 0.610525028645\n",
      " 0.00960703045638 -0.78342480366 -0.115812224637]\n",
      "Predicted: [-1.809639811515808 0.11370964348316193 -4.0658674240112305\n",
      " 0.6695008277893066 -0.010356334038078785 -0.7472653388977051\n",
      " -0.07417823374271393]\n",
      "\n",
      "Image: seq_val/IMG_9675_0.jpg\n",
      "Actual   : [4.94806506568 -1.78824365557 0.240716456289 0.999209535281\n",
      " -0.0274008295024 -0.011671141765 0.0263302790632]\n",
      "Predicted: [4.662139415740967 -1.6144306659698486 0.32611727714538574\n",
      " 0.9870644807815552 -0.010693589225411415 0.005150817334651947\n",
      " 0.02919767238199711]\n",
      "\n",
      "Image: seq_val/IMG_9687_0.jpg\n",
      "Actual   : [4.1840037778 -1.78870992748 1.09278813823 0.842728332515 -0.0350537425789\n",
      " -0.537190507733 -0.00255951341745]\n",
      "Predicted: [4.561147212982178 -1.72419273853302 1.043569803237915 0.8075423836708069\n",
      " -0.03210002928972244 -0.5706886649131775 -0.0065786694176495075]\n",
      "\n",
      "Image: seq_val/S_2_3_52_0.jpg\n",
      "Actual   : [-0.190857604497 0.900264750021 -0.88470624464 0.0129155503758\n",
      " -0.0532506115119 -0.995318572047 0.079614703851]\n",
      "Predicted: [-0.4229879081249237 0.4824194610118866 -1.0065085887908936\n",
      " 0.03997482359409332 -0.062430303543806076 -0.9673844575881958\n",
      " -0.012586409226059914]\n",
      "\n",
      "Image: seq_val/S_2_3_119_0.jpg\n",
      "Actual   : [-1.12913930979 -0.0296684376764 -4.63172561402 0.947533506143\n",
      " 0.000765765005668 0.31870547484 0.0246269902063]\n",
      "Predicted: [-0.7941076755523682 -0.09355929493904114 -3.9993982315063477\n",
      " 0.9624629616737366 0.006023332476615906 0.23901456594467163\n",
      " 0.02925829216837883]\n",
      "\n",
      "Image: seq_val/S_2_3_109_0.jpg\n",
      "Actual   : [-0.0151486590569 -0.139247870421 -4.08151403104 0.80840722478\n",
      " 0.0389052122318 0.583911250098 0.0633387353319]\n",
      "Predicted: [-0.1679859161376953 0.053426820784807205 -3.6812899112701416\n",
      " 0.7465941905975342 0.037305235862731934 0.6564897298812866\n",
      " 0.061249081045389175]\n",
      "\n",
      "Image: seq_val/IMG_9858_0.jpg\n",
      "Actual   : [2.75020038852 0.635508139187 0.845049454056 0.648547786183\n",
      " -0.0604974638975 -0.75875773489 0.00353915864656]\n",
      "Predicted: [2.8438611030578613 0.5658676624298096 0.7477417588233948\n",
      " 0.6526532769203186 -0.03490433096885681 -0.7510491609573364\n",
      " -0.02120138145983219]\n",
      "\n",
      "Image: seq_val/S_2_3_24_0.jpg\n",
      "Actual   : [-0.335661890277 0.777512918768 -2.34358094652 0.967352829791\n",
      " 0.121003199334 0.216569203274 0.0518122441022]\n",
      "Predicted: [-0.24800840020179749 0.5837165713310242 -2.36415696144104\n",
      " 0.9561134576797485 0.10449139773845673 0.22010987997055054\n",
      " 0.0395670086145401]\n",
      "\n",
      "Image: seq_val/S_2_3_79_0.jpg\n",
      "Actual   : [0.0157979178978 0.370004727477 -3.10575490138 0.567783514922\n",
      " 0.00913726210085 0.822762784843 0.0244906206886]\n",
      "Predicted: [0.011565186083316803 0.2536761164665222 -2.994318723678589\n",
      " 0.5841485261917114 0.020019877701997757 0.7897552847862244\n",
      " 0.04062245786190033]\n",
      "\n",
      "Image: seq_val/IMG_9879_0.jpg\n",
      "Actual   : [4.00449201472 0.607644667933 -0.289552967976 0.380312561261\n",
      " -0.0350019731883 -0.923974944362 -0.0201871199504]\n",
      "Predicted: [3.5702433586120605 0.5130130052566528 -0.7214059829711914\n",
      " 0.3755202293395996 -0.038662053644657135 -0.9022766947746277\n",
      " -0.03565638139843941]\n",
      "\n",
      "Image: seq_val/S_2_3_167_0.jpg\n",
      "Actual   : [-1.1883363469 -0.476964536487 -3.06006471002 0.980292412947\n",
      " -0.0240575114581 -0.196071176475 -0.00202855038769]\n",
      "Predicted: [-1.2214875221252441 -0.4002518355846405 -3.003643035888672\n",
      " 0.9601452350616455 -0.028836335986852646 -0.22458314895629883\n",
      " 0.0016173133626580238]\n",
      "\n",
      "Image: seq_val/S_2_3_159_0.jpg\n",
      "Actual   : [-2.09277012563 0.0977369072691 -4.40362042593 0.881411619847\n",
      " 0.127895053993 -0.440259528326 -0.113701184165]\n",
      "Predicted: [-2.1293234825134277 0.21950244903564453 -4.507611274719238\n",
      " 0.8793596029281616 0.14679160714149475 -0.4398796558380127\n",
      " -0.1154572069644928]\n",
      "\n",
      "Image: seq_val/S_2_3_180_0.jpg\n",
      "Actual   : [-0.767976209506 -1.20201116619 -1.24107749991 0.114862451985\n",
      " -0.0610398279953 -0.958250714102 -0.254629781143]\n",
      "Predicted: [-1.0592740774154663 -0.313168466091156 -1.428504228591919\n",
      " 0.12768152356147766 -0.042427968233823776 -0.9419131875038147\n",
      " -0.1853751242160797]\n",
      "\n",
      "Image: seq_val/IMG_9714_0.jpg\n",
      "Actual   : [3.24572406561 -1.67221789395 2.35957770794 0.198910714788 0.0222205344225\n",
      " 0.979219441714 0.0327117771026]\n",
      "Predicted: [3.2332406044006348 -0.9534106850624084 1.982320785522461\n",
      " 0.18920820951461792 0.025514226406812668 0.9691152572631836\n",
      " 0.04375326633453369]\n",
      "\n",
      "Image: seq_val/IMG_9730_0.jpg\n",
      "Actual   : [-1.04097036066 -1.24682733641 -1.32059494958 0.69097126688\n",
      " -0.0600591787693 -0.719808363459 -0.0287667044896]\n",
      "Predicted: [-0.878157377243042 -0.40215060114860535 -1.2415775060653687\n",
      " 0.6995255947113037 -0.03452287241816521 -0.6801937818527222\n",
      " -0.026691656559705734]\n",
      "\n",
      "Image: seq_val/IMG_9626_0.jpg\n",
      "Actual   : [-2.4943886249 -1.22007236782 -0.51954086303 0.944220539986\n",
      " -0.00257715181192 0.325301667695 0.051183543761]\n",
      "Predicted: [-1.9687310457229614 -0.889074981212616 -0.8641875982284546\n",
      " 0.9060927629470825 0.0046154651790857315 0.36512526869773865\n",
      " 0.044142257422208786]\n",
      "\n",
      "Image: seq_val/S_2_3_78_0.jpg\n",
      "Actual   : [0.016017950013 0.373693918974 -3.13109297289 0.504186115177\n",
      " 0.0191086604692 0.862983709161 0.0262742855447]\n",
      "Predicted: [-0.010031558573246002 0.35592132806777954 -2.881474733352661\n",
      " 0.5003939867019653 0.024241391569375992 0.8568735122680664\n",
      " 0.03353074938058853]\n",
      "\n",
      "Image: seq_val/S_2_3_49_0.jpg\n",
      "Actual   : [-0.181952638607 0.920645410182 -0.89158499247 0.0249022344027\n",
      " -0.0585376386887 -0.997875249184 -0.014078730208]\n",
      "Predicted: [0.05090124160051346 0.8455633521080017 -0.9610022306442261\n",
      " 0.05194433033466339 -0.06166566535830498 -0.9756847620010376\n",
      " -0.02997734770178795]\n",
      "\n",
      "Image: seq_val/S_2_3_163_0.jpg\n",
      "Actual   : [-1.26058372904 -0.467158640521 -3.060882385 0.99520697302\n",
      " -0.00523975641806 0.0976354079807 0.00171840442738]\n",
      "Predicted: [-1.0356814861297607 -0.3555752635002136 -2.878598928451538\n",
      " 0.9871869683265686 -0.00775531679391861 0.003780283033847809\n",
      " 0.017229093238711357]\n",
      "\n",
      "Image: seq_val/S_2_3_31_0.jpg\n",
      "Actual   : [-0.99337199883 1.00411971934 -0.865866602431 0.523169210559\n",
      " -0.0263206791232 -0.848720161792 -0.0726311637045]\n",
      "Predicted: [-0.7383785843849182 0.2466389536857605 -0.4826674461364746\n",
      " 0.5320446491241455 -0.04149594157934189 -0.8478041291236877\n",
      " -0.03637353703379631]\n",
      "\n",
      "Image: seq_val/IMG_8285_0.jpg\n",
      "Actual   : [-0.467995178088 0.656260793785 4.41550811372 0.630004112116\n",
      " 0.0470855923989 0.773790501764 0.0461088395629]\n",
      "Predicted: [-0.838658332824707 0.4084954559803009 3.6198136806488037\n",
      " 0.5801081657409668 0.042492348700761795 0.7842771410942078\n",
      " 0.048596106469631195]\n",
      "\n",
      "Image: seq_val/IMG_9689_0.jpg\n",
      "Actual   : [4.50063980491 -1.67781117302 -0.756482275074 0.900390102939\n",
      " -0.0270820574137 -0.433915112526 -0.016789872489]\n",
      "Predicted: [4.132106304168701 -1.432982325553894 -0.6172987818717957\n",
      " 0.8789042830467224 -0.02489933930337429 -0.47239992022514343\n",
      " -0.004578673746436834]\n",
      "\n",
      "Image: seq_val/S_2_3_70_0.jpg\n",
      "Actual   : [0.0601250932149 0.595870268407 -2.78189203729 0.222630506663\n",
      " 0.0406923405341 0.973741231689 0.0246536941199]\n",
      "Predicted: [0.14744886755943298 0.2947794795036316 -2.554811954498291\n",
      " 0.1906132698059082 0.03693097084760666 0.9798904657363892\n",
      " 0.037098757922649384]\n",
      "\n",
      "Image: seq_val/IMG_8199_0.jpg\n",
      "Actual   : [-2.12264071347 0.870292420125 2.35182495103 0.776621304005\n",
      " 0.0277201441185 0.6273413595 0.0503364920925]\n",
      "Predicted: [-1.8896344900131226 0.7444382309913635 2.3830623626708984\n",
      " 0.7509987950325012 0.03961610794067383 0.665031909942627\n",
      " 0.04908929020166397]\n",
      "\n",
      "Image: seq_val/IMG_9790_0.jpg\n",
      "Actual   : [0.000179986412883 0.712633517155 2.01146063676 0.771520974834\n",
      " 0.00572026912293 0.633394646784 0.059446491387]\n",
      "Predicted: [-0.5693355798721313 0.6791245341300964 2.6644136905670166\n",
      " 0.7783200740814209 0.028684891760349274 0.6290854215621948\n",
      " 0.04617990925908089]\n",
      "\n",
      "Image: seq_val/IMG_8287_0.jpg\n",
      "Actual   : [-3.3764149189 0.96015799604 2.53257640776 0.89596966518 -0.0370139068396\n",
      " -0.440956586823 -0.0377573610695]\n",
      "Predicted: [-3.2736616134643555 0.6763262748718262 2.6334478855133057\n",
      " 0.9000847339630127 -0.025363527238368988 -0.40138524770736694\n",
      " -0.002971591893583536]\n",
      "\n",
      "Image: seq_val/IMG_9874_0.jpg\n",
      "Actual   : [5.19604574785 0.410205492155 3.83499239819 0.559916169594\n",
      " -0.0133618110094 -0.827502669694 -0.0394293885668]\n",
      "Predicted: [5.230676651000977 0.14400644600391388 3.13333797454834 0.5365891456604004\n",
      " -0.03773738071322441 -0.8254894018173218 -0.019591737538576126]\n",
      "\n",
      "Image: seq_val/IMG_9723_0.jpg\n",
      "Actual   : [-1.52763083438 -1.44262728638 2.16983508853 0.129416053644\n",
      " -0.0579779232352 -0.989338780521 -0.0331484967011]\n",
      "Predicted: [-1.286216139793396 -0.9637816548347473 2.0470805168151855\n",
      " 0.17318370938301086 -0.05613551661372185 -0.967215895652771\n",
      " -0.037029918283224106]\n",
      "\n",
      "Image: seq_val/IMG_9768_0.jpg\n",
      "Actual   : [-2.32751847619 1.05624973127 -0.825634357031 0.628727457261\n",
      " 0.0453151604877 0.774919708456 0.0463439981312]\n",
      "Predicted: [-1.642348289489746 0.6188018918037415 -1.3904839754104614\n",
      " 0.6762700080871582 0.03985161706805229 0.7423195838928223\n",
      " 0.06117083504796028]\n",
      "\n",
      "Image: seq_val/S_2_3_86_0.jpg\n",
      "Actual   : [-1.98658300106 -0.00311203594978 -3.97363725342 0.525687700257\n",
      " -0.0499500159285 -0.848685572026 -0.0298368487413]\n",
      "Predicted: [-1.4200506210327148 -0.22259685397148132 -3.3614959716796875\n",
      " 0.519738495349884 -0.04466140642762184 -0.8457228541374207\n",
      " -0.04287443682551384]\n",
      "\n",
      "Image: seq_val/S_2_3_153_0.jpg\n",
      "Actual   : [-2.17064391451 0.00782104739974 -4.4056338576 0.962249291892\n",
      " -0.0766132111724 -0.260912479591 0.011462727431]\n",
      "Predicted: [-1.666793704032898 -0.0360986553132534 -4.019467353820801\n",
      " 0.9403951168060303 -0.02564143016934395 -0.28391706943511963\n",
      " -0.006906130816787481]\n",
      "\n",
      "Image: seq_val/IMG_9896_0.jpg\n",
      "Actual   : [5.10855423032 0.684365002275 -1.85679222744 0.626879109281\n",
      " 0.0621301844563 0.77371864538 0.0672449278232]\n",
      "Predicted: [4.459178924560547 0.5508905649185181 -1.674849033355713\n",
      " 0.6323433518409729 0.04148032143712044 0.7659391164779663\n",
      " 0.06120714545249939]\n",
      "\n",
      "Image: seq_val/IMG_9630_0.jpg\n",
      "Actual   : [-2.87018397989 -1.39041357779 4.55982184469 0.283930957545\n",
      " 0.0139347721409 0.958258726317 0.030483551471]\n",
      "Predicted: [-2.2362256050109863 -0.8610889315605164 3.7974984645843506\n",
      " 0.2745392620563507 0.03964496776461601 0.9526418447494507\n",
      " 0.04792946204543114]\n",
      "\n",
      "Image: seq_val/IMG_9923_0.jpg\n",
      "Actual   : [-1.60889967487 0.831339619987 2.20412936709 0.201247403718\n",
      " -0.0604006095823 -0.976769280962 -0.0421072514883]\n",
      "Predicted: [-1.3043626546859741 0.3664492666721344 1.7321611642837524\n",
      " 0.17964290082454681 -0.04881170392036438 -0.9619016647338867\n",
      " -0.06075657531619072]\n",
      "\n",
      "Image: seq_val/S_2_3_143_0.jpg\n",
      "Actual   : [-1.28143892169 -0.23172544071 -3.40255994977 0.909872670499\n",
      " -0.0309377964791 0.413245197326 0.0200744393319]\n",
      "Predicted: [-0.6250148415565491 -0.16963449120521545 -2.7361557483673096\n",
      " 0.8607579469680786 0.002033066004514694 0.46964848041534424\n",
      " 0.040786173194646835]\n",
      "\n",
      "Image: seq_val/S_2_3_25_0.jpg\n",
      "Actual   : [-0.357384124883 0.774422233312 -2.36571836613 0.93984298597\n",
      " 0.118112660169 0.313996200907 0.0644278437183]\n",
      "Predicted: [-0.41271525621414185 0.5698121190071106 -2.4212229251861572\n",
      " 0.914188802242279 0.09517556428909302 0.325277715921402\n",
      " 0.044095344841480255]\n",
      "\n",
      "Image: seq_val/IMG_9713_0.jpg\n",
      "Actual   : [4.78729923058 -1.79514413705 1.18171749255 0.428089253694 0.0126883117044\n",
      " 0.902890424862 0.0369794308992]\n",
      "Predicted: [4.038286209106445 -0.7774607539176941 1.755937933921814\n",
      " 0.3965827226638794 0.027649864554405212 0.9111083745956421\n",
      " 0.044423505663871765]\n",
      "\n",
      "Image: seq_val/S_2_3_41_0.jpg\n",
      "Actual   : [-1.01638275248 1.00394264845 -0.904045459861 0.30961703176\n",
      " -0.053049840072 -0.944909757626 -0.0920247687096]\n",
      "Predicted: [-0.674470841884613 1.0048859119415283 -0.7961386442184448\n",
      " 0.30051684379577637 -0.046487513929605484 -0.9376397132873535\n",
      " -0.08228633552789688]\n",
      "\n",
      "Image: seq_val/S_2_3_74_0.jpg\n",
      "Actual   : [0.163371599476 0.345555036668 -3.23226932849 0.0450945616793\n",
      " -0.0583757375228 -0.997161191868 -0.0151099705166]\n",
      "Predicted: [-0.5121778845787048 -0.06353718042373657 -1.9818682670593262\n",
      " 0.11899501085281372 -0.05435137450695038 -0.9633361101150513\n",
      " -0.042472708970308304]\n",
      "\n",
      "Image: seq_val/S_2_3_26_0.jpg\n",
      "Actual   : [-0.317041340105 0.774092355655 -2.33029322113 0.979008530733\n",
      " 0.114912837748 0.164624575869 0.0351580075206]\n",
      "Predicted: [-0.25305360555648804 0.6953384280204773 -2.146273612976074\n",
      " 0.964760422706604 0.12080846726894379 0.1864289939403534\n",
      " 0.035464029759168625]\n",
      "\n",
      "Image: seq_val/S_2_3_164_0.jpg\n",
      "Actual   : [-1.24584845723 -0.468070284285 -3.04881765404 0.999768296709\n",
      " -0.0040433155287 0.0210344954796 -0.0021341261082]\n",
      "Predicted: [-1.1719194650650024 -0.3119644224643707 -3.360898494720459\n",
      " 0.9841992855072021 -0.01858513616025448 -0.04225824028253555\n",
      " 0.014013243839144707]\n",
      "\n",
      "Image: seq_val/S_2_3_207_0.jpg\n",
      "Actual   : [-0.716551402835 -1.23782634227 -1.80400691314 0.824539105589\n",
      " -0.0485360139018 -0.563585661722 -0.0122768322882]\n",
      "Predicted: [-0.7945059537887573 -0.21143142879009247 -1.7386441230773926\n",
      " 0.8590784668922424 -0.025458279997110367 -0.48132801055908203\n",
      " -0.014250172302126884]\n",
      "\n",
      "Image: seq_val/IMG_9708_0.jpg\n",
      "Actual   : [3.43200362805 -1.72476134784 1.89751957006 0.0451427321489\n",
      " -0.0629555091153 -0.996609354164 -0.0277224241278]\n",
      "Predicted: [3.2366466522216797 -1.397630214691162 1.9008095264434814\n",
      " 0.05733935534954071 -0.05659223720431328 -0.9856019020080566\n",
      " -0.03262627497315407]\n",
      "\n",
      "Image: seq_val/S_2_3_99_0.jpg\n",
      "Actual   : [-0.21015811392 -0.015218574375 -4.50883858392 0.976139953023\n",
      " 0.192839339202 -0.0975800221238 -0.0210219088153]\n",
      "Predicted: [-0.6570112705230713 0.23261108994483948 -4.121275901794434\n",
      " 0.9608234763145447 0.1756337285041809 -0.15327829122543335\n",
      " -0.022850869223475456]\n",
      "\n",
      "Image: seq_val/IMG_9763_0.jpg\n",
      "Actual   : [-0.755907258291 -1.3240330711 -0.491130955812 0.365308054207\n",
      " -0.0296077655396 -0.929234657782 -0.0468653019536]\n",
      "Predicted: [-0.622773289680481 -1.3147839307785034 -0.5851529240608215\n",
      " 0.3316049575805664 -0.04706786572933197 -0.9356369972229004\n",
      " -0.06475359201431274]\n",
      "\n",
      "Image: seq_val/IMG_9806_0.jpg\n",
      "Actual   : [-3.13320075395 1.0040837038 1.31824706513 0.967649153151 -0.0270139413395\n",
      " -0.25082710072 0.00333600406291]\n",
      "Predicted: [-2.775651216506958 0.74515700340271 1.6066319942474365 0.9375320672988892\n",
      " -0.021435651928186417 -0.3085009455680847 0.0015783727867528796]\n",
      "\n",
      "Image: seq_val/S_2_3_83_0.jpg\n",
      "Actual   : [-2.0171867522 0.00615315840208 -4.00248385424 0.364087356012\n",
      " -0.0580711982327 -0.927877096906 -0.0557873297918]\n",
      "Predicted: [-1.659714937210083 -0.11259019374847412 -3.6077654361724854\n",
      " 0.3448430299758911 -0.04362666606903076 -0.9154409766197205\n",
      " -0.051594167947769165]\n",
      "\n",
      "Image: seq_val/S_2_3_193_0.jpg\n",
      "Actual   : [-0.279718586485 -1.43059089176 0.968519452546 0.214513571136\n",
      " 0.0440920433487 0.971786326931 0.0875851260328]\n",
      "Predicted: [-0.2698799669742584 -0.7905969619750977 1.3505860567092896\n",
      " 0.14025825262069702 0.04193805903196335 0.9926464557647705\n",
      " 0.0757807195186615]\n",
      "\n",
      "Image: seq_val/S_2_3_147_0.jpg\n",
      "Actual   : [-1.28708770664 0.0467380112694 -4.6563300663 0.987442283742\n",
      " 0.154987565488 0.0305554463564 0.00171916295611]\n",
      "Predicted: [-1.381719708442688 -0.07508569955825806 -4.976649284362793\n",
      " 0.9747176766395569 0.19418223202228546 0.05068349838256836\n",
      " -0.004161039367318153]\n",
      "\n",
      "Image: seq_val/S_2_3_16_0.jpg\n",
      "Actual   : [-1.46011482009 1.02325852601 -1.35021810252 0.801525794029\n",
      " -0.0145364818489 -0.597598637452 -0.0148647475071]\n",
      "Predicted: [-1.393547773361206 0.5063238739967346 -1.577883005142212\n",
      " 0.8164417147636414 -0.017049800604581833 -0.5745476484298706\n",
      " -0.02787742391228676]\n",
      "\n",
      "Image: seq_val/S_2_3_53_0.jpg\n",
      "Actual   : [0.440936745447 0.899793870174 -1.00443397634 0.748666604679\n",
      " 0.0458252831858 0.658896186802 0.0570453633221]\n",
      "Predicted: [0.0501626655459404 0.21476545929908752 -1.1868929862976074\n",
      " 0.7630795240402222 0.03699924051761627 0.6665676236152649\n",
      " 0.056791916489601135]\n",
      "\n",
      "Image: seq_val/IMG_9783_0.jpg\n",
      "Actual   : [-2.63740331042 0.840473720653 3.61002632867 0.314799431969\n",
      " 0.0288745579048 0.947774949133 0.0423110308861]\n",
      "Predicted: [-2.6800177097320557 0.36639949679374695 3.497260093688965\n",
      " 0.3006495535373688 0.04522402212023735 0.9344605207443237\n",
      " 0.04558027163147926]\n",
      "\n",
      "Image: seq_val/IMG_9926_0.jpg\n",
      "Actual   : [0.876015259392 0.746788664092 2.15829431294 0.210516441601\n",
      " 0.0293203857191 0.975590558267 0.055192439858]\n",
      "Predicted: [0.9333506226539612 0.1964045614004135 1.9460318088531494\n",
      " 0.1837131381034851 0.03649172559380531 0.9595377445220947\n",
      " 0.05636337026953697]\n",
      "\n",
      "Image: seq_val/IMG_9928_0.jpg\n",
      "Actual   : [1.85518573889 0.69492974452 1.65142241013 0.345194750459 0.0265523639393\n",
      " 0.936398983903 0.0573803029824]\n",
      "Predicted: [1.696060299873352 0.3539632558822632 1.400738000869751\n",
      " 0.28797969222068787 0.03445703536272049 0.9344253540039062\n",
      " 0.055252593010663986]\n",
      "\n",
      "Image: seq_val/IMG_9867_0.jpg\n",
      "Actual   : [4.90629103266 0.775325078621 -0.101778161183 0.717900567885\n",
      " -0.0411739386268 -0.694406310936 -0.0268952921603]\n",
      "Predicted: [4.909520626068115 0.5663642287254333 0.5692029595375061\n",
      " 0.6548522710800171 -0.03235594183206558 -0.7553505897521973\n",
      " -0.02486252412199974]\n",
      "\n",
      "Image: seq_val/IMG_9778_0.jpg\n",
      "Actual   : [-2.59516197218 1.12738565568 -1.62381729415 0.949617485193\n",
      " 0.0148425171377 0.311356355184 0.0326121386646]\n",
      "Predicted: [-2.1941137313842773 0.6215332746505737 -1.194797396659851\n",
      " 0.9435765743255615 0.0070247408002614975 0.29778844118118286\n",
      " 0.035565923899412155]\n",
      "\n",
      "Image: seq_val/IMG_9865_0.jpg\n",
      "Actual   : [4.80255828017 0.681400838986 -0.115240564076 0.589519301359\n",
      " -0.043794455683 -0.806319305731 -0.0199553547235]\n",
      "Predicted: [5.192629337310791 0.6454924941062927 0.27671754360198975\n",
      " 0.5764535069465637 -0.033485047519207 -0.8122197389602661\n",
      " -0.03378812596201897]\n",
      "\n",
      "Image: seq_val/IMG_9703_0.jpg\n",
      "Actual   : [3.06727637763 -1.6043069153 0.0362877526256 0.575532446472\n",
      " -0.0386450005863 -0.816656542504 -0.0184677712096]\n",
      "Predicted: [3.393157720565796 -1.6274044513702393 0.329861044883728\n",
      " 0.5214731693267822 -0.04054655134677887 -0.8381437063217163\n",
      " -0.03056703880429268]\n",
      "\n",
      "Image: seq_val/IMG_9704_0.jpg\n",
      "Actual   : [2.9428872222 -1.56434053326 -0.346705533881 0.644719249706\n",
      " -0.0397018955246 -0.762711433915 -0.0321265797293]\n",
      "Predicted: [3.107715129852295 -1.6622121334075928 -0.0864916443824768\n",
      " 0.6328726410865784 -0.03915167227387428 -0.7712023258209229\n",
      " -0.026985641568899155]\n",
      "\n",
      "Image: seq_val/S_2_3_173_0.jpg\n",
      "Actual   : [-0.159802129603 -1.32624604671 -0.681375325552 0.143677352876\n",
      " -0.0460105860965 -0.981964227319 -0.113956572886]\n",
      "Predicted: [-0.24841174483299255 -1.2185251712799072 -0.4637325406074524\n",
      " 0.1466573029756546 -0.0474136658012867 -0.9795819520950317\n",
      " -0.11445286124944687]\n",
      "\n",
      "Image: seq_val/IMG_9784_0.jpg\n",
      "Actual   : [-3.21448806554 0.867049140742 3.96394420646 0.195193553413\n",
      " 0.0299901263028 0.978961996167 0.051317434576]\n",
      "Predicted: [-3.398648262023926 -0.013881705701351166 3.8425452709198\n",
      " 0.20377635955810547 0.04366395249962807 0.9706803560256958\n",
      " 0.04587583616375923]\n",
      "\n",
      "Image: seq_val/S_2_3_137_0.jpg\n",
      "Actual   : [-1.20609658793 -0.231542774625 -3.41054936798 0.869012593617\n",
      " -0.0395460814045 -0.493035719106 -0.0129999717726]\n",
      "Predicted: [-1.2234045267105103 -0.31380751729011536 -3.4516241550445557\n",
      " 0.8435686826705933 -0.03562407195568085 -0.526527464389801\n",
      " -0.022114241495728493]\n",
      "\n",
      "Image: seq_val/IMG_9669_0.jpg\n",
      "Actual   : [5.55124683172 -1.91158915642 1.71629544633 0.90934357045 0.0114285562075\n",
      " 0.413945481047 0.0401596527105]\n",
      "Predicted: [5.768679618835449 -1.8263388872146606 1.3880820274353027\n",
      " 0.9135894179344177 0.0006445366889238358 0.37134724855422974\n",
      " 0.04264618828892708]\n",
      "\n",
      "Image: seq_val/IMG_9639_0.jpg\n",
      "Actual   : [-1.9619412753 -1.44392118184 2.69367023678 0.892109404932 0.010798263202\n",
      " 0.4470296038 0.0647204795221]\n",
      "Predicted: [-1.6059772968292236 -1.1835640668869019 1.98187255859375\n",
      " 0.9120349884033203 0.010250318795442581 0.402817040681839\n",
      " 0.044313330203294754]\n",
      "\n",
      "Image: seq_val/S_2_3_192_0.jpg\n",
      "Actual   : [-0.253983785004 -1.42911955683 0.94391776158 0.108267677189\n",
      " 0.0465765263081 0.989057176936 0.088739157218]\n",
      "Predicted: [-0.12154048681259155 -0.6115301251411438 1.0142664909362793\n",
      " 0.07101194560527802 0.03703400865197182 0.9779031276702881\n",
      " 0.07697425782680511]\n",
      "\n",
      "Image: seq_val/IMG_8192_0.jpg\n",
      "Actual   : [-2.99427988632 1.04320697147 0.283978678711 0.974480387235\n",
      " -0.0199793862945 0.221938983803 0.0270534006439]\n",
      "Predicted: [-2.9931819438934326 0.9722306132316589 0.4293651580810547\n",
      " 0.9640785455703735 0.013503767549991608 0.19480431079864502\n",
      " 0.02708960883319378]\n",
      "\n",
      "Image: seq_val/IMG_9757_0.jpg\n",
      "Actual   : [-0.872958470628 -1.34788353511 0.0110995593262 0.560373557755\n",
      " -0.0340426917168 -0.827365127097 -0.0170152100671]\n",
      "Predicted: [-0.49198484420776367 -1.0982612371444702 -0.061196617782115936\n",
      " 0.5275560617446899 -0.045595064759254456 -0.843166708946228\n",
      " -0.03424699977040291]\n",
      "\n",
      "Image: seq_val/IMG_9671_0.jpg\n",
      "Actual   : [6.42004475839 -2.07400878411 3.5325739687 0.816251525502 0.0148866495417\n",
      " 0.574625107152 0.057600529616]\n",
      "Predicted: [6.595732688903809 -2.1129724979400635 3.7079267501831055\n",
      " 0.8031879663467407 0.01625971496105194 0.5738103985786438\n",
      " 0.04385221377015114]\n",
      "\n",
      "Image: seq_val/IMG_9904_0.jpg\n",
      "Actual   : [5.55964970963 0.698715457038 -1.8171803124 0.736118334916 0.0371335848511\n",
      " 0.673609353915 0.0547844156167]\n",
      "Predicted: [5.181623458862305 0.4561503231525421 -0.9328998327255249\n",
      " 0.6724968552589417 0.03786420077085495 0.7425540685653687\n",
      " 0.05811655893921852]\n",
      "\n",
      "Image: seq_val/IMG_9721_0.jpg\n",
      "Actual   : [2.3666557608 -1.62564745482 1.76107683061 0.271922254943 0.0159972575486\n",
      " 0.961928969076 0.0222493476419]\n",
      "Predicted: [2.6298084259033203 -1.1053670644760132 1.6767551898956299\n",
      " 0.2851269245147705 0.027581606060266495 0.9329854249954224\n",
      " 0.05284090340137482]\n",
      "\n",
      "Image: seq_val/IMG_9881_0.jpg\n",
      "Actual   : [3.89956592856 0.61738800576 -0.339451494931 0.103067425735\n",
      " -0.0515360800424 -0.993243105473 -0.0137576028599]\n",
      "Predicted: [3.791151523590088 0.6800151467323303 -0.4148944616317749\n",
      " 0.1299389749765396 -0.050648029893636703 -1.0021809339523315\n",
      " -0.036394309252500534]\n",
      "\n",
      "Image: seq_val/IMG_9932_0.jpg\n",
      "Actual   : [-0.771523021026 0.923880865475 -0.0520766465143 0.513166204218\n",
      " -0.0403461806087 -0.856111933099 -0.04588017615]\n",
      "Predicted: [-0.7935259342193604 0.4574642777442932 -0.2982054352760315\n",
      " 0.5065246820449829 -0.03638652712106705 -0.8475949764251709\n",
      " -0.041569504886865616]\n",
      "\n",
      "Image: seq_val/S_2_3_85_0.jpg\n",
      "Actual   : [-1.98733174526 0.00134368625979 -3.99239135101 0.475537811812\n",
      " -0.0530335264366 -0.877372363134 -0.0356226194874]\n",
      "Predicted: [-1.202675700187683 -0.2432764619588852 -3.0212297439575195\n",
      " 0.40801572799682617 -0.04695836454629898 -0.8847306370735168\n",
      " -0.04442697390913963]\n",
      "\n",
      "Image: seq_val/S_2_3_51_0.jpg\n",
      "Actual   : [-0.184037607789 0.906157432459 -0.892731411016 0.0101164739826\n",
      " -0.0556182178752 -0.99720828858 0.0487842185886]\n",
      "Predicted: [-0.11884050816297531 0.6570503115653992 -0.8759719133377075\n",
      " 0.0354168564081192 -0.0653776228427887 -0.9984443187713623\n",
      " -0.015281949192285538]\n",
      "\n",
      "Image: seq_val/IMG_9672_0.jpg\n",
      "Actual   : [1.44042080945 -1.67579975604 2.46624805126 0.876389075325\n",
      " -0.0465363131345 -0.479349519931 -0.000773274256196]\n",
      "Predicted: [1.7581268548965454 -1.425503134727478 1.6843931674957275\n",
      " 0.8646674156188965 -0.03719868138432503 -0.46432918310165405\n",
      " 0.0037684058770537376]\n",
      "\n",
      "Image: seq_val/IMG_9835_0.jpg\n",
      "Actual   : [4.12099736999 0.617010805623 -0.391436956637 0.993366721617\n",
      " -0.0251316311856 -0.109037889463 0.0264895481074]\n",
      "Predicted: [3.9190094470977783 0.3993307948112488 -0.4746760129928589\n",
      " 0.9595110416412354 -0.01231408677995205 -0.20327430963516235\n",
      " 0.010881973430514336]\n",
      "\n",
      "Image: seq_val/IMG_9880_0.jpg\n",
      "Actual   : [3.93814296458 0.619376884639 -0.330086174829 0.245695604357\n",
      " -0.0516826042883 -0.967835449087 -0.0160350211973]\n",
      "Predicted: [3.6069812774658203 0.3257457911968231 -0.3601832985877991\n",
      " 0.23009726405143738 -0.04411003738641739 -0.9681857824325562\n",
      " -0.03878452256321907]\n",
      "\n",
      "Image: seq_val/IMG_9683_0.jpg\n",
      "Actual   : [5.65423045764 -1.78693116154 -0.471213866928 0.99188290624\n",
      " -0.0229086570123 -0.123407200711 0.0203508367347]\n",
      "Predicted: [5.7398223876953125 -1.8957260847091675 -0.40311282873153687\n",
      " 0.9894981384277344 -0.017376834526658058 -0.132807195186615\n",
      " 0.02589847519993782]\n",
      "\n",
      "Image: seq_val/IMG_9697_0.jpg\n",
      "Actual   : [4.74654267868 -1.70792012956 -0.720156121916 0.793358902005\n",
      " -0.0309813090247 -0.607697583272 -0.018040465229]\n",
      "Predicted: [4.7745771408081055 -1.7099868059158325 0.12281493842601776\n",
      " 0.7430063486099243 -0.036082133650779724 -0.6662271022796631\n",
      " -0.014650573953986168]\n",
      "\n",
      "Image: seq_val/IMG_9680_0.jpg\n",
      "Actual   : [3.29452749282 -1.70090792781 0.880531498735 0.913767416094\n",
      " -0.0490086487888 -0.403192999562 0.0079162322006]\n",
      "Predicted: [3.8182244300842285 -1.7712782621383667 0.9717615842819214\n",
      " 0.8945464491844177 -0.0365690179169178 -0.4190473258495331\n",
      " 0.006778346374630928]\n",
      "\n",
      "Image: seq_val/IMG_9913_0.jpg\n",
      "Actual   : [1.56822074991 0.64381042715 2.22932112649 0.118010885042 0.0444137158655\n",
      " 0.991138940417 0.0417666570765]\n",
      "Predicted: [1.9161267280578613 0.06699316203594208 1.9782733917236328\n",
      " 0.11013172566890717 0.031844642013311386 0.9837663173675537\n",
      " 0.04809051752090454]\n",
      "\n",
      "Image: seq_val/IMG_9834_0.jpg\n",
      "Actual   : [2.836507946 0.664656721597 0.133785930685 0.968848166178 -0.0244451731694\n",
      " -0.246083320023 0.0133665256479]\n",
      "Predicted: [2.939514398574829 0.5932576060295105 -0.2951587438583374\n",
      " 0.9740886688232422 -0.01734169013798237 -0.18244200944900513\n",
      " 0.011423047631978989]\n",
      "\n",
      "Image: seq_val/IMG_9648_0.jpg\n",
      "Actual   : [1.13219954297 -1.70151038452 3.38826830081 0.866279514072\n",
      " -0.000624017034816 0.498030783862 0.0390480784056]\n",
      "Predicted: [1.5843802690505981 -1.6092718839645386 3.099504232406616\n",
      " 0.8699334263801575 0.008726231753826141 0.46858787536621094\n",
      " 0.04599014297127724]\n",
      "\n",
      "Image: seq_val/IMG_9751_0.jpg\n",
      "Actual   : [-0.847501801271 -1.34523264598 -0.0581100415519 0.482449005158\n",
      " -0.0558916749615 -0.873703030854 -0.0276060132673]\n",
      "Predicted: [-1.2156486511230469 -1.043199062347412 -0.10640808939933777\n",
      " 0.4681473672389984 -0.04663580656051636 -0.8794592618942261\n",
      " -0.04169805720448494]\n",
      "\n",
      "Image: seq_val/IMG_9759_0.jpg\n",
      "Actual   : [-0.800367002862 -1.27547109445 -1.09984151679 0.820444308418\n",
      " -0.0415604387854 -0.569803034636 -0.0216418213508]\n",
      "Predicted: [-0.2752704918384552 -0.8801999688148499 -1.477962613105774\n",
      " 0.8570375442504883 -0.03278625011444092 -0.5365636348724365\n",
      " -0.016103610396385193]\n",
      "\n",
      "Image: seq_val/S_2_3_187_0.jpg\n",
      "Actual   : [-0.0750398876788 -1.45345786066 0.954071723406 0.379894294735\n",
      " -0.0340391659942 -0.923729827331 -0.0352826601318]\n",
      "Predicted: [0.7725709676742554 -1.2032049894332886 0.8436216115951538\n",
      " 0.3111381232738495 -0.04754806309938431 -0.9401140213012695\n",
      " -0.043268315494060516]\n",
      "\n",
      "Image: seq_val/IMG_9847_0.jpg\n",
      "Actual   : [5.6586461961 0.531253452593 2.11079504808 0.82015182138 -0.0426113028338\n",
      " -0.570422990156 -0.012364427199]\n",
      "Predicted: [5.256625175476074 0.5399259924888611 2.084493398666382 0.7881345748901367\n",
      " -0.032778479158878326 -0.5784550905227661 -0.0021155490539968014]\n",
      "\n",
      "Image: seq_val/IMG_9695_0.jpg\n",
      "Actual   : [4.93098813626 -1.74411896302 0.303223141022 0.569689150816\n",
      " -0.0372701814667 -0.820419614587 -0.0312547758886]\n",
      "Predicted: [5.742910861968994 -1.6055470705032349 0.5743489265441895\n",
      " 0.6155412197113037 -0.03430648893117905 -0.7745645046234131\n",
      " -0.022833654657006264]\n",
      "\n",
      "Image: seq_val/IMG_9774_0.jpg\n",
      "Actual   : [-1.69252805074 0.910483447481 0.983214859495 0.561522560443\n",
      " 0.0558209619226 0.823821481048 0.0538015026443]\n",
      "Predicted: [-1.4649704694747925 0.7537676095962524 0.7379227876663208\n",
      " 0.5095226168632507 0.043890755623579025 0.8543317317962646\n",
      " 0.05917580798268318]\n",
      "\n",
      "Image: seq_val/S_2_3_107_0.jpg\n",
      "Actual   : [-0.017342665239 -0.137909929956 -4.10792196017 0.705143761079\n",
      " 0.0517960578756 0.703726523254 0.0697024036135]\n",
      "Predicted: [0.5796810388565063 -0.4642871916294098 -3.9679365158081055\n",
      " 0.6788933277130127 0.03843606263399124 0.7372860312461853\n",
      " 0.0689341202378273]\n",
      "\n",
      "Image: seq_val/IMG_9732_0.jpg\n",
      "Actual   : [-1.520800291 -1.18271779132 -1.16193030943 0.206153196486\n",
      " -0.0592594076285 -0.958762638281 -0.186449418401]\n",
      "Predicted: [-1.2220995426177979 -0.8878144025802612 -1.2488255500793457\n",
      " 0.23086436092853546 -0.04242105036973953 -0.9382444620132446\n",
      " -0.1515507996082306]\n",
      "\n",
      "Image: seq_val/S_2_3_48_0.jpg\n",
      "Actual   : [-0.182836287963 0.927259250715 -0.909521714562 0.0353508967618\n",
      " -0.0586228278325 -0.996510514691 -0.0477542905409]\n",
      "Predicted: [-0.09586847573518753 0.8583598136901855 -1.2212841510772705\n",
      " 0.08601692318916321 -0.05824834853410721 -0.9289944171905518\n",
      " -0.03429314121603966]\n",
      "\n",
      "Image: seq_val/S_2_3_115_0.jpg\n",
      "Actual   : [0.000232100342402 -0.14746060075 -4.03995145768 0.980592553811\n",
      " -0.0192859022493 0.193969394822 0.0210278685792]\n",
      "Predicted: [-0.39779040217399597 -0.21848228573799133 -3.792902946472168\n",
      " 0.9630749821662903 -0.0030296221375465393 0.21746325492858887\n",
      " 0.028977172449231148]\n",
      "\n",
      "Image: seq_val/IMG_9954_0.jpg\n",
      "Actual   : [0.0255287441049 0.860666686896 0.346615986204 0.528208866709\n",
      " 0.0512043700263 0.846351538118 0.0454156310689]\n",
      "Predicted: [0.14865455031394958 0.7376801371574402 0.11152549088001251\n",
      " 0.4981295168399811 0.045487795025110245 0.878570020198822\n",
      " 0.06217629462480545]\n",
      "\n",
      "Image: seq_val/IMG_9688_0.jpg\n",
      "Actual   : [4.14993105901 -1.72520270124 -0.0734621349575 0.83936014179\n",
      " -0.0386411568305 -0.542144020717 -0.00782778224681]\n",
      "Predicted: [4.041373252868652 -1.4633270502090454 -0.3757035732269287\n",
      " 0.8665381669998169 -0.02759552374482155 -0.4973655045032501\n",
      " -0.004991589579731226]\n",
      "\n",
      "Image: seq_val/S_2_3_182_0.jpg\n",
      "Actual   : [-0.93611556314 -1.1603475583 -1.4969118567 0.0900230062497\n",
      " -0.0371254152581 -0.950343293432 -0.295576024935]\n",
      "Predicted: [-1.1026818752288818 -0.393273264169693 -1.2534923553466797\n",
      " 0.08529295027256012 -0.04594874009490013 -0.9584324359893799\n",
      " -0.24004033207893372]\n",
      "\n",
      "Image: seq_val/IMG_8293_0.jpg\n",
      "Actual   : [-0.547844870535 0.876681869228 0.725203693141 0.99351120308\n",
      " -0.014737466068 0.111683254043 0.0156571777625]\n",
      "Predicted: [-0.5658111572265625 0.7246506810188293 1.0170286893844604\n",
      " 0.9723892211914062 -0.002049153670668602 0.13445362448692322\n",
      " 0.02665729448199272]\n",
      "\n",
      "Image: seq_val/IMG_8295_0.jpg\n",
      "Actual   : [0.238292781882 0.79046883689 1.32829829852 0.959682031169 0.0102109835533\n",
      " 0.278988985297 0.0327304284927]\n",
      "Predicted: [1.2197847366333008 0.677668035030365 1.2898337841033936\n",
      " 0.9209669828414917 0.012548677623271942 0.3608874976634979\n",
      " 0.034968167543411255]\n",
      "\n",
      "Image: seq_val/S_2_3_88_0.jpg\n",
      "Actual   : [-1.9517177006 -0.0273816502895 -3.9203954915 0.699007643769\n",
      " -0.0367273775164 -0.713706965373 -0.0257251097498]\n",
      "Predicted: [-1.667132019996643 0.01539413258433342 -3.8936550617218018\n",
      " 0.6874663829803467 -0.034580688923597336 -0.7203134298324585\n",
      " -0.03617442026734352]\n",
      "\n",
      "Image: seq_val/IMG_9617_0.jpg\n",
      "Actual   : [-2.11088270842 -1.2631495913 -0.50681546936 0.5489805892 0.0281152811069\n",
      " 0.833523660987 0.0553908857507]\n",
      "Predicted: [-1.7152246236801147 -0.48795029520988464 -0.8073479533195496\n",
      " 0.5562499165534973 0.03805732727050781 0.8515065312385559\n",
      " 0.06196046993136406]\n",
      "\n",
      "Image: seq_val/S_2_3_80_0.jpg\n",
      "Actual   : [-2.09118506528 0.0207868191645 -4.00587345396 0.0528534448873\n",
      " -0.0810146160149 -0.991991305917 -0.0812181897114]\n",
      "Predicted: [-1.3540605306625366 -0.10303647816181183 -2.5477640628814697\n",
      " 0.15641407668590546 -0.053127471357584 -0.9775351285934448\n",
      " -0.06082389876246452]\n",
      "\n",
      "Image: seq_val/IMG_8296_0.jpg\n",
      "Actual   : [0.455497678062 0.752585823798 1.65706150853 0.948517770809\n",
      " 0.0190554449896 0.314782997202 0.0293699361456]\n",
      "Predicted: [0.8122738599777222 0.7389630675315857 1.5333685874938965\n",
      " 0.9304638504981995 0.009228410199284554 0.3126254677772522\n",
      " 0.0340757742524147]\n",
      "\n",
      "Image: seq_val/IMG_9719_0.jpg\n",
      "Actual   : [-2.79012993681 -1.27108153413 0.765437028372 0.431610027264\n",
      " -0.0519538715212 -0.900055061245 -0.0302401442932]\n",
      "Predicted: [-2.7334961891174316 -0.8452113270759583 0.6611391305923462\n",
      " 0.4481655955314636 -0.0510186143219471 -0.8820173740386963\n",
      " -0.03980382904410362]\n",
      "\n",
      "Image: seq_val/IMG_9924_0.jpg\n",
      "Actual   : [-0.755353665043 0.770242110595 2.48698751127 0.0835531473292\n",
      " -0.0559870281146 -0.994000958656 -0.0429699714279]\n",
      "Predicted: [-0.5382676124572754 0.1875782608985901 1.531488060951233\n",
      " 0.062380194664001465 -0.055897511541843414 -0.9900403022766113\n",
      " -0.05610300227999687]\n",
      "\n",
      "Image: seq_val/IMG_9655_0.jpg\n",
      "Actual   : [1.80962904951 -1.77536276425 3.90532570002 0.808386099695 0.0166192023932\n",
      " 0.587184975476 0.0380725689403]\n",
      "Predicted: [1.649977684020996 -1.671629548072815 3.2510204315185547\n",
      " 0.8451762199401855 0.010948073118925095 0.5000190138816833\n",
      " 0.04030351713299751]\n",
      "\n",
      "Image: seq_val/IMG_9869_0.jpg\n",
      "Actual   : [5.43094603338 0.594362282917 1.97497243599 0.347540634043\n",
      " -0.00361882937959 -0.935619302822 -0.0617975076412]\n",
      "Predicted: [5.035391330718994 0.3269372880458832 2.059237480163574\n",
      " 0.32301270961761475 -0.03927648067474365 -0.932007908821106\n",
      " -0.03531554341316223]\n",
      "\n",
      "Image: seq_val/IMG_9748_0.jpg\n",
      "Actual   : [2.43107683458 -1.68761889427 1.82231834081 0.360407481442\n",
      " 0.00775058831711 0.931468753597 0.0491155451296]\n",
      "Predicted: [2.817551612854004 -1.4073573350906372 1.7253199815750122\n",
      " 0.31346261501312256 0.025059133768081665 0.9293076992034912\n",
      " 0.051222871989011765]\n",
      "\n",
      "Image: seq_val/IMG_9931_0.jpg\n",
      "Actual   : [-0.885033296386 0.890779679727 0.60829172893 0.396924225939\n",
      " -0.0395968912028 -0.915964459221 -0.0435012013045]\n",
      "Predicted: [-0.6075634360313416 0.48904356360435486 0.19940318167209625\n",
      " 0.33561956882476807 -0.042364947497844696 -0.9332685470581055\n",
      " -0.04713490977883339]\n",
      "\n",
      "Image: seq_val/IMG_9758_0.jpg\n",
      "Actual   : [-0.742813043926 -1.32735330537 -0.463148629564 0.696911772951\n",
      " -0.0410721636083 -0.715529498118 -0.0253888838328]\n",
      "Predicted: [-0.1456442028284073 -1.1947721242904663 -0.689328670501709\n",
      " 0.6619830131530762 -0.04355604574084282 -0.7645567655563354\n",
      " -0.030872892588377]\n",
      "\n",
      "Image: seq_val/IMG_9943_0.jpg\n",
      "Actual   : [-1.49145650163 1.01176856115 -0.791777175673 0.00896186364368\n",
      " -0.0513396232994 -0.997409728653 -0.0495758133456]\n",
      "Predicted: [-1.2277190685272217 -0.2018890082836151 -1.3372567892074585\n",
      " 0.06857770681381226 -0.061334799975156784 -0.9742851257324219\n",
      " -0.03383750841021538]\n",
      "\n",
      "Image: seq_val/IMG_9716_0.jpg\n",
      "Actual   : [0.127944638718 -1.60146047738 2.8086616274 0.137446360968\n",
      " -0.0603732992928 -0.988277304701 -0.0277764576797]\n",
      "Predicted: [0.1368137001991272 -1.197463870048523 1.9818098545074463\n",
      " 0.13891105353832245 -0.05409776046872139 -0.9770345687866211\n",
      " -0.047655798494815826]\n",
      "\n",
      "Image: seq_val/IMG_9755_0.jpg\n",
      "Actual   : [-0.433595296367 -1.43010469429 0.803316890202 0.338329642931\n",
      " -0.0465781373076 -0.939272448502 -0.0336273300069]\n",
      "Predicted: [-0.5034093260765076 -1.2558043003082275 0.7011750936508179\n",
      " 0.30884942412376404 -0.04928285628557205 -0.9388005137443542\n",
      " -0.04731830582022667]\n",
      "\n",
      "Image: seq_val/IMG_9803_0.jpg\n",
      "Actual   : [-0.561554115965 0.761745952362 2.56096209113 0.870604166331\n",
      " -0.0073112522212 0.491625918002 0.0172883749072]\n",
      "Predicted: [-0.9769951105117798 0.6883525252342224 2.62969708442688\n",
      " 0.8955296874046326 0.018594928085803986 0.44475632905960083\n",
      " 0.04038851335644722]\n",
      "\n",
      "Image: seq_val/S_2_3_15_0.jpg\n",
      "Actual   : [-1.47076251442 1.02419229564 -1.38650281805 0.692699262408\n",
      " -0.0268910880312 -0.720465854607 -0.0193275345046]\n",
      "Predicted: [-1.425271987915039 0.29566583037376404 -1.6946101188659668\n",
      " 0.6349660158157349 -0.03003930300474167 -0.7538963556289673\n",
      " -0.03791875019669533]\n",
      "\n",
      "Image: seq_val/IMG_9658_0.jpg\n",
      "Actual   : [0.963920245557 -1.5656208031 1.02508465364 0.997798369644\n",
      " -0.0492846925256 -0.034623374063 0.0277606661796]\n",
      "Predicted: [1.023032546043396 -1.5066615343093872 0.6684227585792542\n",
      " 0.9854094386100769 -0.02192884497344494 -0.07512630522251129\n",
      " 0.023772869259119034]\n",
      "\n",
      "Image: seq_val/S_2_3_45_0.jpg\n",
      "Actual   : [-0.177810666759 0.962618287793 -0.883693363878 0.0414914031227\n",
      " -0.0651319435269 -0.987323741518 -0.138665507009]\n",
      "Predicted: [-0.009788759052753448 0.749539315700531 -0.8952165842056274\n",
      " 0.0435185432434082 -0.060146741569042206 -0.9892495274543762\n",
      " -0.12131150811910629]\n",
      "\n",
      "Image: seq_val/S_2_3_62_0.jpg\n",
      "Actual   : [0.503486734648 0.890807544754 -1.10530939903 0.246639855112\n",
      " 0.0275325583245 0.967350458182 0.0514181987033]\n",
      "Predicted: [0.4713496267795563 0.6354133486747742 -1.3856651782989502\n",
      " 0.25624001026153564 0.02925936132669449 0.9449466466903687\n",
      " 0.04294537380337715]\n",
      "\n",
      "Image: seq_val/S_2_3_82_0.jpg\n",
      "Actual   : [-2.02575144562 0.00920246175781 -4.01474239487 0.300447110975\n",
      " -0.0586467713315 -0.950348383098 -0.0559467645308]\n",
      "Predicted: [-1.6328046321868896 -0.10979209840297699 -3.285614013671875\n",
      " 0.2596380412578583 -0.04280221462249756 -0.9420562982559204\n",
      " -0.07038091868162155]\n",
      "\n",
      "Image: seq_val/IMG_8284_0.jpg\n",
      "Actual   : [-0.416178348775 0.653259655368 4.28653893974 0.73285519105\n",
      " 0.0373652859868 0.677521622069 0.0499154884091]\n",
      "Predicted: [-0.3249247968196869 0.21270641684532166 4.012624740600586\n",
      " 0.7456800937652588 0.033994968980550766 0.6589771509170532\n",
      " 0.04560954496264458]\n",
      "\n",
      "Image: seq_val/IMG_9833_0.jpg\n",
      "Actual   : [1.7633926443 0.699691877772 0.913963518509 0.93103472953 -0.0133406475076\n",
      " -0.364683933344 0.00141006847081]\n",
      "Predicted: [1.7869415283203125 0.6279013752937317 0.5031678080558777\n",
      " 0.9137148261070251 -0.02962712198495865 -0.39366480708122253\n",
      " 0.0008287699893116951]\n",
      "\n",
      "Image: seq_val/IMG_9914_0.jpg\n",
      "Actual   : [0.692428179733 0.706806840496 2.16876688731 0.0731085583742\n",
      " 0.0404135539616 0.995101380322 0.0528689533632]\n",
      "Predicted: [0.6373558640480042 0.36702123284339905 1.932952880859375\n",
      " 0.045197755098342896 0.03917990252375603 0.992997407913208\n",
      " 0.056743670254945755]\n",
      "\n",
      "Image: seq_val/IMG_9832_0.jpg\n",
      "Actual   : [0.888342540009 0.717666190907 1.67283865249 0.873853785574\n",
      " -0.0335457279127 -0.484966956712 -0.00782920650898]\n",
      "Predicted: [1.6456737518310547 0.7202847003936768 1.2290605306625366\n",
      " 0.8771162629127502 -0.03535059839487076 -0.45371729135513306\n",
      " -4.392396658658981e-05]\n",
      "\n",
      "Image: seq_val/IMG_9934_0.jpg\n",
      "Actual   : [-1.06361601589 0.989212787236 -0.962059176403 0.729634845932\n",
      " -0.0306013459907 -0.681886802979 -0.0415564332985]\n",
      "Predicted: [-1.030545949935913 0.599941074848175 -0.8290143609046936\n",
      " 0.6699650287628174 -0.02413281239569187 -0.707206130027771\n",
      " -0.036183495074510574]\n",
      "\n",
      "Image: seq_val/S_2_3_194_0.jpg\n",
      "Actual   : [-1.95576947232 -1.32131181943 0.858965054025 0.300600501538\n",
      " -0.0663125432222 -0.94921856262 -0.0650085030129]\n",
      "Predicted: [-1.3060864210128784 -1.1608542203903198 0.78770512342453\n",
      " 0.2896486520767212 -0.05216822400689125 -0.9168645739555359\n",
      " -0.04505433514714241]\n",
      "\n",
      "Image: seq_val/S_2_3_114_0.jpg\n",
      "Actual   : [0.0387266437155 -0.155410981742 -4.03325605828 0.95473101591\n",
      " -0.0176729107778 0.294254073452 0.039886034403]\n",
      "Predicted: [-0.29184794425964355 -0.15410970151424408 -3.779862403869629\n",
      " 0.9413151741027832 0.009778190404176712 0.3028262257575989\n",
      " 0.032888785004615784]\n",
      "\n",
      "Image: seq_val/IMG_9915_0.jpg\n",
      "Actual   : [-0.0717885510241 0.833996274062 1.61143903237 0.036548913795\n",
      " -0.0534286578214 -0.99482271769 -0.0783410224068]\n",
      "Predicted: [-0.05931330472230911 0.28943106532096863 1.3476457595825195\n",
      " 0.036820366978645325 -0.05614509806036949 -0.9324387311935425\n",
      " -0.04445689916610718]\n",
      "\n",
      "Image: seq_val/IMG_9818_0.jpg\n",
      "Actual   : [-1.45692410934 0.881099112692 1.3480069601 0.944268959929\n",
      " -0.0493856595996 -0.325436550635 -0.00287044447017]\n",
      "Predicted: [-0.7199869155883789 0.7559998631477356 1.3023561239242554\n",
      " 0.9526296854019165 -0.025779593735933304 -0.26133760809898376\n",
      " 0.008282270282506943]\n",
      "\n",
      "Image: seq_val/IMG_9929_0.jpg\n",
      "Actual   : [2.68375222669 0.713540070632 1.24468755559 0.403543487638 0.0301363048252\n",
      " 0.912896021615 0.0535285945704]\n",
      "Predicted: [3.0264334678649902 0.4033149182796478 1.4545859098434448\n",
      " 0.3812927305698395 0.035074662417173386 0.9154050350189209\n",
      " 0.05435686185956001]\n",
      "\n",
      "Image: seq_val/IMG_9897_0.jpg\n",
      "Actual   : [4.78223828324 0.63808578022 -0.863687217611 0.502046439281 0.05094011603\n",
      " 0.859804320124 0.0780449132223]\n",
      "Predicted: [5.250140190124512 0.44070008397102356 -0.49462246894836426\n",
      " 0.46621274948120117 0.03750382363796234 0.8752577900886536\n",
      " 0.055815439671278]\n",
      "\n",
      "Image: seq_val/S_2_3_152_0.jpg\n",
      "Actual   : [-1.24638327458 -0.0357132169581 -4.65898943453 0.984028182547\n",
      " -0.0630968548305 -0.166073357224 0.0112677807993]\n",
      "Predicted: [-1.1890523433685303 -0.06976823508739471 -4.546121120452881\n",
      " 0.9712750315666199 -0.017891936004161835 -0.16216132044792175\n",
      " 0.0030599497258663177]\n",
      "\n",
      "Image: seq_val/IMG_9631_0.jpg\n",
      "Actual   : [-2.61967194918 -1.28681941348 0.634400521139 0.962762733947\n",
      " -0.00908881890919 0.267748614085 0.0362765922942]\n",
      "Predicted: [-2.5533268451690674 -0.9457311034202576 0.5256731510162354\n",
      " 0.969378650188446 -0.007257129997014999 0.2104719579219818\n",
      " 0.03641596809029579]\n",
      "\n",
      "Image: seq_val/IMG_8198_0.jpg\n",
      "Actual   : [-2.1646691008 0.871077379349 2.33068753237 0.741445456967 0.048165917806\n",
      " 0.666328027197 0.0628143206308]\n",
      "Predicted: [-2.14424204826355 0.6955046057701111 2.4517455101013184\n",
      " 0.7721781730651855 0.040294136852025986 0.6361687779426575\n",
      " 0.048827845603227615]\n",
      "\n",
      "Image: seq_val/S_2_3_175_0.jpg\n",
      "Actual   : [-0.134490896241 -1.3148878823 -0.6950420666 0.0566444874776\n",
      " 0.0614011820441 0.990068459494 0.113074057137]\n",
      "Predicted: [-0.010026194155216217 -0.3914400041103363 -1.082608938217163\n",
      " 0.10453978180885315 0.04998163506388664 0.9904857873916626\n",
      " 0.09399410337209702]\n",
      "\n",
      "Image: seq_val/IMG_9917_0.jpg\n",
      "Actual   : [-1.349049256 0.850492013732 1.62931199697 0.232629952671 -0.0563357608607\n",
      " -0.969240134862 -0.0572987621228]\n",
      "Predicted: [-0.9170019030570984 0.6142339110374451 1.3788487911224365\n",
      " 0.18156105279922485 -0.049058012664318085 -0.9557894468307495\n",
      " -0.049070537090301514]\n",
      "\n",
      "Image: seq_val/IMG_9955_0.jpg\n",
      "Actual   : [-0.24952541837 0.848863249101 0.898839820805 0.326765818086\n",
      " 0.0350383570613 0.94297891556 0.052793735173]\n",
      "Predicted: [-0.4354154169559479 0.037317994982004166 0.8061121106147766\n",
      " 0.2886200547218323 0.042836856096982956 0.9400066137313843\n",
      " 0.06132805347442627]\n",
      "\n",
      "Image: seq_val/IMG_9878_0.jpg\n",
      "Actual   : [4.04523390568 0.601617983512 -0.244089141611 0.503853250497\n",
      " -0.0315848986188 -0.862789049405 -0.0270065245819]\n",
      "Predicted: [3.3688530921936035 0.444920152425766 -0.33939653635025024\n",
      " 0.5976260900497437 -0.03693310171365738 -0.8064165115356445\n",
      " -0.028910785913467407]\n",
      "\n",
      "Image: seq_val/IMG_9811_0.jpg\n",
      "Actual   : [1.24556150103 0.686543907891 1.95611671886 0.903689078746 0.0106216863246\n",
      " 0.426844307562 0.0322050591763]\n",
      "Predicted: [1.217921495437622 0.4617762267589569 2.328540563583374 0.8773411512374878\n",
      " 0.012584246695041656 0.4424111247062683 0.038872335106134415]\n",
      "\n",
      "Image: seq_val/IMG_9780_0.jpg\n",
      "Actual   : [-1.33878074859 0.886486494664 0.652417200152 0.734913655137\n",
      " 0.010685067265 0.676954218544 0.038996600171]\n",
      "Predicted: [-1.501675009727478 0.43433672189712524 0.6253613233566284\n",
      " 0.7687084078788757 0.02686741203069687 0.6448379158973694\n",
      " 0.048662543296813965]\n",
      "\n",
      "Image: seq_val/S_2_3_20_0.jpg\n",
      "Actual   : [-0.223341491728 0.782237625358 -2.34734773347 0.962785448677\n",
      " 0.117382401686 -0.241820188583 -0.0280810965555]\n",
      "Predicted: [-0.4658949375152588 0.6945014595985413 -2.3403759002685547\n",
      " 0.9483439326286316 0.09618069231510162 -0.24561259150505066\n",
      " -0.017555249854922295]\n",
      "\n",
      "Image: seq_val/S_2_3_144_0.jpg\n",
      "Actual   : [-1.29009953443 -0.228789267199 -3.4495113695 0.834752405763\n",
      " -0.0255909811374 0.549414735831 0.0260186626884]\n",
      "Predicted: [-1.1441892385482788 0.056774187833070755 -3.1362826824188232\n",
      " 0.833381175994873 0.008067578077316284 0.5410133004188538\n",
      " 0.04614432156085968]\n",
      "\n",
      "Image: seq_val/IMG_8278_0.jpg\n",
      "Actual   : [-1.52606904652 0.859093830603 2.07971455213 0.951812683444\n",
      " 0.00467902459758 0.305037619379 0.0313492125563]\n",
      "Predicted: [-1.3781968355178833 0.7521788477897644 1.5690240859985352\n",
      " 0.9497535824775696 0.013676118105649948 0.31142497062683105\n",
      " 0.03540077432990074]\n",
      "\n",
      "Image: seq_val/IMG_9882_0.jpg\n",
      "Actual   : [3.85277868207 0.62144423868 -0.348892259913 0.0443015871321\n",
      " 0.0331428940272 0.998348013039 0.0154971873222]\n",
      "Predicted: [3.7252209186553955 0.639857828617096 -0.2327437847852707\n",
      " 0.07573522627353668 0.031472668051719666 1.0105080604553223\n",
      " 0.0337088480591774]\n",
      "\n",
      "Image: seq_val/IMG_8281_0.jpg\n",
      "Actual   : [-0.412438046454 0.747616496261 3.00700611091 0.905337117726\n",
      " 0.0407405729629 0.419388910346 0.0530834330137]\n",
      "Predicted: [-0.40476909279823303 0.657762885093689 2.7609636783599854\n",
      " 0.9018111228942871 0.02123570814728737 0.4225459694862366\n",
      " 0.03829087316989899]\n",
      "\n",
      "Image: seq_val/IMG_9815_0.jpg\n",
      "Actual   : [2.10344342897 0.561840043994 3.60996311892 0.81998494581 0.00713218103179\n",
      " 0.571515958311 0.0307136783017]\n",
      "Predicted: [1.5657578706741333 0.216892808675766 3.5918478965759277\n",
      " 0.7924061417579651 0.018453530967235565 0.5811300277709961\n",
      " 0.037229109555482864]\n",
      "\n",
      "Image: seq_val/IMG_9769_0.jpg\n",
      "Actual   : [-2.01550417881 0.976735397853 0.0675357979926 0.485070096021\n",
      " 0.0512032513689 0.871637193268 0.0483097537457]\n",
      "Predicted: [-1.6375302076339722 0.38831350207328796 0.03877417743206024\n",
      " 0.4762563705444336 0.04647386446595192 0.8904412984848022\n",
      " 0.06312534958124161]\n",
      "\n",
      "Image: seq_val/IMG_9690_0.jpg\n",
      "Actual   : [5.07193752615 -1.67210230713 -0.883072031569 0.94493041515\n",
      " 0.000117546699367 -0.325267755777 0.0361577620924]\n",
      "Predicted: [4.938145160675049 -1.5237904787063599 -0.8516160249710083\n",
      " 0.9419081807136536 -0.015474832616746426 -0.28539857268333435\n",
      " 0.011426283046603203]\n",
      "\n",
      "Image: seq_val/S_2_3_38_0.jpg\n",
      "Actual   : [-1.04540393375 0.966214514108 -0.905062386048 0.166259123232\n",
      " -0.0825619839377 -0.981080377638 0.0549792266543]\n",
      "Predicted: [-0.6576008796691895 0.7864230275154114 -0.9177682995796204\n",
      " 0.18624688684940338 -0.05753560736775398 -0.9522862434387207\n",
      " -0.0029210587963461876]\n",
      "\n",
      "Image: seq_val/IMG_9935_0.jpg\n",
      "Actual   : [-0.91868016217 0.994846522531 -1.48083377098 0.792705342299\n",
      " -0.0246998771866 -0.608636528651 -0.0238690667819]\n",
      "Predicted: [-0.9305310249328613 0.7387153506278992 -1.2628778219223022\n",
      " 0.7722796201705933 -0.010029992088675499 -0.6092101335525513\n",
      " -0.03280707076191902]\n",
      "\n",
      "Image: seq_val/IMG_9724_0.jpg\n",
      "Actual   : [-2.3032476754 -1.3654858893 1.77316779814 0.279176001236 -0.0517741597656\n",
      " -0.958196565068 -0.0352070931351]\n",
      "Predicted: [-2.0100722312927246 -1.1593890190124512 1.6676561832427979\n",
      " 0.28684282302856445 -0.054856523871421814 -0.9459326267242432\n",
      " -0.036248233169317245]\n",
      "\n",
      "Image: seq_val/IMG_9939_0.jpg\n",
      "Actual   : [-1.38794349697 0.981797871248 -0.761484567136 0.535168959236\n",
      " -0.0489059809427 -0.843002182633 -0.0234458988795]\n",
      "Predicted: [-0.6828010082244873 0.3572050631046295 -0.1169949620962143\n",
      " 0.5632878541946411 -0.03664954751729965 -0.8048325777053833\n",
      " -0.03596002981066704]\n",
      "\n",
      "Image: seq_val/IMG_9760_0.jpg\n",
      "Actual   : [-0.752758991128 -1.25885355856 -1.53107533028 0.837776129173\n",
      " -0.0426120254271 -0.544218183088 -0.0119139361063]\n",
      "Predicted: [-0.6630987524986267 -0.5963426828384399 -1.8553860187530518\n",
      " 0.834362804889679 -0.03557586669921875 -0.5458458065986633\n",
      " -0.01670173555612564]\n",
      "\n",
      "Image: seq_val/IMG_9625_0.jpg\n",
      "Actual   : [-3.44241840751 -1.38966898282 3.4763559764 0.204641997618 0.032034278129\n",
      " 0.976906422368 0.0524337655649]\n",
      "Predicted: [-3.407170295715332 -0.4756665527820587 3.787328004837036\n",
      " 0.2141488790512085 0.04294640198349953 0.961125373840332\n",
      " 0.051344022154808044]\n",
      "\n",
      "Image: seq_val/IMG_8274_0.jpg\n",
      "Actual   : [-2.9198992455 0.930716252421 2.22871007146 0.99093881292 -0.0256563659643\n",
      " -0.131338296817 0.0115009444873]\n",
      "Predicted: [-2.8382985591888428 0.9658803343772888 2.218569040298462\n",
      " 0.9784984588623047 -0.00693261343985796 -0.1175704225897789\n",
      " 0.012420851737260818]\n",
      "\n",
      "Image: seq_val/IMG_9728_0.jpg\n",
      "Actual   : [-0.793542979967 -1.36516900149 0.415863429074 0.329852591045\n",
      " -0.034032827985 -0.941058677085 -0.0666903372645]\n",
      "Predicted: [-0.8374962210655212 -1.1937254667282104 0.15437869727611542\n",
      " 0.3411749601364136 -0.04648834466934204 -0.928584635257721\n",
      " -0.05582999438047409]\n",
      "\n",
      "Image: seq_val/S_2_3_101_0.jpg\n",
      "Actual   : [0.0877652672469 -0.0661010613911 -3.85273583307 0.958958450577\n",
      " 0.243807648835 0.141343876805 0.0312798485702]\n",
      "Predicted: [-0.30835258960723877 0.32153528928756714 -3.956782817840576\n",
      " 0.9634345769882202 0.21071197092533112 0.1021232083439827\n",
      " 0.009490683674812317]\n",
      "\n",
      "Image: seq_val/IMG_9741_0.jpg\n",
      "Actual   : [-3.81657926281 -1.22457020955 1.13993194233 0.494309626265\n",
      " -0.0434050503242 -0.867392605454 -0.0374708285407]\n",
      "Predicted: [-2.8778035640716553 -0.9085487723350525 0.7421091794967651\n",
      " 0.44667452573776245 -0.05150231719017029 -0.8643158078193665\n",
      " -0.03905360773205757]\n",
      "\n",
      "Image: seq_val/IMG_8277_0.jpg\n",
      "Actual   : [-1.88784291477 0.875107942802 2.04702297112 0.972272517386\n",
      " -0.0127260973759 0.231163953261 0.0329761291572]\n",
      "Predicted: [-2.504875421524048 0.7536852955818176 2.368054151535034\n",
      " 0.9885281324386597 0.005545472726225853 0.1456259787082672\n",
      " 0.028043167665600777]\n",
      "\n",
      "Image: seq_val/S_2_3_122_0.jpg\n",
      "Actual   : [-1.09490369404 -0.0340650120806 -4.60053796364 0.991255432094\n",
      " -0.00211037530972 0.131733146997 0.0073886834004]\n",
      "Predicted: [-1.1963529586791992 0.005634639412164688 -4.582882881164551\n",
      " 0.9855977296829224 -0.0001543872058391571 0.11331696063280106\n",
      " 0.023370493203401566]\n",
      "\n",
      "Image: seq_val/IMG_9694_0.jpg\n",
      "Actual   : [5.3635073788 -1.80953682038 0.894257679168 0.42305237337 -0.0409449116569\n",
      " -0.904727537846 -0.0286056962647]\n",
      "Predicted: [6.870202541351318 -1.2120169401168823 1.371835708618164\n",
      " 0.39215466380119324 -0.03432641923427582 -0.9019545316696167\n",
      " -0.034282151609659195]\n",
      "\n",
      "Image: seq_val/IMG_9691_0.jpg\n",
      "Actual   : [5.59361875872 -1.68730357073 -1.56287487883 0.966634992315\n",
      " -0.0271061684931 -0.254315238668 0.0143459625994]\n",
      "Predicted: [4.482267379760742 -1.3298346996307373 -1.2045378684997559\n",
      " 0.9441152811050415 -0.013271717354655266 -0.237856924533844\n",
      " 0.013540854677557945]\n",
      "\n",
      "Image: seq_val/IMG_9912_0.jpg\n",
      "Actual   : [2.54024460097 0.605534182141 1.90256897668 0.272387103073 0.0399487433236\n",
      " 0.959484332524 0.0599931631768]\n",
      "Predicted: [3.5548157691955566 0.00022310391068458557 0.9879708290100098\n",
      " 0.30014634132385254 0.037723150104284286 0.9387400150299072\n",
      " 0.057893551886081696]\n",
      "\n",
      "Image: seq_val/IMG_9870_0.jpg\n",
      "Actual   : [6.07600946242 0.561108806425 2.38641492869 0.196858612035\n",
      " -0.0329620663925 -0.976971428431 -0.0754123138305]\n",
      "Predicted: [5.298222541809082 0.24772676825523376 2.0512735843658447\n",
      " 0.2140721082687378 -0.04265991598367691 -0.9665133357048035\n",
      " -0.05346761271357536]\n",
      "\n",
      "Image: seq_val/IMG_9824_0.jpg\n",
      "Actual   : [-0.706666297469 0.785223477193 2.31829011058 0.83280738007\n",
      " -0.0391388738979 -0.551907774622 0.0172575943378]\n",
      "Predicted: [-0.03223777562379837 0.6073914170265198 1.9691176414489746\n",
      " 0.8486765027046204 -0.03549879044294357 -0.49535638093948364\n",
      " -0.002274830359965563]\n",
      "\n",
      "Image: seq_val/IMG_9794_0.jpg\n",
      "Actual   : [-1.67037241978 0.906855389867 1.6605664354 0.99551976764\n",
      " -0.00428621059661 0.0903592843478 0.0275176374303]\n",
      "Predicted: [-1.997599482536316 0.8806256055831909 1.453203558921814\n",
      " 0.9844518899917603 0.006900874897837639 0.07394320517778397\n",
      " 0.02164027839899063]\n",
      "\n",
      "Image: seq_val/IMG_9948_0.jpg\n",
      "Actual   : [-0.299550117995 0.829151926486 1.38696840591 0.00240705197394\n",
      " -0.0375054914779 -0.997965829182 -0.0514951260185]\n",
      "Predicted: [-0.30574658513069153 0.3172167241573334 1.0194222927093506\n",
      " 0.019791871309280396 -0.05887055769562721 -0.9899312257766724\n",
      " -0.05196387693285942]\n",
      "\n",
      "Image: seq_val/IMG_8205_0.jpg\n",
      "Actual   : [-2.94842772444 0.917723603344 3.13934698587 0.978859981211\n",
      " 0.0473176014606 0.197464197849 0.0245371625015]\n",
      "Predicted: [-3.171217918395996 0.8503844738006592 2.779327392578125\n",
      " 0.9827162623405457 0.015554387122392654 0.08905880898237228\n",
      " 0.023618370294570923]\n",
      "\n",
      "Image: seq_val/IMG_9854_0.jpg\n",
      "Actual   : [3.51052348899 0.804044135649 -3.26061440238 0.873532014408\n",
      " -0.0453773626636 -0.48443098781 -0.0144683382417]\n",
      "Predicted: [3.1589393615722656 0.41044881939888 -1.8880424499511719\n",
      " 0.8601452112197876 -0.021320847794413567 -0.4930960536003113\n",
      " -0.01052110269665718]\n",
      "\n",
      "Image: seq_val/S_2_3_123_0.jpg\n",
      "Actual   : [-1.09386596049 -0.0355828042234 -4.59702894563 0.997099229143\n",
      " -0.00381210186607 0.075627160067 0.00768945908426]\n",
      "Predicted: [-1.2934165000915527 0.018724020570516586 -4.473006725311279\n",
      " 0.9899711608886719 -0.0021629296243190765 0.060658566653728485\n",
      " 0.01943991892039776]\n",
      "\n",
      "Image: seq_val/S_2_3_132_0.jpg\n",
      "Actual   : [-1.40126550549 -0.224946997183 -3.34305825752 0.997908706808\n",
      " -0.036647891338 -0.0462865652389 0.0263191720322]\n",
      "Predicted: [-1.031482219696045 -0.1356663703918457 -2.8834011554718018\n",
      " 0.9780294299125671 -0.018052345141768456 -0.10797974467277527\n",
      " 0.010334216058254242]\n",
      "\n",
      "Image: seq_val/IMG_9767_0.jpg\n",
      "Actual   : [-2.58995142233 1.18630235511 -2.48624436456 0.812345387061\n",
      " 0.0230709934218 0.579238427242 0.0636046050939]\n",
      "Predicted: [-1.6334986686706543 0.3225918114185333 -2.2884182929992676\n",
      " 0.7246569395065308 0.02258211001753807 0.691282331943512\n",
      " 0.05191550403833389]\n",
      "\n",
      "Image: seq_val/S_2_3_134_0.jpg\n",
      "Actual   : [-1.32268893804 -0.231756987992 -3.3426489284 0.982909686243\n",
      " -0.0391665504057 -0.178692459034 0.0205799684709]\n",
      "Predicted: [-1.136068344116211 -0.09046097099781036 -3.1010901927948\n",
      " 0.9689735770225525 -0.017705243080854416 -0.18419092893600464\n",
      " 0.0034867417998611927]\n",
      "\n",
      "Image: seq_val/S_2_3_1_0.jpg\n",
      "Actual   : [1.09742241067 0.722838657279 1.42221243279 0.21628259422 0.0431256993694\n",
      " 0.972702301601 0.0721958859785]\n",
      "Predicted: [0.7681407928466797 0.33720043301582336 1.102764368057251\n",
      " 0.205983966588974 0.03460340201854706 0.9450544118881226\n",
      " 0.057644154876470566]\n",
      "\n",
      "Image: seq_val/IMG_9853_0.jpg\n",
      "Actual   : [4.58331222253 0.478877735244 1.92406384565 0.346140956017 -0.040036083808\n",
      " -0.937129883896 -0.0192647675984]\n",
      "Predicted: [4.341224193572998 0.3345099091529846 2.02254319190979 0.34196004271507263\n",
      " -0.04091353714466095 -0.9207351803779602 -0.032058726996183395]\n",
      "\n",
      "Image: seq_val/S_2_3_108_0.jpg\n",
      "Actual   : [-0.0131891928606 -0.139403632943 -4.10859904164 0.760415279779\n",
      " 0.0497093455716 0.643453092801 0.072565147324]\n",
      "Predicted: [-0.2975142002105713 -0.012944424524903297 -3.9956653118133545\n",
      " 0.7887512445449829 0.041145894676446915 0.6029787063598633\n",
      " 0.06115327402949333]\n",
      "\n",
      "Image: seq_val/IMG_8273_0.jpg\n",
      "Actual   : [-3.38997520929 0.953055676462 2.46700145797 0.967755252709\n",
      " -0.0212066880867 -0.250913795702 0.00650494901029]\n",
      "Predicted: [-3.3092713356018066 0.9232054352760315 2.341691732406616\n",
      " 0.9599537253379822 -0.012148231267929077 -0.2372039258480072\n",
      " 0.0054166726768016815]\n",
      "\n",
      "Image: seq_val/IMG_9712_0.jpg\n",
      "Actual   : [-0.24120818401 -1.40821929649 0.447655472444 0.548097957493\n",
      " -0.047907551605 -0.834748912741 -0.022085021387]\n",
      "Predicted: [0.015689872205257416 -1.5687605142593384 0.30953115224838257\n",
      " 0.4719449281692505 -0.047849610447883606 -0.8735033273696899\n",
      " -0.03463339805603027]\n",
      "\n",
      "Image: seq_val/S_2_3_43_0.jpg\n",
      "Actual   : [-0.164528389061 0.999001321395 -0.881145184251 0.0353230952198\n",
      " -0.0753409740838 -0.970523855628 -0.226184575571]\n",
      "Predicted: [-0.32680603861808777 0.4371945858001709 -1.18297278881073\n",
      " 0.021409332752227783 -0.056943222880363464 -0.9861837029457092\n",
      " -0.17529568076133728]\n",
      "\n",
      "Image: seq_val/IMG_9873_0.jpg\n",
      "Actual   : [5.14249889903 0.445062665701 3.33240101558 0.436014398619 -0.023613921576\n",
      " -0.898462697454 -0.0458105685287]\n",
      "Predicted: [4.961951732635498 0.2913564145565033 2.9621529579162598 0.417621910572052\n",
      " -0.03755448758602142 -0.9010151624679565 -0.030668437480926514]\n",
      "\n",
      "Image: seq_val/S_2_3_42_0.jpg\n",
      "Actual   : [-1.02707900604 0.985447062652 -0.902231195585 0.294578941011\n",
      " -0.0679343356569 -0.952866456993 -0.0255673363715]\n",
      "Predicted: [-0.6505274176597595 1.0018221139907837 -0.9163169860839844\n",
      " 0.24718895554542542 -0.05328821390867233 -0.9518135190010071\n",
      " -0.03157864138484001]\n",
      "\n",
      "Image: seq_val/S_2_3_18_0.jpg\n",
      "Actual   : [-1.50036016739 1.042260283 -1.43912906257 0.928874489253 0.0162092609783\n",
      " -0.370005359898 -0.00504744683885]\n",
      "Predicted: [-1.0823966264724731 0.631549596786499 -1.2319717407226562\n",
      " 0.9294334053993225 0.022020012140274048 -0.3500472903251648\n",
      " -0.008763406425714493]\n",
      "\n",
      "Image: seq_val/IMG_9857_0.jpg\n",
      "Actual   : [2.63825924126 0.704161461766 -0.355445265203 0.781450347558\n",
      " -0.0523415019659 -0.621684469317 -0.0102050028807]\n",
      "Predicted: [2.6292266845703125 0.6622840762138367 -0.3999057412147522\n",
      " 0.7647578716278076 -0.029450152069330215 -0.6552175283432007\n",
      " -0.017558783292770386]\n",
      "\n",
      "Image: seq_val/S_2_3_75_0.jpg\n",
      "Actual   : [0.11637996765 0.35852329197 -3.21344771732 0.125254539093 0.0424170248324\n",
      " 0.991006985802 0.0204267112714]\n",
      "Predicted: [0.08663260191679001 0.2976500391960144 -2.160372018814087\n",
      " 0.09335651993751526 0.03905084729194641 0.9892115592956543\n",
      " 0.0627664104104042]\n",
      "\n",
      "Image: seq_val/IMG_9842_0.jpg\n",
      "Actual   : [5.18343453371 0.539308914574 -0.128642730677 0.981814490823\n",
      " -0.0198083845439 -0.186675267956 0.028289182459]\n",
      "Predicted: [4.942205429077148 0.32924166321754456 0.2792208790779114\n",
      " 0.9744749665260315 -0.02135191299021244 -0.19680285453796387\n",
      " 0.0140554029494524]\n",
      "\n",
      "Image: seq_val/IMG_9781_0.jpg\n",
      "Actual   : [-1.62269432765 0.853059767341 1.95433097815 0.598320856121\n",
      " 0.0449013441743 0.798859258474 0.0426603747334]\n",
      "Predicted: [-1.479548692703247 0.49336758255958557 0.9782484769821167\n",
      " 0.6440145373344421 0.03608474135398865 0.7763912677764893\n",
      " 0.0537264309823513]\n",
      "\n",
      "Image: seq_val/IMG_9851_0.jpg\n",
      "Actual   : [4.62010547425 0.46036155732 1.95787767838 0.593075891717 -0.0512305275362\n",
      " -0.803475043322 -0.00801713607227]\n",
      "Predicted: [4.891948223114014 0.35070309042930603 2.1373937129974365\n",
      " 0.5619511604309082 -0.03581191971898079 -0.8131997585296631\n",
      " -0.02256724052131176]\n",
      "\n",
      "Image: seq_val/IMG_9949_0.jpg\n",
      "Actual   : [-0.336596501492 0.832905393062 1.96247720212 0.0037323600453\n",
      " 0.0387529428173 0.998518349191 0.0380182224878]\n",
      "Predicted: [-0.09011908620595932 0.3911679685115814 1.3341580629348755\n",
      " 0.005568593740463257 0.03953450173139572 0.9981610774993896\n",
      " 0.06793393939733505]\n",
      "\n",
      "Image: seq_val/IMG_9843_0.jpg\n",
      "Actual   : [6.04900107849 0.498339766458 -0.383642849087 0.995242354626\n",
      " -0.0168575135382 -0.0945976128624 0.0161174264725]\n",
      "Predicted: [5.527009010314941 0.4013330042362213 -0.1411246806383133 0.97856205701828\n",
      " -0.005497440695762634 -0.1006474643945694 0.01854657009243965]\n",
      "\n",
      "Image: seq_val/IMG_9749_0.jpg\n",
      "Actual   : [3.02615004291 -1.68659720769 1.1068100905 0.474587733988 0.00923552690514\n",
      " 0.879206461824 0.0409534525771]\n",
      "Predicted: [2.9633383750915527 -1.1687705516815186 0.9641693830490112\n",
      " 0.4537804424762726 0.023828081786632538 0.8930441737174988\n",
      " 0.049759287387132645]\n",
      "\n",
      "Image: seq_val/S_2_3_179_0.jpg\n",
      "Actual   : [-0.216385791378 -1.28880195058 -0.942669263832 0.370335819045\n",
      " 0.077540856651 0.921364204168 0.0890325780852]\n",
      "Predicted: [-0.24172666668891907 -0.822709858417511 -1.1551074981689453\n",
      " 0.3983127176761627 0.05688946321606636 0.9025874137878418\n",
      " 0.08578823506832123]\n",
      "\n",
      "Image: seq_val/IMG_9735_0.jpg\n",
      "Actual   : [-1.40734968038 -1.23970150026 -1.13270146022 0.514481105429\n",
      " -0.04058809858 -0.856040171004 -0.0292749728822]\n",
      "Predicted: [-0.9954236149787903 -1.0396004915237427 -1.0704697370529175\n",
      " 0.4326569437980652 -0.04594091325998306 -0.8967244029045105\n",
      " -0.05237706005573273]\n",
      "\n",
      "Image: seq_val/IMG_9908_0.jpg\n",
      "Actual   : [2.38785195728 0.964915854322 -2.03856219906 0.877878293078\n",
      " 0.0574563225372 -0.468820765325 -0.0789655845419]\n",
      "Predicted: [2.15191650390625 0.5947973132133484 -2.371286630630493 0.8776695132255554\n",
      " 0.04225887358188629 -0.44723567366600037 -0.04938912019133568]\n",
      "\n",
      "Image: seq_val/IMG_9698_0.jpg\n",
      "Actual   : [5.42389687092 -1.87210325111 1.82371369118 0.228616974207\n",
      " -0.0397834655651 -0.972232890845 -0.0302450149723]\n",
      "Predicted: [5.883254051208496 -1.6981697082519531 2.0065879821777344\n",
      " 0.22321176528930664 -0.04409212991595268 -0.9808514714241028\n",
      " -0.04098742455244064]\n",
      "\n",
      "Image: seq_val/IMG_9852_0.jpg\n",
      "Actual   : [4.63297425585 0.471239119986 1.92093464867 0.459657481696\n",
      " -0.0397455174092 -0.887090147173 -0.0143653804259]\n",
      "Predicted: [5.195107460021973 0.352542906999588 2.3468387126922607\n",
      " 0.47196775674819946 -0.0370723158121109 -0.8737136721611023\n",
      " -0.027852008119225502]\n",
      "\n",
      "Image: seq_val/IMG_9801_0.jpg\n",
      "Actual   : [-2.09158404651 0.888945968479 2.33939930277 0.967597437293\n",
      " 0.0252133933348 0.249950093507 0.0253857222867]\n",
      "Predicted: [-2.2515652179718018 0.5954720973968506 2.1854302883148193\n",
      " 0.9732142686843872 0.011535368859767914 0.2419808804988861\n",
      " 0.03396514058113098]\n",
      "\n",
      "Image: seq_val/IMG_9766_0.jpg\n",
      "Actual   : [-2.81284290885 1.21392848323 -3.3571407511 0.885218870594\n",
      " 0.00157628077607 0.461956563683 0.0546003640397]\n",
      "Predicted: [-2.188560724258423 0.13609731197357178 -3.427896738052368\n",
      " 0.8519225716590881 0.01188373751938343 0.4845191240310669\n",
      " 0.04517310485243797]\n",
      "\n",
      "Image: seq_val/IMG_9776_0.jpg\n",
      "Actual   : [-2.92217457434 0.91368069889 2.77913139255 0.268733747075 0.049107414598\n",
      " 0.960743246233 0.0484050599882]\n",
      "Predicted: [-2.3824751377105713 0.15687109529972076 2.922208070755005\n",
      " 0.2934657633304596 0.04429573938250542 0.9387466907501221\n",
      " 0.047681763768196106]\n",
      "\n",
      "Image: seq_val/IMG_8299_0.jpg\n",
      "Actual   : [1.07439539842 0.653384948162 2.8193722278 0.856918646756 0.0157641658523\n",
      " 0.514429891137 0.0283515612038]\n",
      "Predicted: [1.47855544090271 0.5249298810958862 2.6820201873779297 0.8453003168106079\n",
      " 0.01914316415786743 0.5084854960441589 0.03923815116286278]\n",
      "\n",
      "Image: seq_val/S_2_3_117_0.jpg\n",
      "Actual   : [0.0329173094535 -0.150742526638 -4.02077443537 0.996807043162\n",
      " -0.0274569029852 0.0738875063113 0.0127465129735]\n",
      "Predicted: [-0.3258706033229828 -0.19023793935775757 -3.859968662261963\n",
      " 0.9843651652336121 -0.010262330994009972 0.05261469632387161\n",
      " 0.02139701321721077]\n",
      "\n",
      "Image: seq_val/S_2_3_71_0.jpg\n",
      "Actual   : [0.078904090015 0.487439139241 -2.99853535225 0.250574856384\n",
      " -0.0671952208873 -0.965023780661 -0.0377643535105]\n",
      "Predicted: [-0.9643240571022034 0.2645702660083771 -1.8877614736557007\n",
      " 0.20173192024230957 -0.05228710547089577 -0.9641191363334656\n",
      " -0.05501740053296089]\n",
      "\n",
      "Image: seq_val/S_2_3_69_0.jpg\n",
      "Actual   : [0.0828390073533 0.600616546001 -2.82056364397 0.130628029415\n",
      " 0.0465390051932 0.989890492597 0.0297867687509]\n",
      "Predicted: [-0.10774971544742584 0.23025977611541748 -2.7478432655334473\n",
      " 0.1348734200000763 0.040213827043771744 0.9578397274017334\n",
      " 0.04834846779704094]\n",
      "\n",
      "Image: seq_val/S_2_3_148_0.jpg\n",
      "Actual   : [-1.28585542609 0.0230265957287 -4.65124222568 0.99474143285\n",
      " 0.0979024515655 0.029653970119 0.00502332615155]\n",
      "Predicted: [-1.3205387592315674 -0.015294857323169708 -4.996026992797852\n",
      " 0.9840198755264282 0.08557625114917755 0.035607486963272095\n",
      " 0.017616989091038704]\n",
      "\n",
      "Image: seq_val/IMG_9876_0.jpg\n",
      "Actual   : [6.28714407835 0.548128215878 2.40111974298 0.672094220616 -0.031302383559\n",
      " -0.737419291958 -0.0593490290188]\n",
      "Predicted: [5.558071136474609 0.3584861755371094 2.12825608253479 0.613834023475647\n",
      " -0.03610771894454956 -0.7658776044845581 -0.019411131739616394]\n",
      "\n",
      "Image: seq_val/IMG_9667_0.jpg\n",
      "Actual   : [4.31924525266 -1.73920666411 0.0592371336802 0.974460137878\n",
      " -0.0120751605552 0.221836920421 0.0327110214279]\n",
      "Predicted: [4.156686305999756 -1.3621916770935059 0.33948957920074463\n",
      " 0.9780014157295227 -0.0028428025543689728 0.17032673954963684\n",
      " 0.03204391151666641]\n",
      "\n",
      "Image: seq_val/IMG_9696_0.jpg\n",
      "Actual   : [4.86835061226 -1.71360157872 -0.232491716525 0.664212156527\n",
      " -0.0324643266771 -0.746687460126 -0.0150371375366]\n",
      "Predicted: [4.16788911819458 -1.6386381387710571 -0.39956027269363403\n",
      " 0.6478995084762573 -0.0368577241897583 -0.7294262051582336\n",
      " -0.023851312696933746]\n",
      "\n",
      "Image: seq_val/S_2_3_141_0.jpg\n",
      "Actual   : [-1.27462906457 -0.236999652513 -3.36796789644 0.995072718773\n",
      " -0.041248693007 0.0892341464843 0.0128878539823]\n",
      "Predicted: [-1.2568556070327759 -0.2575165033340454 -3.516185998916626\n",
      " 0.9890367388725281 -0.011304100975394249 0.06621447950601578\n",
      " 0.0209101103246212]\n",
      "\n",
      "Image: seq_val/IMG_9805_0.jpg\n",
      "Actual   : [-3.47336085479 1.00784561483 2.05094290237 0.954220713758\n",
      " -0.00365994911715 -0.299078170311 0.00129701652346]\n",
      "Predicted: [-3.497687816619873 0.686482846736908 2.30540132522583 0.9368423819541931\n",
      " -0.015432908199727535 -0.32879403233528137 -0.0014708139933645725]\n",
      "\n",
      "Image: seq_val/IMG_8201_0.jpg\n",
      "Actual   : [-2.25763536027 0.830073859989 3.17071021009 0.712691139858\n",
      " 0.0465130204791 0.697377612935 0.0597690812298]\n",
      "Predicted: [-2.0914454460144043 0.6513091325759888 3.2315075397491455\n",
      " 0.6843814253807068 0.0483119897544384 0.7205373644828796\n",
      " 0.05057132989168167]\n",
      "\n",
      "Image: seq_val/IMG_9622_0.jpg\n",
      "Actual   : [-1.87621870506 -1.35212660422 0.629090126476 0.728753845259\n",
      " 0.0171833500478 0.681938085664 0.059858272799]\n",
      "Predicted: [-1.9564263820648193 -1.2092230319976807 0.09781302511692047\n",
      " 0.6709260940551758 0.029071494936943054 0.7203636169433594\n",
      " 0.06441774219274521]\n",
      "\n",
      "Image: seq_val/S_2_3_4_0.jpg\n",
      "Actual   : [-0.632438693273 0.838553904994 1.08562738609 0.141346591728\n",
      " -0.0523647062646 -0.986228246773 -0.0680655846377]\n",
      "Predicted: [-0.5239561796188354 0.5740019679069519 0.5916330814361572\n",
      " 0.18971166014671326 -0.04912826791405678 -0.9423412680625916\n",
      " -0.055998701602220535]\n",
      "\n",
      "Image: seq_val/S_2_3_184_0.jpg\n",
      "Actual   : [-1.017618848 -1.13641123336 -1.6646993473 0.130459104436 0.0808832528921\n",
      " 0.950598908501 0.269814819141]\n",
      "Predicted: [-0.6189272403717041 -0.2308223396539688 -1.7540689706802368\n",
      " 0.1404310017824173 0.08111655712127686 0.923089325428009\n",
      " 0.1745055913925171]\n",
      "\n",
      "Image: seq_val/IMG_9709_0.jpg\n",
      "Actual   : [2.25026010042 -1.66388758845 1.82659820547 0.162763675755\n",
      " -0.0602405451514 -0.984505271523 -0.0250685643204]\n",
      "Predicted: [2.868332862854004 -1.307085633277893 1.7898147106170654\n",
      " 0.14378349483013153 -0.05219791457056999 -0.9733980894088745\n",
      " -0.03725052624940872]\n",
      "\n",
      "Image: seq_val/S_2_3_139_0.jpg\n",
      "Actual   : [-1.2289744017 -0.234453963334 -3.36505081638 0.986849896019\n",
      " -0.0343326547918 -0.157862962415 0.00527604390405]\n",
      "Predicted: [-1.0636237859725952 -0.07061120867729187 -3.2560691833496094\n",
      " 0.9735267162322998 -0.016370899975299835 -0.17304055392742157\n",
      " 0.0037978836335241795]\n",
      "\n",
      "Image: seq_val/IMG_8291_0.jpg\n",
      "Actual   : [-1.6670359163 0.927495276292 0.930921617264 0.998764658701\n",
      " -0.0172664335977 -0.0463624976367 0.00464172521219]\n",
      "Predicted: [-1.4552454948425293 0.8573122024536133 1.1599870920181274\n",
      " 0.9827913045883179 -0.005793048068881035 -0.03998508304357529\n",
      " 0.01613367721438408]\n",
      "\n",
      "Image: seq_val/IMG_9620_0.jpg\n",
      "Actual   : [-2.99156713409 -1.07183054242 -2.36874857538 0.960063570001\n",
      " -0.00125326772297 0.276388869566 0.0434230774688]\n",
      "Predicted: [-2.584498882293701 -0.702693521976471 -1.641282081604004\n",
      " 0.9364739060401917 -0.004791682586073875 0.3015076518058777\n",
      " 0.04010119289159775]\n",
      "\n",
      "Image: seq_val/IMG_9888_0.jpg\n",
      "Actual   : [2.48860689118 0.662460069973 0.225863113052 0.0424948835181\n",
      " 0.0347332564452 0.99824066431 0.0224357279457]\n",
      "Predicted: [1.8804454803466797 0.4910418689250946 0.27693504095077515\n",
      " 0.07262024283409119 0.037287287414073944 0.976590633392334\n",
      " 0.05998263135552406]\n",
      "\n",
      "Image: seq_val/IMG_9654_0.jpg\n",
      "Actual   : [1.68507957178 -1.73179788251 3.40736593236 0.852278369409 0.031019000656\n",
      " 0.520408924642 0.0428246866696]\n",
      "Predicted: [1.5490812063217163 -1.5371476411819458 3.3958070278167725\n",
      " 0.8471343517303467 0.011718031018972397 0.5052804946899414\n",
      " 0.04501262307167053]\n",
      "\n",
      "Image: seq_val/IMG_9647_0.jpg\n",
      "Actual   : [0.542578900666 -1.65675618848 3.36735810499 0.881022533656\n",
      " 0.00906755677791 0.471196802957 0.0411174839657]\n",
      "Predicted: [1.374672532081604 -1.6943076848983765 2.7595508098602295\n",
      " 0.8719043731689453 0.010291101410984993 0.46713078022003174\n",
      " 0.04674594849348068]\n",
      "\n",
      "Image: seq_val/IMG_9921_0.jpg\n",
      "Actual   : [-3.39952139927 0.979143841672 1.59849587642 0.366906647458\n",
      " -0.040267940818 -0.928573927327 -0.038838981477]\n",
      "Predicted: [-2.1592583656311035 0.24311941862106323 0.9875580072402954\n",
      " 0.3531268835067749 -0.03883204609155655 -0.9115502238273621\n",
      " -0.056028347462415695]\n",
      "\n",
      "Image: seq_val/IMG_9638_0.jpg\n",
      "Actual   : [-2.37055588275 -1.38325475969 2.25783798977 0.936466012685\n",
      " 0.00956339267234 0.346197385322 0.0555636482144]\n",
      "Predicted: [-2.419025182723999 -0.9119142889976501 1.8504204750061035\n",
      " 0.9628192186355591 0.0073641035705804825 0.2684119939804077\n",
      " 0.03882099315524101]\n",
      "\n",
      "Image: seq_val/S_2_3_118_0.jpg\n",
      "Actual   : [-1.13581157638 -0.0167183623932 -4.64217791226 0.926128391529\n",
      " 0.0166440186913 0.375715048041 0.0291098217406]\n",
      "Predicted: [-0.48022544384002686 -0.17727789282798767 -4.226314067840576\n",
      " 0.9105362296104431 0.028362561017274857 0.3732278347015381\n",
      " 0.04557211697101593]\n",
      "\n",
      "Image: seq_val/IMG_9660_0.jpg\n",
      "Actual   : [3.69538507823 -1.78591214919 1.72381354121 0.935276142723\n",
      " -0.0291091884584 0.351837777594 0.0249272994604]\n",
      "Predicted: [4.288717269897461 -1.7573041915893555 1.3331609964370728\n",
      " 0.9192374348640442 -0.005098095163702965 0.3709995448589325\n",
      " 0.040059152990579605]\n",
      "\n",
      "Image: seq_val/IMG_9782_0.jpg\n",
      "Actual   : [-2.18603419519 0.841607840434 2.92327941805 0.425959706924\n",
      " 0.0355164478358 0.902096858288 0.0593141490486]\n",
      "Predicted: [-2.489234209060669 0.38762491941452026 3.500173807144165\n",
      " 0.42888128757476807 0.04980723187327385 0.8863605856895447\n",
      " 0.05130705609917641]\n",
      "\n",
      "Image: seq_val/S_2_3_97_0.jpg\n",
      "Actual   : [-0.259363906434 -0.0192166266654 -4.49994573154 0.968164623772\n",
      " 0.220134415511 0.114891115891 0.0315932251308]\n",
      "Predicted: [-0.1753045916557312 0.024266380816698074 -4.147136211395264\n",
      " 0.9655358791351318 0.17407549917697906 0.10885050147771835\n",
      " 0.02087392471730709]\n",
      "\n",
      "Image: seq_val/IMG_9640_0.jpg\n",
      "Actual   : [-1.65293612453 -1.4987029016 3.27352548018 0.847330762373\n",
      " 0.00779376310711 0.527414304577 0.0616764762321]\n",
      "Predicted: [-1.1745039224624634 -1.3742059469223022 3.0472004413604736\n",
      " 0.8458290696144104 0.01503685861825943 0.5204808115959167\n",
      " 0.04905260354280472]\n",
      "\n",
      "Image: seq_val/IMG_9616_0.jpg\n",
      "Actual   : [-2.45594251809 -1.1247044053 -2.26640007168 0.873470755212\n",
      " -0.00353065754543 0.483536791614 0.0568202904152]\n",
      "Predicted: [-2.7331185340881348 -0.7434754371643066 -2.256474494934082\n",
      " 0.9168291091918945 0.0026843007653951645 0.38876456022262573\n",
      " 0.04647516831755638]\n",
      "\n",
      "Image: seq_val/S_2_3_156_0.jpg\n",
      "Actual   : [-2.13172382141 0.0352036074828 -4.40137988529 0.926430782648\n",
      " -0.0018286448764 -0.37514452428 -0.0314522959841]\n",
      "Predicted: [-2.0697481632232666 -0.016885388642549515 -4.58361291885376\n",
      " 0.9380263090133667 -0.00022489950060844421 -0.3585748076438904\n",
      " -0.030089110136032104]\n",
      "\n",
      "Image: seq_val/S_2_3_185_0.jpg\n",
      "Actual   : [-1.04189593618 -1.16279060861 -1.67457584745 0.127264325962\n",
      " -0.0452259436969 -0.959944548636 -0.245489040281]\n",
      "Predicted: [-1.043091893196106 -0.46889200806617737 -1.4205095767974854\n",
      " 0.12130841612815857 -0.041237592697143555 -0.9558463096618652\n",
      " -0.1538057178258896]\n",
      "\n",
      "Image: seq_val/S_2_3_60_0.jpg\n",
      "Actual   : [0.462708360352 0.900494787072 -1.08955234022 0.383473345021\n",
      " 0.0368459408264 0.920968456435 0.0583752734822]\n",
      "Predicted: [0.8055115342140198 0.5608827471733093 -0.46279799938201904\n",
      " 0.30392664670944214 0.035630062222480774 0.9335806369781494\n",
      " 0.0527329295873642]\n",
      "\n",
      "Image: seq_val/S_2_3_110_0.jpg\n",
      "Actual   : [-0.00472774074349 -0.145018611874 -4.056241621 0.84013817009\n",
      " 0.0238901850887 0.539835673249 0.0466321788345]\n",
      "Predicted: [-0.26454877853393555 -0.020981039851903915 -3.6447174549102783\n",
      " 0.840496301651001 0.017229020595550537 0.5196610689163208\n",
      " 0.048505913466215134]\n",
      "\n",
      "Image: seq_val/IMG_9786_0.jpg\n",
      "Actual   : [-1.31397516387 0.905403956044 0.534021246171 0.894383157667\n",
      " 0.0207516514486 0.443082995511 0.0576679749331]\n",
      "Predicted: [-1.1628206968307495 0.9484372138977051 -0.07566231489181519\n",
      " 0.8577100038528442 0.019745811820030212 0.5024698376655579\n",
      " 0.044243719428777695]\n",
      "\n",
      "Image: seq_val/IMG_9830_0.jpg\n",
      "Actual   : [6.59100381314 0.343829963163 1.53545581734 0.937923752497\n",
      " 0.00496886734198 0.34583813024 0.0258907808099]\n",
      "Predicted: [5.727298736572266 0.10868701338768005 2.002669095993042\n",
      " 0.9110887050628662 0.007963811978697777 0.3567207157611847\n",
      " 0.03036351501941681]\n",
      "\n",
      "Image: seq_val/S_2_3_50_0.jpg\n",
      "Actual   : [-0.181446876177 0.914165089734 -0.891829352024 0.0138473578786\n",
      " -0.0529173609511 -0.998424051972 0.0125465546674]\n",
      "Predicted: [-0.18611374497413635 0.719458281993866 -0.9885474443435669\n",
      " 0.058409884572029114 -0.06134718656539917 -0.9724160432815552\n",
      " -0.02165386639535427]\n",
      "\n",
      "Image: seq_val/S_2_3_126_0.jpg\n",
      "Actual   : [-1.08877064618 -0.0334657295384 -4.55267183837 0.984730953494\n",
      " -0.00219592249897 -0.173803209055 -0.00962141766305]\n",
      "Predicted: [-1.0690118074417114 -0.0672464445233345 -4.5835041999816895\n",
      " 0.9677019119262695 -0.017118539661169052 -0.17732512950897217\n",
      " 0.0013635249342769384]\n",
      "\n",
      "Image: seq_val/IMG_9750_0.jpg\n",
      "Actual   : [3.33885549806 -1.66453567324 0.508331362097 0.545053552791\n",
      " 0.0170938890273 0.837205547279 0.0413678033299]\n",
      "Predicted: [3.7205910682678223 -1.1679433584213257 0.9759190082550049\n",
      " 0.5495975017547607 0.02246708795428276 0.8504857420921326\n",
      " 0.04975449666380882]\n",
      "\n",
      "Image: seq_val/IMG_9718_0.jpg\n",
      "Actual   : [-2.12193000574 -1.36103148172 1.37124452473 0.32007104482\n",
      " -0.0554184892233 -0.94547349775 -0.0237314637611]\n",
      "Predicted: [-1.6311014890670776 -0.9700478315353394 1.3583974838256836\n",
      " 0.31467339396476746 -0.051558032631874084 -0.9297990798950195\n",
      " -0.039729464799165726]\n",
      "\n",
      "Image: seq_val/S_2_3_12_0.jpg\n",
      "Actual   : [-1.46545702457 1.03604650408 -1.45598734462 0.538746317957\n",
      " -0.0318663643104 -0.84129491217 -0.0309808081076]\n",
      "Predicted: [-1.3658230304718018 0.7549346685409546 -1.2492241859436035\n",
      " 0.5715606212615967 -0.03754470497369766 -0.8147411346435547\n",
      " -0.04008715972304344]\n",
      "\n",
      "Image: seq_val/S_2_3_120_0.jpg\n",
      "Actual   : [-1.11665451173 -0.0312309708031 -4.63046128938 0.96641694462\n",
      " 0.00155739249608 0.256139716079 0.0206956402869]\n",
      "Predicted: [-0.7970375418663025 -0.09489118307828903 -4.048162460327148\n",
      " 0.9536000490188599 -0.0026243850588798523 0.2727010250091553\n",
      " 0.03167764097452164]\n",
      "\n",
      "Image: seq_val/IMG_9744_0.jpg\n",
      "Actual   : [-1.77022617254 -1.47211731073 2.61780872076 0.183554999763\n",
      " -0.0560425547306 -0.981143176847 -0.0229098373561]\n",
      "Predicted: [-1.0525628328323364 -1.1919840574264526 2.081371307373047\n",
      " 0.15996628999710083 -0.05573514476418495 -0.9592558145523071\n",
      " -0.036423955112695694]\n",
      "\n",
      "Image: seq_val/S_2_3_149_0.jpg\n",
      "Actual   : [-1.29165179944 0.00230672174853 -4.66084497059 0.998724343143\n",
      " 0.0411729052979 0.0268654361194 0.0115207041552]\n",
      "Predicted: [-1.354109764099121 -0.04086468741297722 -4.654334545135498\n",
      " 0.9817395806312561 0.0126153863966465 0.011515103280544281\n",
      " 0.016512278467416763]\n",
      "\n",
      "Image: seq_val/IMG_9726_0.jpg\n",
      "Actual   : [-3.08463201673 -1.25906302978 0.955178736452 0.493507050415\n",
      " -0.0378059019312 -0.86843526106 -0.0290121063909]\n",
      "Predicted: [-2.7632875442504883 -1.0566660165786743 1.0755754709243774\n",
      " 0.43597009778022766 -0.05352168530225754 -0.8837639093399048\n",
      " -0.03859705105423927]\n",
      "\n",
      "Image: seq_val/S_2_3_100_0.jpg\n",
      "Actual   : [0.0786825469134 -0.05515627248 -3.86768036286 0.944652941827\n",
      " 0.258091114275 0.19200260603 0.0644577032384]\n",
      "Predicted: [-0.1000165268778801 0.29144275188446045 -3.816505193710327\n",
      " 0.9553257822990417 0.19619765877723694 0.1525881290435791\n",
      " 0.01666702888906002]\n",
      "\n",
      "Image: seq_val/S_2_3_178_0.jpg\n",
      "Actual   : [-0.333939490726 1.10731849401 -1.01718941793 0.650300445161\n",
      " 0.0130748727048 0.758995378245 0.0294005872388]\n",
      "Predicted: [-0.7548452019691467 -0.17323097586631775 -0.9915987849235535\n",
      " 0.6477709412574768 0.0287029966711998 0.7826783657073975\n",
      " 0.057425763458013535]\n",
      "\n",
      "Image: seq_val/S_2_3_195_0.jpg\n",
      "Actual   : [-0.265684753825 0.47291091587 1.40525371618 0.0310680960083\n",
      " -0.0455888216616 0.98737259491 0.14849845646]\n",
      "Predicted: [-0.23985588550567627 -0.4521878659725189 1.0564231872558594\n",
      " 0.08571621775627136 0.03524626046419144 0.8188983201980591\n",
      " 0.0740358978509903]\n",
      "\n",
      "Image: seq_val/S_2_3_160_0.jpg\n",
      "Actual   : [-2.08839329009 0.121469169625 -4.41126756912 0.867337713392 0.17973621732\n",
      " -0.441478099568 -0.143238509884]\n",
      "Predicted: [-2.0928220748901367 0.20892125368118286 -4.76171350479126\n",
      " 0.8816083073616028 0.18684597313404083 -0.43011677265167236\n",
      " -0.12710528075695038]\n",
      "\n",
      "Image: seq_val/IMG_9829_0.jpg\n",
      "Actual   : [5.90268997423 0.419438951166 0.914229153863 0.967717480405\n",
      " 0.00631209486122 0.251015601143 0.0217762154491]\n",
      "Predicted: [5.362492084503174 0.22276216745376587 1.1668025255203247\n",
      " 0.9462876319885254 0.003745386376976967 0.24063640832901\n",
      " 0.028843844309449196]\n",
      "\n",
      "Image: seq_val/IMG_9950_0.jpg\n",
      "Actual   : [-0.462824498965 0.98271415126 -1.73804807649 0.918548643651\n",
      " 0.0231566035642 0.393691336249 0.0271899378589]\n",
      "Predicted: [-0.5649227499961853 0.9417557716369629 -1.3688970804214478\n",
      " 0.919378399848938 0.04228069633245468 0.38220223784446716\n",
      " 0.04147642105817795]\n",
      "\n",
      "Image: seq_val/IMG_9706_0.jpg\n",
      "Actual   : [5.81604375011 -1.89733264473 1.73605069777 0.225544981855 0.0258701539131\n",
      " 0.973779649814 0.0146078713483]\n",
      "Predicted: [5.232880592346191 -1.1779652833938599 1.1876428127288818\n",
      " 0.2534906268119812 0.022421423345804214 0.9254745244979858\n",
      " 0.02607080154120922]\n",
      "\n",
      "Image: seq_val/IMG_9940_0.jpg\n",
      "Actual   : [-1.41504039297 0.995574183847 -0.763287604463 0.408090373375\n",
      " -0.0472445660589 -0.911487720958 -0.0205020164598]\n",
      "Predicted: [-1.2011181116104126 0.8687645196914673 -0.6716765761375427\n",
      " 0.3981516361236572 -0.044428832828998566 -0.9066382646560669\n",
      " -0.045474614948034286]\n",
      "\n",
      "Image: seq_val/IMG_9628_0.jpg\n",
      "Actual   : [-0.91454794673 -1.55014923546 2.99884405808 0.613898689941\n",
      " 0.0377325717166 0.787521612998 0.0389147861592]\n",
      "Predicted: [-1.1346908807754517 -1.055595874786377 1.8497726917266846\n",
      " 0.6463509798049927 0.02584218606352806 0.7722011208534241\n",
      " 0.05825614556670189]\n",
      "\n",
      "Image: seq_val/S_2_3_14_0.jpg\n",
      "Actual   : [-1.48073455848 1.03455624466 -1.40806434522 0.576110341094\n",
      " -0.0307650996843 -0.816155240921 -0.0322646283672]\n",
      "Predicted: [-1.3189805746078491 0.6764325499534607 -1.3187979459762573\n",
      " 0.5856192708015442 -0.03840933367609978 -0.8185800313949585\n",
      " -0.03584520146250725]\n",
      "\n",
      "Image: seq_val/IMG_9821_0.jpg\n",
      "Actual   : [3.17010371274 0.572273855238 0.861802936935 0.973227264341\n",
      " 0.0138231504148 0.228725419663 0.0179525724102]\n",
      "Predicted: [3.0039658546447754 0.34262388944625854 0.6934962272644043\n",
      " 0.9633520841598511 0.0021301545202732086 0.2232460379600525\n",
      " 0.030400164425373077]\n",
      "\n",
      "Image: seq_val/IMG_8275_0.jpg\n",
      "Actual   : [-2.47801588421 0.908849364655 2.09584847391 0.999240077058\n",
      " -0.0181102819148 -0.0315812311856 0.0139252262468]\n",
      "Predicted: [-2.886578321456909 0.8503329157829285 1.9660954475402832\n",
      " 0.9906495809555054 -0.00028160959482192993 -0.018175914883613586\n",
      " 0.01678755134344101]\n",
      "\n",
      "Image: seq_val/IMG_9637_0.jpg\n",
      "Actual   : [-2.87123471701 -1.33015282857 1.95506159885 0.976802225172\n",
      " -0.00344922016833 0.209955492624 0.0420024629837]\n",
      "Predicted: [-2.308896541595459 -0.8532639741897583 1.6782326698303223\n",
      " 0.9783458709716797 0.004206661134958267 0.19042426347732544\n",
      " 0.03424401953816414]\n",
      "\n",
      "Image: seq_val/S_2_3_81_0.jpg\n",
      "Actual   : [-2.06838109483 0.0185499176605 -4.01730707828 0.157681263833\n",
      " -0.070183410269 -0.982089286001 -0.075574746318]\n",
      "Predicted: [-1.4270724058151245 -0.02464401349425316 -2.638915777206421\n",
      " 0.10660190880298615 -0.05738922953605652 -0.9898908138275146\n",
      " -0.07853595167398453]\n",
      "\n",
      "Image: seq_val/IMG_9796_0.jpg\n",
      "Actual   : [-2.17112447296 1.01923128859 3.82908220131 0.887418044208 0.0172325002575\n",
      " 0.459232437591 0.0360253246165]\n",
      "Predicted: [-2.492239475250244 0.9247345924377441 3.4146230220794678\n",
      " 0.9024146199226379 0.034474533051252365 0.3972685635089874\n",
      " 0.03791171684861183]\n",
      "\n",
      "Image: seq_val/IMG_8276_0.jpg\n",
      "Actual   : [-2.14537552465 0.90899310136 2.0077962152 0.999886128076 -0.0105251423368\n",
      " 0.00557674491579 0.00926564491524]\n",
      "Predicted: [-2.5897605419158936 0.9663464426994324 1.6659915447235107\n",
      " 0.9917171597480774 -0.00176316499710083 -0.0467863604426384\n",
      " 0.01492653600871563]\n",
      "\n",
      "Image: seq_val/S_2_3_32_0.jpg\n",
      "Actual   : [-0.984172401542 0.998206113166 -0.897766646759 0.42501830049\n",
      " -0.034402551177 -0.901439774628 -0.0747143991447]\n",
      "Predicted: [-1.1044583320617676 0.7363691329956055 -0.8205773830413818\n",
      " 0.4191330671310425 -0.04127776622772217 -0.8934950232505798\n",
      " -0.05332757532596588]\n",
      "\n",
      "Image: seq_val/IMG_9826_0.jpg\n",
      "Actual   : [1.55551704385 0.715893920235 0.509799915001 0.967387715663\n",
      " -0.026299607376 -0.251449990831 0.0155640723059]\n",
      "Predicted: [1.9206501245498657 0.5763221979141235 0.6363972425460815\n",
      " 0.9630030393600464 -0.022582637146115303 -0.2256883680820465\n",
      " 0.010703766718506813]\n",
      "\n",
      "Image: seq_val/IMG_9807_0.jpg\n",
      "Actual   : [-2.11794148846 0.965192987587 0.848828505039 0.994215383942\n",
      " -0.0210371815208 -0.104022301231 0.0165096388211]\n",
      "Predicted: [-2.2770485877990723 0.7893872857093811 1.4399445056915283\n",
      " 0.9861318469047546 -0.005050012841820717 -0.07507229596376419\n",
      " 0.014524472877383232]\n",
      "\n",
      "Image: seq_val/IMG_9859_0.jpg\n",
      "Actual   : [3.28140623254 0.552066733068 1.83644706164 0.520660487777\n",
      " -0.0494491035748 -0.852317927263 0.00464687963723]\n",
      "Predicted: [3.290238380432129 0.40576744079589844 2.1037867069244385\n",
      " 0.49978959560394287 -0.04080028086900711 -0.8536005020141602\n",
      " -0.024413270875811577]\n",
      "\n",
      "Image: seq_val/S_2_3_124_0.jpg\n",
      "Actual   : [-1.11885884023 -0.0366762845235 -4.58301797647 0.999934468231\n",
      " -0.00196798796327 -0.00985441411952 0.00548423098255]\n",
      "Predicted: [-1.2002007961273193 0.022836972028017044 -4.775243282318115\n",
      " 0.9909578561782837 0.003956351429224014 0.023347772657871246\n",
      " 0.01635897159576416]\n",
      "\n",
      "Image: seq_val/IMG_8289_0.jpg\n",
      "Actual   : [-2.78807367905 0.9597973146 1.60896869368 0.947009681696 -0.0357861680964\n",
      " -0.319002036984 -0.0113891767636]\n",
      "Predicted: [-2.655851364135742 0.5954230427742004 1.7109811305999756\n",
      " 0.9369450807571411 -0.025292670354247093 -0.3150708079338074\n",
      " 0.003128017531707883]\n",
      "\n",
      "Image: seq_val/IMG_9892_0.jpg\n",
      "Actual   : [3.57287904512 0.688626993072 -1.02516663599 0.224030894122\n",
      " -0.0419615406246 -0.97315258312 -0.0319912105717]\n",
      "Predicted: [3.2177622318267822 0.675244152545929 -0.6854984760284424\n",
      " 0.21389508247375488 -0.04507461562752724 -0.9736483693122864\n",
      " -0.03750259801745415]\n",
      "\n",
      "Image: seq_val/S_2_3_59_0.jpg\n",
      "Actual   : [0.446174408605 0.899874900682 -1.07403436068 0.454894244277\n",
      " 0.0387865621028 0.887726278251 0.0592358339756]\n",
      "Predicted: [0.3548847734928131 0.7575788497924805 -1.0094664096832275\n",
      " 0.47058406472206116 0.04164121672511101 0.8856710195541382\n",
      " 0.0606117881834507]\n",
      "\n",
      "Image: seq_val/S_2_3_145_0.jpg\n",
      "Actual   : [-1.28919439877 -0.224099847945 -3.49347837929 0.73385807405\n",
      " -0.0188508584061 0.678386754363 0.0298057678008]\n",
      "Predicted: [-0.6443641781806946 0.212434321641922 -2.728487491607666\n",
      " 0.6400542259216309 0.024586256593465805 0.8045330047607422\n",
      " 0.05167757347226143]\n",
      "\n",
      "Image: seq_val/IMG_9922_0.jpg\n",
      "Actual   : [-2.63927932336 0.895368219987 2.07272464718 0.307720653112\n",
      " -0.0642990351047 -0.948742872633 -0.0325667831066]\n",
      "Predicted: [-2.5369222164154053 0.20865178108215332 1.7428269386291504\n",
      " 0.30930933356285095 -0.044074513018131256 -0.9366629123687744\n",
      " -0.05228500813245773]\n",
      "\n",
      "Image: seq_val/S_2_3_112_0.jpg\n",
      "Actual   : [0.013175792505 -0.150113849031 -4.03684554178 0.908020584363\n",
      " -0.00567636102209 0.41656688323 0.0440275946911]\n",
      "Predicted: [-0.02200300246477127 -0.026470741257071495 -3.4214696884155273\n",
      " 0.896848201751709 0.00911606103181839 0.425874799489975\n",
      " 0.03993834927678108]\n",
      "\n",
      "Image: seq_val/IMG_9945_0.jpg\n",
      "Actual   : [-0.547716485771 0.932369411028 -0.107786224241 0.0457000835186\n",
      " 0.0337093880463 0.99367207663 0.0969070877166]\n",
      "Predicted: [-0.3041554391384125 0.24027040600776672 -0.41461288928985596\n",
      " 0.02546626329421997 0.0440332405269146 1.0009710788726807\n",
      " 0.09539549052715302]\n",
      "\n",
      "Image: seq_val/IMG_9615_0.jpg\n",
      "Actual   : [-2.72197491946 -1.04357581206 -3.17164246532 0.93172672426\n",
      " -0.0157733820624 0.359755621595 0.0470362035992]\n",
      "Predicted: [-2.444958209991455 -0.5496055483818054 -2.9518587589263916\n",
      " 0.9368588328361511 -0.002980545163154602 0.33248957991600037\n",
      " 0.0424032099545002]\n",
      "\n",
      "Image: seq_val/S_2_3_68_0.jpg\n",
      "Actual   : [0.125772263741 0.597377180207 -2.81227670615 0.0186769349558\n",
      " 0.0575379866895 0.997726699343 0.0296982424974]\n",
      "Predicted: [0.10934650152921677 0.40892818570137024 -2.8228607177734375\n",
      " 0.11289389431476593 0.03970014676451683 0.9620163440704346\n",
      " 0.049637116491794586]\n",
      "\n",
      "Image: seq_val/IMG_9773_0.jpg\n",
      "Actual   : [-2.21896228612 1.08258807791 -1.16233900002 0.790438036659\n",
      " 0.0505517187193 0.606431152365 0.0699534943948]\n",
      "Predicted: [-2.084031105041504 0.6134846806526184 -0.7310117483139038\n",
      " 0.8403229117393494 0.03145170211791992 0.5221578478813171\n",
      " 0.04616124927997589]\n",
      "\n",
      "Image: seq_val/IMG_9937_0.jpg\n",
      "Actual   : [-1.37454883141 0.976121358266 -0.71346733479 0.663746489219\n",
      " -0.0483009746746 -0.746334258478 -0.00963267958291]\n",
      "Predicted: [-1.1516494750976562 0.549062967300415 -0.6578726172447205\n",
      " 0.6739680767059326 -0.03020116686820984 -0.7387254238128662\n",
      " -0.032273780554533005]\n",
      "\n",
      "Image: seq_val/IMG_9725_0.jpg\n",
      "Actual   : [-2.79364576501 -1.31028816961 1.42383686548 0.399475896795\n",
      " -0.0417308064687 -0.914864435515 -0.0412384808239]\n",
      "Predicted: [-2.2648990154266357 -0.8371996283531189 1.4699887037277222\n",
      " 0.3652169704437256 -0.05142690986394882 -0.9162247180938721\n",
      " -0.037347130477428436]\n",
      "\n",
      "Image: seq_val/IMG_9919_0.jpg\n",
      "Actual   : [-2.54046285286 0.964635233038 1.18419521221 0.388166305557\n",
      " -0.0402922069805 -0.918058941726 -0.069794246213]\n",
      "Predicted: [-2.0874452590942383 0.3672654628753662 1.1112661361694336\n",
      " 0.3554794490337372 -0.040886301547288895 -0.9177521467208862\n",
      " -0.051046453416347504]\n",
      "\n",
      "Image: seq_val/IMG_9812_0.jpg\n",
      "Actual   : [1.5753148221 0.620922651393 2.74272411391 0.872117469685\n",
      " -0.00226876663668 0.48804886864 0.0348464286228]\n",
      "Predicted: [1.4485145807266235 0.3365556299686432 2.8768293857574463\n",
      " 0.8245865106582642 0.014603376388549805 0.5198244452476501\n",
      " 0.03827669471502304]\n",
      "\n",
      "Image: seq_val/IMG_9909_0.jpg\n",
      "Actual   : [2.46634325965 0.857515045561 -0.294069615496 0.445175711623\n",
      " -0.0288491588102 -0.888425300765 -0.10810548912]\n",
      "Predicted: [2.8377342224121094 0.4709707796573639 0.013110429048538208\n",
      " 0.4160340130329132 -0.029272664338350296 -0.885575532913208\n",
      " -0.06954261660575867]\n",
      "\n",
      "Image: seq_val/IMG_9819_0.jpg\n",
      "Actual   : [0.306479802726 0.77798574385 0.762733359872 0.989005302274\n",
      " -0.0277820876771 -0.144539155805 0.0143213169176]\n",
      "Predicted: [0.13971343636512756 0.8006204962730408 0.548542857170105\n",
      " 0.9797670245170593 -0.008323208428919315 -0.16946253180503845\n",
      " 0.011044330894947052]\n",
      "\n",
      "Image: seq_val/IMG_9668_0.jpg\n",
      "Actual   : [5.10787061054 -1.83514801109 0.791888718525 0.945403089277\n",
      " -0.00174032817116 0.32280270639 0.0448149840075]\n",
      "Predicted: [4.967639446258545 -1.4184348583221436 0.7982809543609619\n",
      " 0.9288228750228882 -0.0012985095381736755 0.3350793719291687\n",
      " 0.037989478558301926]\n",
      "\n",
      "Image: seq_val/S_2_3_155_0.jpg\n",
      "Actual   : [-2.14480192487 0.0244443716376 -4.39846131858 0.937901648586\n",
      " -0.0266200734612 -0.345461673018 -0.0169735601619]\n",
      "Predicted: [-1.9337489604949951 -0.11509726941585541 -4.490578651428223\n",
      " 0.9371421933174133 -0.014679647982120514 -0.3446372151374817\n",
      " -0.018534956499934196]\n",
      "\n",
      "Image: seq_val/IMG_9839_0.jpg\n",
      "Actual   : [7.62142779032 0.384784508138 2.95582517528 0.852367643233 0.0218531400207\n",
      " 0.522307879523 0.0136499094334]\n",
      "Predicted: [6.5272979736328125 -0.10197123885154724 2.1782915592193604\n",
      " 0.8811302781105042 0.015348505228757858 0.4488653540611267\n",
      " 0.03367002680897713]\n",
      "\n",
      "Image: seq_val/S_2_3_102_0.jpg\n",
      "Actual   : [0.099467506936 -0.0545362781492 -3.83677840867 0.963918657305\n",
      " 0.264340583571 0.0311669555782 0.00367407892041]\n",
      "Predicted: [-0.4504716992378235 0.2377430498600006 -4.168401718139648\n",
      " 0.9530550837516785 0.2043173462152481 -0.0024974197149276733\n",
      " -0.007726310286670923]\n",
      "\n",
      "Image: seq_val/S_2_3_92_0.jpg\n",
      "Actual   : [-2.03577168375 0.0729909399858 -4.42136481572 0.791349977296\n",
      " 0.0406279348987 -0.600919464452 -0.104930365394]\n",
      "Predicted: [-1.7694709300994873 0.37109315395355225 -4.4203948974609375\n",
      " 0.7756282091140747 0.03335392102599144 -0.6175569295883179\n",
      " -0.08288601040840149]\n",
      "\n",
      "Image: seq_val/S_2_3_154_0.jpg\n",
      "Actual   : [-2.15445216625 0.0126184037989 -4.39215936383 0.951033427289\n",
      " -0.0517752798518 -0.304720631108 -0.000278480662118]\n",
      "Predicted: [-1.8926066160202026 -0.155975341796875 -4.548735618591309\n",
      " 0.9477450847625732 -0.024341417476534843 -0.3056097626686096\n",
      " -0.010027904063463211]\n",
      "\n",
      "Image: seq_val/IMG_9816_0.jpg\n",
      "Actual   : [-3.60514718884 0.915707871163 3.49784685956 0.812688619389\n",
      " -0.0450762031854 -0.5808845553 -0.00885873779807]\n",
      "Predicted: [-2.659917116165161 0.7320407629013062 2.168315887451172\n",
      " 0.7728134393692017 -0.03453291952610016 -0.6006065011024475\n",
      " -0.012126468122005463]\n",
      "\n",
      "Image: seq_val/IMG_9635_0.jpg\n",
      "Actual   : [-2.6193871859 -1.5229198058 4.76993278979 0.511493223783 0.0424724181481\n",
      " 0.856372231756 0.0565453481582]\n",
      "Predicted: [-2.634523868560791 -0.8748811483383179 4.782534122467041\n",
      " 0.5038946866989136 0.05357038602232933 0.8492182493209839\n",
      " 0.05875443294644356]\n",
      "\n",
      "Image: seq_val/S_2_3_34_0.jpg\n",
      "Actual   : [-1.00537406061 0.982177165241 -0.923731448715 0.291686801999\n",
      " -0.048460663893 -0.954733431994 -0.0324722562725]\n",
      "Predicted: [-0.9179880619049072 0.9649874567985535 -0.9669182896614075\n",
      " 0.2986275851726532 -0.050426289439201355 -0.9401130676269531\n",
      " -0.04238323122262955]\n",
      "\n",
      "Image: seq_val/IMG_8202_0.jpg\n",
      "Actual   : [-2.34310327025 0.839693999188 3.71286910353 0.627041136481 0.04959561954\n",
      " 0.77641988715 0.039138810941]\n",
      "Predicted: [-2.453277826309204 0.47214797139167786 3.761291027069092\n",
      " 0.6302789449691772 0.053901080042123795 0.7571020722389221\n",
      " 0.051513414829969406]\n",
      "\n",
      "Image: seq_val/IMG_9652_0.jpg\n",
      "Actual   : [0.406245294517 -1.54521924587 1.61015675129 0.969541825314\n",
      " -0.0012137770442 0.242982883324 0.0307651446085]\n",
      "Predicted: [0.93910813331604 -1.654654860496521 1.5354280471801758 0.973930835723877\n",
      " -0.005268770270049572 0.21883466839790344 0.040075477212667465]\n",
      "\n",
      "Image: seq_val/S_2_3_33_0.jpg\n",
      "Actual   : [-0.992933213356 0.990442412594 -0.906447044635 0.353642192649\n",
      " -0.0363349209873 -0.932613285039 -0.0620438044018]\n",
      "Predicted: [-0.9695897102355957 0.9557544589042664 -1.0316654443740845\n",
      " 0.36946308612823486 -0.04448482394218445 -0.9231184124946594\n",
      " -0.05927569046616554]\n",
      "\n",
      "Image: seq_val/IMG_9788_0.jpg\n",
      "Actual   : [-1.99359492294 0.835140120894 4.15377735231 0.540565695919\n",
      " 0.0508705137291 0.838847757318 0.0391836607785]\n",
      "Predicted: [-1.9248517751693726 0.4795393645763397 3.927502393722534\n",
      " 0.5464390516281128 0.051399823278188705 0.8234336972236633\n",
      " 0.04964064061641693]\n",
      "\n",
      "Image: seq_val/S_2_3_36_0.jpg\n",
      "Actual   : [-1.02478036954 0.974561474651 -0.914032993503 0.230638204169\n",
      " -0.0683135597973 -0.970554979917 0.0127399876033]\n",
      "Predicted: [-0.8057571053504944 0.9168305397033691 -0.793806254863739\n",
      " 0.24009756743907928 -0.053902897983789444 -0.9513824582099915\n",
      " -0.023059459403157234]\n",
      "\n",
      "Image: seq_val/IMG_9734_0.jpg\n",
      "Actual   : [-1.41817529832 -1.21144675285 -1.10388133486 0.527308261316\n",
      " -0.0260595267552 -0.843957022293 -0.094886474992]\n",
      "Predicted: [-1.1897886991500854 -0.49471408128738403 -1.7355149984359741\n",
      " 0.4715535342693329 -0.043008580803871155 -0.8933603763580322\n",
      " -0.0582570917904377]\n",
      "\n",
      "Image: seq_val/IMG_9659_0.jpg\n",
      "Actual   : [2.32037972511 -1.65393447771 1.03619246506 0.986405357554 -0.039777860981\n",
      " 0.157573686845 0.0243459560917]\n",
      "Predicted: [2.5883257389068604 -1.5076589584350586 0.6000791192054749\n",
      " 0.9830821752548218 -0.017644444480538368 0.15022161602973938\n",
      " 0.033015575259923935]\n",
      "\n",
      "Image: seq_val/IMG_9893_0.jpg\n",
      "Actual   : [4.3449721303 0.631575369784 -0.962234083696 0.198121563624\n",
      " -0.0270495703512 -0.979039868428 -0.038692412684]\n",
      "Predicted: [3.622577667236328 0.30169975757598877 -0.302304744720459\n",
      " 0.2066625952720642 -0.04551602154970169 -0.976333498954773\n",
      " -0.041259996592998505]\n",
      "\n",
      "Image: seq_val/IMG_9632_0.jpg\n",
      "Actual   : [-1.83623834646 -1.39890111944 1.55211262304 0.907714118174\n",
      " 0.00165721573465 0.417783183332 0.0388528638385]\n",
      "Predicted: [-2.0268197059631348 -0.8426801562309265 1.4655981063842773\n",
      " 0.9394960999488831 0.00461914949119091 0.3257486820220947\n",
      " 0.04070543125271797]\n",
      "\n",
      "Image: seq_val/IMG_8288_0.jpg\n",
      "Actual   : [-3.15768660397 0.969394947267 2.00098745752 0.921275569487\n",
      " -0.0457157233301 -0.38584434236 -0.0168979636723]\n",
      "Predicted: [-3.0789260864257812 0.5891821384429932 2.0694024562835693\n",
      " 0.928992748260498 -0.02640727162361145 -0.34137967228889465\n",
      " 0.001021557953208685]\n",
      "\n",
      "Image: seq_val/IMG_9836_0.jpg\n",
      "Actual   : [5.53211370482 0.494783260674 -0.519292858298 0.999374057029\n",
      " -0.0153938535616 0.0290881906968 0.0129769245631]\n",
      "Predicted: [5.1951680183410645 0.2465105950832367 -0.12065152823925018\n",
      " 0.9857609272003174 -0.005198322236537933 0.03878149390220642\n",
      " 0.025449542328715324]\n",
      "\n",
      "Image: seq_val/IMG_9670_0.jpg\n",
      "Actual   : [6.26616527826 -2.00712225905 2.62425391091 0.879140094113\n",
      " 0.00915665881943 0.474970667873 0.0378380124163]\n",
      "Predicted: [5.761867523193359 -1.8938915729522705 2.0156397819519043\n",
      " 0.8720791935920715 0.008062602952122688 0.4628666043281555\n",
      " 0.04515719413757324]\n",
      "\n",
      "Image: seq_val/IMG_9798_0.jpg\n",
      "Actual   : [-3.70567895416 1.00366577551 2.72420938708 0.994668118375 0.0166798164103\n",
      " -0.0975327038731 0.0290601047385]\n",
      "Predicted: [-3.3848929405212402 0.9011343121528625 2.597100257873535\n",
      " 0.9833981394767761 0.0009008795022964478 -0.08830603957176208\n",
      " 0.013821324333548546]\n",
      "\n",
      "Image: seq_val/IMG_9664_0.jpg\n",
      "Actual   : [0.106495607822 -1.48466895844 0.795174120604 0.936147441072\n",
      " -0.0264957623307 -0.350595000508 0.00301475889208]\n",
      "Predicted: [0.7293218374252319 -1.3449000120162964 0.5113149285316467\n",
      " 0.9256289005279541 -0.03388097882270813 -0.3487923741340637\n",
      " 0.006231959909200668]\n",
      "\n",
      "Image: seq_val/S_2_3_150_0.jpg\n",
      "Actual   : [-1.28926897909 -0.0120006706851 -4.66881377813 0.999843685937\n",
      " -0.00462395057165 0.0143510059842 0.00923425148823]\n",
      "Predicted: [-1.4922219514846802 0.005830898880958557 -4.726066589355469\n",
      " 0.9860218167304993 0.005384085699915886 0.008530378341674805\n",
      " 0.015129473060369492]\n",
      "\n",
      "Image: seq_val/S_2_3_95_0.jpg\n",
      "Actual   : [-1.12858866449 0.0349858121625 -4.59091245193 0.951273572373\n",
      " 0.142587770422 -0.264077114002 -0.0707855641401]\n",
      "Predicted: [-1.3494728803634644 0.28482958674430847 -4.355957508087158\n",
      " 0.9360522031784058 0.1240265816450119 -0.270060271024704\n",
      " -0.03504713624715805]\n",
      "\n",
      "Image: seq_val/IMG_9731_0.jpg\n",
      "Actual   : [-1.54805716055 -1.15822064569 -1.17524884373 0.0751121801755\n",
      " -0.0760952457679 -0.968691247946 -0.224064589157]\n",
      "Predicted: [-1.113160252571106 -0.2571045756340027 -1.3799982070922852\n",
      " 0.06446094810962677 -0.049421828240156174 -0.9756302833557129\n",
      " -0.1704501062631607]\n",
      "\n",
      "Image: seq_val/IMG_9646_0.jpg\n",
      "Actual   : [0.159888916073 -1.59021973805 2.71676764001 0.927028801894\n",
      " 0.0149935557215 0.373570813037 0.0289420350357]\n",
      "Predicted: [0.4438786804676056 -1.3750160932540894 2.004455804824829\n",
      " 0.9230442643165588 0.009436840191483498 0.344102680683136\n",
      " 0.04196837544441223]\n",
      "\n",
      "Image: seq_val/IMG_9907_0.jpg\n",
      "Actual   : [2.79894705074 0.983281443516 -3.00743825348 0.977108821745\n",
      " 0.0459685590029 -0.20735175785 -0.0122674597334]\n",
      "Predicted: [2.6525354385375977 0.4661962687969208 -2.693246841430664\n",
      " 0.9730108976364136 0.030964773148298264 -0.11556123942136765\n",
      " 0.007875693961977959]\n",
      "\n",
      "Image: seq_val/IMG_8196_0.jpg\n",
      "Actual   : [-1.72035597262 0.848425226177 1.92672632061 0.80741517541 0.0307953685605\n",
      " 0.586967465074 0.0510056343703]\n",
      "Predicted: [-1.6625138521194458 0.6886777877807617 2.0513691902160645\n",
      " 0.7914169430732727 0.03349709138274193 0.6037142276763916\n",
      " 0.0484393835067749]\n",
      "\n",
      "Image: seq_val/IMG_9871_0.jpg\n",
      "Actual   : [5.2746529142 0.708781696604 0.891221083021 0.741337250832\n",
      " -0.0195537874839 -0.668276791573 -0.0586758874527]\n",
      "Predicted: [5.379143238067627 0.5318156480789185 0.7015820741653442\n",
      " 0.6983118057250977 -0.030088499188423157 -0.7123740911483765\n",
      " -0.020495939999818802]\n",
      "\n",
      "Image: seq_val/S_2_3_64_0.jpg\n",
      "Actual   : [0.525945331484 0.881471362586 -1.11626138275 0.124533169466\n",
      " 0.0238320167128 0.99140882424 0.0321258136411]\n",
      "Predicted: [0.3507850468158722 0.727088987827301 -1.017269492149353\n",
      " 0.11840468645095825 0.031242750585079193 0.987329363822937\n",
      " 0.04467858001589775]\n",
      "\n",
      "Image: seq_val/IMG_9682_0.jpg\n",
      "Actual   : [4.83974887136 -1.74704471858 -0.194422019618 0.975523174016\n",
      " -0.0224335287008 -0.217583459032 0.0225546470253]\n",
      "Predicted: [4.562178611755371 -1.6858978271484375 -0.37985163927078247\n",
      " 0.9556127190589905 -0.021471941843628883 -0.2600127160549164\n",
      " 0.014788426458835602]\n",
      "\n",
      "Image: seq_val/IMG_9720_0.jpg\n",
      "Actual   : [-3.09514090217 -1.24421352964 0.555871649803 0.517498382473\n",
      " -0.0438556114487 -0.854498010053 -0.010259644116]\n",
      "Predicted: [-2.4402480125427246 -0.8703755736351013 0.5070152282714844\n",
      " 0.452647864818573 -0.04981169104576111 -0.8759644031524658\n",
      " -0.03859208524227142]\n",
      "\n",
      "Image: seq_val/IMG_9641_0.jpg\n",
      "Actual   : [-1.51692154072 -1.55950774562 3.96607969531 0.770567743336\n",
      " 0.0117618003051 0.633513562199 0.0689026813245]\n",
      "Predicted: [-0.7069300413131714 -1.496980905532837 3.6737451553344727\n",
      " 0.7616525888442993 0.02460324391722679 0.6249594688415527\n",
      " 0.05013652518391609]\n",
      "\n",
      "Image: seq_val/IMG_9890_0.jpg\n",
      "Actual   : [2.31791150431 0.860306436657 -1.97634675657 0.238068297147\n",
      " -0.0556200461325 -0.967588906252 -0.0632582394648]\n",
      "Predicted: [1.8073982000350952 0.8907999396324158 -0.812950849533081\n",
      " 0.24139009416103363 -0.04542064666748047 -0.9572762250900269\n",
      " -0.043393123894929886]\n",
      "\n",
      "Image: seq_val/IMG_9795_0.jpg\n",
      "Actual   : [-2.16396296429 0.842081180161 4.61415655719 0.556493689727\n",
      " 0.0562531361316 0.825886843023 0.0711426770022]\n",
      "Predicted: [-2.3196380138397217 0.3894765079021454 4.083113670349121\n",
      " 0.5498346090316772 0.05617265775799751 0.8183653950691223\n",
      " 0.05073606222867966]\n",
      "\n",
      "Image: seq_val/IMG_9956_0.jpg\n",
      "Actual   : [-0.64192768813 0.849330489008 1.13426250905 0.169413883122\n",
      " 0.0337471180213 0.983307130902 0.0571590285806]\n",
      "Predicted: [-0.30213069915771484 0.4148111045360565 1.1662520170211792\n",
      " 0.14181847870349884 0.041863493621349335 0.9813448190689087\n",
      " 0.06443020701408386]\n",
      "\n",
      "Image: seq_val/IMG_8282_0.jpg\n",
      "Actual   : [-0.361426838068 0.715628615644 3.52852342405 0.841685441172\n",
      " 0.0352727627439 0.5353165085 0.0612999678272]\n",
      "Predicted: [-0.09060154110193253 0.5571451783180237 3.4752838611602783\n",
      " 0.8308355808258057 0.025592945516109467 0.5343478322029114\n",
      " 0.043133754283189774]\n",
      "\n",
      "Image: seq_val/IMG_9927_0.jpg\n",
      "Actual   : [1.37919431249 0.691117698527 2.07798508897 0.26463785053 0.0384654048463\n",
      " 0.961847721386 0.0577596880332]\n",
      "Predicted: [1.110430359840393 0.43125006556510925 1.718247413635254\n",
      " 0.21333464980125427 0.035874031484127045 0.9521859884262085\n",
      " 0.05279238149523735]\n",
      "\n",
      "Image: seq_val/S_2_3_135_0.jpg\n",
      "Actual   : [-1.22727350421 -0.233574171135 -3.38096510101 0.961374659694\n",
      " -0.0439365840823 -0.271605016546 0.00768474233127]\n",
      "Predicted: [-0.9491279721260071 -0.36748433113098145 -3.213102102279663\n",
      " 0.9505586624145508 -0.0252455435693264 -0.2726893424987793\n",
      " -0.0024568536318838596]\n",
      "\n",
      "Image: seq_val/IMG_9764_0.jpg\n",
      "Actual   : [-0.767171786773 -1.33107389693 -0.485644224019 0.212958656252\n",
      " -0.03121591852 -0.975372631864 -0.0481913495217]\n",
      "Predicted: [-0.3751187026500702 -1.561453104019165 -0.12632247805595398\n",
      " 0.23651547729969025 -0.0484437458217144 -0.9609506726264954\n",
      " -0.08698827028274536]\n",
      "\n",
      "Image: seq_val/S_2_3_40_0.jpg\n",
      "Actual   : [-1.01182431163 1.02349238658 -0.893572323865 0.35189302611\n",
      " -0.0417198641072 -0.924563552523 -0.140046379675]\n",
      "Predicted: [-0.7691964507102966 0.893390953540802 -1.2264186143875122\n",
      " 0.32702845335006714 -0.04098416864871979 -0.9162492752075195\n",
      " -0.11848094314336777]\n",
      "\n",
      "Image: seq_val/S_2_3_162_0.jpg\n",
      "Actual   : [-1.27533848554 -0.467054734686 -3.06566924606 0.983294504266\n",
      " -0.00687073988602 0.181806220152 0.00558651306122]\n",
      "Predicted: [-1.0482810735702515 -0.5817309021949768 -2.6881206035614014\n",
      " 0.9752177596092224 -0.0033823512494564056 0.13737338781356812\n",
      " 0.023917177692055702]\n",
      "\n",
      "Image: seq_val/IMG_9840_0.jpg\n",
      "Actual   : [3.32517796458 0.583691535383 1.4984604824 0.880396527752 -0.0342641041902\n",
      " -0.472810188176 0.01335855696]\n",
      "Predicted: [3.2061281204223633 0.378608763217926 1.813429355621338 0.8662238717079163\n",
      " -0.03430125489830971 -0.46509572863578796 0.001597076072357595]\n",
      "\n",
      "Image: seq_val/S_2_3_106_0.jpg\n",
      "Actual   : [0.00119382966681 -0.139010614774 -4.10794555713 0.676535529903\n",
      " 0.0444160582635 0.731538011337 0.0719654675265]\n",
      "Predicted: [-0.3892991244792938 -0.5210927128791809 -3.4200422763824463\n",
      " 0.7145261764526367 0.042721983045339584 0.7082706093788147\n",
      " 0.07010332494974136]\n",
      "\n",
      "Image: seq_val/IMG_9856_0.jpg\n",
      "Actual   : [2.6621646681 0.755541229042 -1.51263421071 0.834909241151\n",
      " -0.0411912556938 -0.548838832714 -0.00240316466473]\n",
      "Predicted: [2.6129002571105957 0.5551052689552307 -1.2636767625808716\n",
      " 0.8260994553565979 -0.02686622366309166 -0.5565745234489441\n",
      " -0.013532405719161034]\n",
      "\n",
      "Image: seq_val/S_2_3_165_0.jpg\n",
      "Actual   : [-1.22856582529 -0.469966889279 -3.0480898713 0.99884191822\n",
      " -0.00731832875725 -0.0475051377234 -0.00212752473013]\n",
      "Predicted: [-1.0297927856445312 -0.1776164025068283 -3.0608038902282715\n",
      " 0.9847930669784546 -0.012674948200583458 -0.07962185889482498\n",
      " 0.011545535176992416]\n",
      "\n",
      "Image: seq_val/IMG_9742_0.jpg\n",
      "Actual   : [-3.17828911868 -1.33360837879 1.95471287041 0.394243319508\n",
      " -0.0480186666571 -0.917426116856 -0.0244076378622]\n",
      "Predicted: [-2.19216251373291 -1.0610705614089966 1.5920517444610596\n",
      " 0.364888072013855 -0.05306859686970711 -0.9177110195159912\n",
      " -0.0385785736143589]\n",
      "\n",
      "Image: seq_val/IMG_8297_0.jpg\n",
      "Actual   : [0.700479961444 0.722211641162 2.03087811698 0.923295184913\n",
      " 0.0326195879971 0.381417271667 0.0313501015963]\n",
      "Predicted: [1.0911657810211182 0.7090800404548645 2.1053740978240967\n",
      " 0.8929682970046997 0.016188979148864746 0.4048854112625122\n",
      " 0.03731235861778259]\n",
      "\n",
      "Image: seq_val/IMG_9809_0.jpg\n",
      "Actual   : [-0.444413448225 0.832738970641 0.893308459103 0.983764357129\n",
      " -0.0132984041841 0.177603912338 0.022083758999]\n",
      "Predicted: [-0.5851669907569885 0.8452860116958618 1.2669947147369385\n",
      " 0.9730687141418457 0.004934219643473625 0.217698872089386\n",
      " 0.028307711705565453]\n",
      "\n",
      "Image: seq_val/S_2_3_204_0.jpg\n",
      "Actual   : [-0.785745457143 -1.34208397227 0.075701551173 0.140387251029\n",
      " 0.0586746564546 0.984305904917 0.0893901000331]\n",
      "Predicted: [-0.40712377429008484 -0.520187258720398 -0.16417813301086426\n",
      " 0.16948877274990082 0.046637337654829025 0.9621546268463135\n",
      " 0.09149850904941559]\n",
      "\n",
      "Image: seq_val/S_2_3_129_0.jpg\n",
      "Actual   : [-1.06684045689 -0.0394298960664 -4.590242722 0.897301871699\n",
      " -0.0149121356492 -0.440896598645 -0.0154002779104]\n",
      "Predicted: [-1.2753738164901733 0.28355634212493896 -4.225911617279053\n",
      " 0.8839787244796753 -0.002271164208650589 -0.4411284625530243\n",
      " -0.01783716306090355]\n",
      "\n",
      "Image: seq_val/S_2_3_65_0.jpg\n",
      "Actual   : [0.533791781663 0.866086399562 -1.10948990581 0.0996872579829\n",
      " 0.0158712030725 0.994777876822 -0.0150841404639]\n",
      "Predicted: [0.34371882677078247 0.7241970896720886 -0.7673420310020447\n",
      " 0.07835379242897034 0.03185056149959564 0.9798285961151123\n",
      " 0.052443910390138626]\n",
      "\n",
      "Image: seq_val/IMG_9902_0.jpg\n",
      "Actual   : [4.88281622477 0.686220103031 -0.00836274396068 0.306024557942\n",
      " 0.0477399464783 0.947451820356 0.0800307162907]\n",
      "Predicted: [4.206762790679932 0.3966118097305298 -0.20974275469779968\n",
      " 0.27063244581222534 0.04174255579710007 0.9347479343414307\n",
      " 0.0701836496591568]\n",
      "\n",
      "Image: seq_val/IMG_9799_0.jpg\n",
      "Actual   : [-3.05732780455 0.947988409006 2.37333729426 0.99919097048 0.0157929897747\n",
      " 0.0352667326719 0.0111464591398]\n",
      "Predicted: [-3.0135085582733154 0.8316596746444702 2.2101194858551025\n",
      " 0.9866353273391724 0.007492441684007645 0.02546190470457077\n",
      " 0.019661523401737213]\n",
      "\n",
      "Image: seq_val/S_2_3_57_0.jpg\n",
      "Actual   : [0.437119671345 0.89900623731 -1.04815655459 0.570810485908\n",
      " 0.0373756877382 0.818337375431 0.0556990764399]\n",
      "Predicted: [0.5894566178321838 0.9047293663024902 -0.4882161617279053\n",
      " 0.5234965682029724 0.03999349847435951 0.8356308937072754\n",
      " 0.057571083307266235]\n",
      "\n",
      "Image: seq_val/IMG_9883_0.jpg\n",
      "Actual   : [6.02381413018 0.642470489196 -2.69237405243 0.494280279429\n",
      " 0.0308133677828 0.866003649655 0.0691029703954]\n",
      "Predicted: [5.54150915145874 0.7481169104576111 -2.195263624191284 0.4950800836086273\n",
      " 0.03897586464881897 0.8608716130256653 0.05637357756495476]\n",
      "\n",
      "Image: seq_val/IMG_9808_0.jpg\n",
      "Actual   : [-1.28215971156 0.913933690037 0.740407152783 0.998287889304\n",
      " -0.01253742257 0.0569732718963 0.00426021044581]\n",
      "Predicted: [-1.868320107460022 0.8640255331993103 1.039716124534607\n",
      " 0.9923747181892395 0.003628768026828766 0.04104005545377731\n",
      " 0.019758667796850204]\n",
      "\n",
      "Image: seq_val/IMG_9941_0.jpg\n",
      "Actual   : [-1.429349365 0.998649795764 -0.778509395021 0.271692168543\n",
      " -0.0466568838854 -0.960739778437 -0.0313939304229]\n",
      "Predicted: [-1.1139143705368042 0.7642022967338562 -0.5957896709442139\n",
      " 0.28876176476478577 -0.04646451026201248 -0.9448286294937134\n",
      " -0.06081005558371544]\n",
      "\n",
      "Image: seq_val/IMG_9707_0.jpg\n",
      "Actual   : [4.51037519345 -1.819479356 1.85493786586 0.0827117257537 0.0338743283008\n",
      " 0.995339984973 0.0361885979126]\n",
      "Predicted: [4.2301530838012695 -1.3543639183044434 1.8748801946640015\n",
      " 0.06206601858139038 0.02455778792500496 1.0092889070510864\n",
      " 0.031844861805438995]\n",
      "\n",
      "Image: seq_val/S_2_3_197_0.jpg\n",
      "Actual   : [-0.871329445153 -1.39953076124 0.808459167389 0.210147985275\n",
      " -0.0622712362622 -0.973865010922 -0.0595571819457]\n",
      "Predicted: [-0.7491941452026367 -1.1596434116363525 0.9995770454406738\n",
      " 0.24955494701862335 -0.050289738923311234 -0.9568183422088623\n",
      " -0.05413210019469261]\n",
      "\n",
      "Image: seq_val/IMG_9905_0.jpg\n",
      "Actual   : [5.28231426781 0.769158588398 -2.66762187215 0.893121451023\n",
      " 0.0261048617698 0.446841262377 0.0445589065387]\n",
      "Predicted: [4.798161506652832 0.5480534434318542 -1.6286977529525757\n",
      " 0.8611685037612915 0.035874538123607635 0.4789190888404846\n",
      " 0.04726039990782738]\n",
      "\n",
      "Image: seq_val/IMG_9754_0.jpg\n",
      "Actual   : [-0.0238769081022 -1.47053456076 1.0129285457 0.248123295209\n",
      " -0.0455957149458 -0.966202723096 -0.05299206578]\n",
      "Predicted: [-0.15932351350784302 -0.9314593076705933 1.2515522241592407\n",
      " 0.26349854469299316 -0.05143588408827782 -0.9610628485679626\n",
      " -0.044154901057481766]\n",
      "\n",
      "Image: seq_val/IMG_9825_0.jpg\n",
      "Actual   : [0.331172344787 0.779001481493 1.00874884957 0.90883884336\n",
      " -0.0373752005523 -0.415403173472 0.00743334738796]\n",
      "Predicted: [1.0422885417938232 0.7601261138916016 0.9015778303146362\n",
      " 0.897330105304718 -0.031438037753105164 -0.3919075131416321\n",
      " 0.0013539770152419806]\n",
      "\n",
      "Image: seq_val/IMG_9837_0.jpg\n",
      "Actual   : [6.62050550797 0.570104846934 -0.00953546203212 0.992099227785\n",
      " -0.0255828082583 0.112560858604 0.0491395488328]\n",
      "Predicted: [6.326263427734375 0.15268124639987946 0.19596882164478302\n",
      " 0.9599943161010742 0.006486544385552406 0.16036173701286316\n",
      " 0.028300942853093147]\n",
      "\n",
      "Image: seq_val/S_2_3_151_0.jpg\n",
      "Actual   : [-1.2855565531 -0.0263856951382 -4.66424682715 0.998908531774\n",
      " -0.0409768084834 -0.0149392640094 0.0167171979551]\n",
      "Predicted: [-1.3581122159957886 -0.09854929149150848 -4.211632251739502\n",
      " 0.9868659973144531 -0.01491384208202362 -0.048784688115119934\n",
      " 0.012449871748685837]\n",
      "\n",
      "Image: seq_val/IMG_9619_0.jpg\n",
      "Actual   : [-3.3520612971 -1.2633415525 1.16932968104 0.258374944984 0.037224340215\n",
      " 0.964632382294 0.0366210776851]\n",
      "Predicted: [-2.925170421600342 -0.5440655946731567 1.898754358291626\n",
      " 0.25577759742736816 0.044691164046525955 0.9371415376663208\n",
      " 0.057686418294906616]\n",
      "\n",
      "Image: seq_val/S_2_3_10_0.jpg\n",
      "Actual   : [-1.3627734399 1.0281360464 -1.41573724876 0.719526257831\n",
      " -0.00414422268796 -0.69255304066 -0.051332987284]\n",
      "Predicted: [-1.275441288948059 0.3587624132633209 -1.3070957660675049\n",
      " 0.6883235573768616 -0.02769218385219574 -0.702621579170227\n",
      " -0.03269803151488304]\n",
      "\n",
      "Image: seq_val/IMG_9823_0.jpg\n",
      "Actual   : [5.41474752634 0.402061272379 1.88649087944 0.91318826344 0.0105693729284\n",
      " 0.406261913763 0.0304424259485]\n",
      "Predicted: [5.71417760848999 0.21036288142204285 2.0566141605377197 0.897365152835846\n",
      " 0.009646456688642502 0.42091959714889526 0.03193163499236107]\n",
      "\n",
      "Image: seq_val/IMG_9681_0.jpg\n",
      "Actual   : [4.2425105946 -1.73404761827 0.163754338195 0.955388191571\n",
      " -0.0265676387712 -0.293165005469 0.0241214332934]\n",
      "Predicted: [4.5534772872924805 -1.6424994468688965 -0.040803030133247375\n",
      " 0.9179750084877014 -0.02716730907559395 -0.36486417055130005\n",
      " 0.008261770009994507]\n",
      "\n",
      "Image: seq_val/IMG_9643_0.jpg\n",
      "Actual   : [-3.38797445547 -1.29162964478 2.10305081776 0.998267872125\n",
      " 0.0131595827379 -0.053267418712 0.021228824002]\n",
      "Predicted: [-2.5427420139312744 -0.951530933380127 2.052027940750122\n",
      " 0.9888219237327576 8.076056838035583e-05 -0.031009849160909653\n",
      " 0.0255790613591671]\n",
      "\n",
      "Image: seq_val/IMG_8204_0.jpg\n",
      "Actual   : [-2.34206611548 0.815785033135 4.5176817376 0.595428457185 0.0709424798389\n",
      " 0.798391953605 0.0547942090718]\n",
      "Predicted: [-2.7833287715911865 0.21757525205612183 4.438510894775391\n",
      " 0.6015136241912842 0.062211934477090836 0.7875974178314209\n",
      " 0.05359327420592308]\n",
      "\n",
      "Image: seq_val/IMG_9729_0.jpg\n",
      "Actual   : [-0.736656422885 -1.31660203455 -0.474292065955 0.441509191042\n",
      " -0.0432944373507 -0.894866682879 -0.0490799938161]\n",
      "Predicted: [-0.4599422812461853 -0.8302602767944336 -0.056480880826711655\n",
      " 0.45625925064086914 -0.04389078915119171 -0.8755537867546082\n",
      " -0.04146596044301987]\n",
      "\n",
      "Image: seq_val/IMG_9895_0.jpg\n",
      "Actual   : [5.09373810253 0.810181929597 -2.70567613887 0.594920396137\n",
      " 0.0317161195722 0.801852426205 0.0457875158534]\n",
      "Predicted: [4.822076320648193 0.5863307118415833 -2.0844061374664307\n",
      " 0.559416651725769 0.034400153905153275 0.8111322522163391\n",
      " 0.05735882744193077]\n",
      "\n",
      "Image: seq_val/S_2_3_158_0.jpg\n",
      "Actual   : [-2.09739305156 0.0838082894904 -4.40237221453 0.892425065336\n",
      " 0.0978428841417 -0.429307253974 -0.0984863161413]\n",
      "Predicted: [-2.046602725982666 0.17650146782398224 -4.602628707885742\n",
      " 0.8899256587028503 0.12108065187931061 -0.43663668632507324\n",
      " -0.10746567696332932]\n",
      "\n",
      "Image: seq_val/S_2_3_128_0.jpg\n",
      "Actual   : [-1.0810062905 -0.0352860324711 -4.59743116078 0.942844007877\n",
      " -0.0182207352835 -0.332519204719 -0.0120066693839]\n",
      "Predicted: [-1.273496389389038 -0.21749641001224518 -4.1465582847595215\n",
      " 0.9390901923179626 -0.015420818701386452 -0.3152192234992981\n",
      " -0.010200204327702522]\n",
      "\n",
      "Image: seq_val/IMG_9831_0.jpg\n",
      "Actual   : [7.0817166429 0.30453586864 2.57717135913 0.870299161157 0.00744456779217\n",
      " 0.491342677427 0.033261416676]\n",
      "Predicted: [7.248482704162598 -0.09976189583539963 2.533701181411743\n",
      " 0.8833983540534973 0.008192498236894608 0.45914995670318604\n",
      " 0.03173445165157318]\n",
      "\n",
      "Image: seq_val/IMG_9656_0.jpg\n",
      "Actual   : [-1.28294314161 -1.44611212697 1.66306087627 0.95101328985\n",
      " -0.0396452589344 -0.305986887224 0.0193391007931]\n",
      "Predicted: [-1.4036442041397095 -0.8264206647872925 0.7159560918807983\n",
      " 0.9281890392303467 -0.037880778312683105 -0.3363562822341919\n",
      " 0.006237180903553963]\n",
      "\n",
      "Image: seq_val/IMG_9861_0.jpg\n",
      "Actual   : [4.39171564728 0.383996572603 3.89267878843 0.570481213128\n",
      " -0.0321760327864 -0.820565653028 -0.0137075691862]\n",
      "Predicted: [3.917921543121338 0.04882632568478584 3.4523355960845947\n",
      " 0.5595424175262451 -0.04010801762342453 -0.8123215436935425\n",
      " -0.01705591194331646]\n",
      "\n",
      "Image: seq_val/IMG_8195_0.jpg\n",
      "Actual   : [-1.88236030674 0.898043358009 1.45316948643 0.893472871829\n",
      " 0.0121845329063 0.445811244952 0.0530103606752]\n",
      "Predicted: [-1.9626730680465698 0.8127298951148987 1.778831958770752\n",
      " 0.8609600067138672 0.026460301131010056 0.5078769326210022\n",
      " 0.04507318511605263]\n",
      "\n",
      "Image: seq_val/S_2_3_199_0.jpg\n",
      "Actual   : [-0.150361621762 -1.43774186684 0.691126993863 0.0818027757244\n",
      " -0.0518348753783 -0.993645686905 -0.057355910538]\n",
      "Predicted: [0.07287377864122391 -1.1322742700576782 0.8037773370742798\n",
      " 0.08887287974357605 -0.053933195769786835 -0.9856630563735962\n",
      " -0.0804375559091568]\n",
      "\n",
      "Image: seq_val/S_2_3_39_0.jpg\n",
      "Actual   : [-1.05055234083 0.961200474348 -0.903408115523 0.140454672802\n",
      " -0.0821290571816 -0.983874089163 0.0742905076649]\n",
      "Predicted: [-0.7807771563529968 0.6556691527366638 -1.1363142728805542\n",
      " 0.16445192694664001 -0.05965498834848404 -0.9571529626846313\n",
      " 0.00419418141245842]\n",
      "\n",
      "Image: seq_val/IMG_9737_0.jpg\n",
      "Actual   : [-1.5142230094 -1.22906986864 -1.16350728959 0.223881608127\n",
      " -0.0537676405279 -0.972001070901 -0.0469039928215]\n",
      "Predicted: [-1.1262027025222778 -0.6974239945411682 -1.6312435865402222\n",
      " 0.2176206409931183 -0.054112572222948074 -0.9670987725257874\n",
      " -0.05422798544168472]\n",
      "\n",
      "Image: seq_val/S_2_3_138_0.jpg\n",
      "Actual   : [-1.20497215921 -0.228833175483 -3.43728668018 0.813667486688\n",
      " -0.0350923562387 -0.580030033615 -0.016700531257]\n",
      "Predicted: [-1.4333308935165405 -0.013343214988708496 -3.8881280422210693\n",
      " 0.860854983329773 -0.01989102177321911 -0.5103110671043396\n",
      " -0.023973390460014343]\n",
      "\n",
      "Image: seq_val/IMG_9848_0.jpg\n",
      "Actual   : [4.67138627382 0.455454542763 2.05718299113 0.809795595704\n",
      " -0.0454745092824 -0.58494162612 -0.00254090842014]\n",
      "Predicted: [4.703145503997803 0.37093406915664673 1.9702911376953125\n",
      " 0.7772679924964905 -0.0357130765914917 -0.6153934001922607\n",
      " -0.002998040057718754]\n",
      "\n",
      "Image: seq_val/IMG_9817_0.jpg\n",
      "Actual   : [-2.81400679716 0.914911391604 2.31120240098 0.874458971369\n",
      " -0.0638672370888 -0.480539240088 -0.0180144984655]\n",
      "Predicted: [-2.522014856338501 0.7885941863059998 2.1909782886505127\n",
      " 0.8591805696487427 -0.029904048889875412 -0.46589499711990356\n",
      " -0.0037462222389876842]\n",
      "\n",
      "Image: seq_val/S_2_3_2_0.jpg\n",
      "Actual   : [0.564623119194 0.758805430545 1.25171198338 0.095302178737\n",
      " 0.0504814602942 0.992488312388 0.0577586934323]\n",
      "Predicted: [0.906965970993042 0.4086313843727112 1.114506483078003\n",
      " 0.13248693943023682 0.03958557918667793 0.9259706735610962\n",
      " 0.06404654681682587]\n",
      "\n",
      "Image: seq_val/IMG_9864_0.jpg\n",
      "Actual   : [4.68532360616 0.76279853444 -1.26301372072 0.801495260629\n",
      " -0.0422250354498 -0.596340002322 -0.01417727766]\n",
      "Predicted: [4.727702617645264 0.4274345934391022 -0.3495675325393677\n",
      " 0.7801454067230225 -0.02812773920595646 -0.6178920865058899\n",
      " -0.013533670455217361]\n",
      "\n",
      "Image: seq_val/IMG_8292_0.jpg\n",
      "Actual   : [-1.10323250861 0.89051869373 0.896678157006 0.999224898984\n",
      " -0.0103343858267 0.0378311849494 0.00340634178332]\n",
      "Predicted: [-0.8280096054077148 0.6258910894393921 0.6163938045501709\n",
      " 0.9840664863586426 -0.0026034843176603317 0.07073912769556046\n",
      " 0.023275917395949364]\n",
      "\n",
      "Image: seq_val/S_2_3_189_0.jpg\n",
      "Actual   : [-0.138437482823 -1.45080701123 0.959970434043 0.20540980812\n",
      " -0.0343543421113 -0.975723591501 -0.0677500029101]\n",
      "Predicted: [0.30260169506073 -1.2586824893951416 0.8510804176330566\n",
      " 0.17964449524879456 -0.05113432556390762 -0.9687132835388184\n",
      " -0.05791207775473595]\n",
      "\n",
      "Image: seq_val/IMG_9779_0.jpg\n",
      "Actual   : [-2.27157825513 1.0487378586 -0.568836266525 0.86320842029 0.0157631778166\n",
      " 0.503165540802 0.0380418705495]\n",
      "Predicted: [-2.040248155593872 0.3495924174785614 -0.22047017514705658\n",
      " 0.8845593929290771 0.01918594166636467 0.45685917139053345\n",
      " 0.04221706837415695]\n",
      "\n",
      "Image: seq_val/IMG_9900_0.jpg\n",
      "Actual   : [3.03087608472 0.784470566151 0.407216356818 0.195882120961\n",
      " -0.0266459913317 -0.975543863008 -0.0960955627542]\n",
      "Predicted: [2.690532922744751 0.630352795124054 0.20338033139705658\n",
      " 0.2034502923488617 -0.030573619529604912 -0.9383453726768494\n",
      " -0.09149665385484695]\n",
      "\n",
      "Image: seq_val/IMG_9868_0.jpg\n",
      "Actual   : [4.64371847418 0.677961095732 1.01713600264 0.549852041919\n",
      " -0.0398684926094 -0.834000667086 -0.0227183316993]\n",
      "Predicted: [4.704691410064697 0.6589458584785461 1.629758358001709 0.5071465969085693\n",
      " -0.03519104793667793 -0.8566145300865173 -0.029886409640312195]\n",
      "\n",
      "Image: seq_val/S_2_3_176_0.jpg\n",
      "Actual   : [-0.144783400137 -1.32128931921 -0.71335092501 0.151797885726\n",
      " 0.0606858468404 0.981186671006 0.102700275186]\n",
      "Predicted: [-0.17010259628295898 -0.28220134973526 -0.9371474981307983\n",
      " 0.08215287327766418 0.05556781217455864 0.9868539571762085\n",
      " 0.11524365097284317]\n",
      "\n",
      "Image: seq_val/IMG_9692_0.jpg\n",
      "Actual   : [5.71452569339 -1.71066188816 -1.62049660782 0.952414072609\n",
      " 0.00163858871824 -0.304709508896 0.00754085610806]\n",
      "Predicted: [4.868074893951416 -1.3965436220169067 -1.4777333736419678\n",
      " 0.9345269799232483 -0.018298663198947906 -0.33338797092437744\n",
      " 0.0057117631658911705]\n",
      "\n",
      "Image: seq_val/S_2_3_29_0.jpg\n",
      "Actual   : [-0.234417815404 0.776195805612 -2.34172688976 0.975026968502\n",
      " 0.104166350831 -0.195201882356 -0.0191835130633]\n",
      "Predicted: [0.08101796358823776 0.6674933433532715 -2.089460611343384\n",
      " 0.957038938999176 0.07205985486507416 -0.23425495624542236\n",
      " -0.008419236168265343]\n",
      "\n",
      "Image: seq_val/IMG_9653_0.jpg\n",
      "Actual   : [1.3066814656 -1.65598627862 2.43657906312 0.921725362903 0.0124028659725\n",
      " 0.385668236947 0.0390964871402]\n",
      "Predicted: [1.4900822639465332 -1.6057857275009155 2.6779820919036865\n",
      " 0.9025861620903015 0.003543497994542122 0.3882129490375519\n",
      " 0.04502379149198532]\n",
      "\n",
      "Image: seq_val/IMG_9651_0.jpg\n",
      "Actual   : [-1.21944738621 -1.408441244 1.18381771623 0.998642996142\n",
      " -0.00325626388132 0.0444929977083 0.0268688696469]\n",
      "Predicted: [-0.8882649540901184 -1.2846795320510864 1.254404067993164\n",
      " 0.9914457201957703 -0.014610807411372662 0.062294892966747284\n",
      " 0.030934032052755356]\n",
      "\n",
      "Image: seq_val/S_2_3_191_0.jpg\n",
      "Actual   : [-0.213412821595 -1.4377104645 0.95149515996 0.00378776897719\n",
      " -0.0443272592578 -0.99541952119 -0.0846210595899]\n",
      "Predicted: [-0.41711145639419556 -0.8356541991233826 0.5632315278053284\n",
      " 0.041301652789115906 -0.0560629740357399 -0.9847550988197327\n",
      " -0.0617031492292881]\n",
      "\n",
      "Image: seq_val/IMG_9642_0.jpg\n",
      "Actual   : [-0.967120454223 -1.59519765631 4.22070593983 0.738317415418\n",
      " 0.0130261584388 0.672736082831 0.0463020101483]\n",
      "Predicted: [-0.8388272523880005 -1.426566481590271 3.795328378677368\n",
      " 0.7341390252113342 0.026365026831626892 0.6723136901855469\n",
      " 0.05082481727004051]\n",
      "\n",
      "Image: seq_val/IMG_9645_0.jpg\n",
      "Actual   : [-0.957662133165 -1.50197372327 2.48715004338 0.958611166791\n",
      " 0.0101265075997 0.282332571358 0.0353610505819]\n",
      "Predicted: [-0.4191095530986786 -1.4894007444381714 1.6635921001434326\n",
      " 0.9609407186508179 0.004934653639793396 0.25748831033706665\n",
      " 0.04134567826986313]\n",
      "\n",
      "Image: seq_val/S_2_3_7_0.jpg\n",
      "Actual   : [-0.358566121085 0.905803229342 -0.273689626877 0.167122750262\n",
      " -0.0561177854141 -0.979405082364 -0.098419841218]\n",
      "Predicted: [-0.3314596116542816 0.9577111005783081 -0.5775937438011169\n",
      " 0.15892797708511353 -0.05337205156683922 -0.9627248048782349\n",
      " -0.05330120772123337]\n",
      "\n",
      "Image: seq_val/S_2_3_203_0.jpg\n",
      "Actual   : [-0.721330259464 -1.3468953785 0.0665151538486 0.0153076367448\n",
      " -0.0566908269097 -0.994722791413 -0.0841331958595]\n",
      "Predicted: [-0.6079732179641724 -0.7651161551475525 -0.13431984186172485\n",
      " 0.07122421264648438 -0.055757224559783936 -0.9755063652992249\n",
      " -0.05955056473612785]\n",
      "\n",
      "Image: seq_val/S_2_3_28_0.jpg\n",
      "Actual   : [-0.261267997575 0.774128184108 -2.33872599172 0.990079067887\n",
      " 0.103495037998 -0.0949744124699 -0.00347525790666]\n",
      "Predicted: [-0.41981086134910583 0.6544846892356873 -2.495757818222046\n",
      " 0.964326024055481 0.10874077677726746 -0.16601695120334625\n",
      " -0.00729964068159461]\n",
      "\n",
      "Image: seq_val/S_2_3_63_0.jpg\n",
      "Actual   : [0.513764613693 0.88643428185 -1.11025892352 0.187375608765\n",
      " 0.0274937654815 0.981067813825 0.0405020835991]\n",
      "Predicted: [0.4905126392841339 0.6928841471672058 -0.9437458515167236\n",
      " 0.19281598925590515 0.02924945577979088 0.9601799249649048\n",
      " 0.03610554710030556]\n",
      "\n",
      "Image: seq_val/IMG_9787_0.jpg\n",
      "Actual   : [-1.16741835335 0.815236109335 1.73098486835 0.802931202098\n",
      " 0.0267371019395 0.592624599476 0.0581609505931]\n",
      "Predicted: [-0.711296021938324 0.8734337687492371 1.9269356727600098\n",
      " 0.7874526977539062 0.0247260220348835 0.5894320011138916\n",
      " 0.04604190215468407]\n",
      "\n",
      "Image: seq_val/S_2_3_87_0.jpg\n",
      "Actual   : [-1.97109226628 -0.0276826646549 -3.94739225824 0.614056934404\n",
      " -0.049161718325 -0.787125149974 -0.0308416121405]\n",
      "Predicted: [-1.6740549802780151 -0.19364556670188904 -3.457775831222534\n",
      " 0.5502286553382874 -0.042512230575084686 -0.8175272941589355\n",
      " -0.042455948889255524]\n",
      "\n",
      "Image: seq_val/S_2_3_46_0.jpg\n",
      "Actual   : [-0.1789617325 0.951079188053 -0.902049545127 0.0413481257157\n",
      " -0.0621995458339 -0.990296454796 -0.117194200442]\n",
      "Predicted: [-0.1603676676750183 0.6339337229728699 -0.9325177669525146\n",
      " 0.022082924842834473 -0.05990132689476013 -0.977298378944397\n",
      " -0.1052355244755745]\n",
      "\n",
      "Image: seq_val/IMG_9872_0.jpg\n",
      "Actual   : [5.02530261219 0.507127785107 2.32863874484 0.605983974795\n",
      " -0.0125622653313 -0.793926456672 -0.0480249224702]\n",
      "Predicted: [4.8326416015625 0.3375491797924042 2.0286543369293213 0.6104970574378967\n",
      " -0.0354081392288208 -0.7862399816513062 -0.022066347301006317]\n",
      "\n",
      "Image: seq_val/IMG_9727_0.jpg\n",
      "Actual   : [-0.314041526118 -1.4269029485 0.969161868435 0.231171346524\n",
      " -0.0405175297583 -0.968505254478 -0.0831607502165]\n",
      "Predicted: [-0.21038278937339783 -1.0076258182525635 0.9770764112472534\n",
      " 0.2093830108642578 -0.048196353018283844 -0.966857373714447\n",
      " -0.06464571505784988]\n",
      "\n",
      "Image: seq_val/S_2_3_96_0.jpg\n",
      "Actual   : [-0.823136205819 0.0111189064912 -4.63309163183 0.97916547482\n",
      " 0.158408007212 -0.122986289351 -0.0318786574842]\n",
      "Predicted: [-0.9239422082901001 0.21504327654838562 -4.138026714324951\n",
      " 0.9605308175086975 0.1357286274433136 -0.14972741901874542\n",
      " -0.013886157423257828]\n",
      "\n",
      "Image: seq_val/IMG_9762_0.jpg\n",
      "Actual   : [-0.716276793129 -1.32436776418 -0.489090268344 0.47654761282\n",
      " -0.0322215819558 -0.877283603409 -0.0473035047412]\n",
      "Predicted: [-0.7877495884895325 -0.9259090423583984 -0.5782908201217651\n",
      " 0.4674786329269409 -0.04575974494218826 -0.8806228041648865\n",
      " -0.04028582200407982]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# test_df = pd.read_csv('./csbf3nvm/NVM23_test_stair.txt', delimiter=' ', skiprows=1, names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "# test_df[['X', 'Y', 'Z', 'W', 'P', 'Q', 'R']] = test_df[['X', 'Y', 'Z', 'W', 'P', 'Q', 'R']] \n",
    "test_df = pd.read_csv('csbf3nvm/NVM23_test.txt', delimiter=' ',\n",
    "                   names=['ImageFile', 'X', 'Y', 'Z', 'W', 'P', 'Q', 'R'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory='./',\n",
    "    x_col='ImageFile',\n",
    "    y_col=['X', 'Y', 'Z', 'W', 'P', 'Q', 'R'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(test_generator, steps=len(test_df))\n",
    "\n",
    "# Combine actual and predicted values\n",
    "results = test_df.copy()\n",
    "results[['Pred_X', 'Pred_Y', 'Pred_Z', 'Pred_W', 'Pred_P', 'Pred_Q', 'Pred_R']] = predictions\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv('./DeepNav_test_score.csv', index=False)\n",
    "\n",
    "# Print Mean Squared Error\n",
    "mse = np.mean((test_df[['X', 'Y', 'Z', 'W', 'P', 'Q', 'R']].values - predictions)**2)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Print example outputs for verification\n",
    "for i, row in results.iterrows():\n",
    "    print(f\"Image: {row['ImageFile']}\")\n",
    "    print(f\"Actual   : {row[['X', 'Y', 'Z', 'W', 'P', 'Q', 'R']].values}\")\n",
    "    print(f\"Predicted: {row[['Pred_X', 'Pred_Y', 'Pred_Z', 'Pred_W', 'Pred_P', 'Pred_Q', 'Pred_R']].values}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.418182, 0.151939, 0.000666)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the uploaded CSV again\n",
    "csv_path = \"./DeepNav_test_score.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "# Define true and predicted column names based on actual file structure\n",
    "true_cols = ['X', 'Y', 'Z', 'W', 'P', 'Q', 'R']\n",
    "pred_cols = ['Pred_X', 'Pred_Y', 'Pred_Z', 'Pred_W', 'Pred_P', 'Pred_Q', 'Pred_R']\n",
    "\n",
    "y_true = df[true_cols].values\n",
    "y_pred = df[pred_cols].values\n",
    "\n",
    "# Compute MSE\n",
    "mse_xyz = mean_squared_error(y_true[:, :3], y_pred[:, :3])\n",
    "mse_wpqr = mean_squared_error(y_true[:, 3:], y_pred[:, 3:])\n",
    "custom_loss = mse_xyz + 400 * mse_wpqr\n",
    "custom_loss = round(custom_loss, 6)\n",
    "mse_xyz = round(mse_xyz, 6)\n",
    "mse_wpqr = round(mse_wpqr, 6)\n",
    "\n",
    "custom_loss, mse_xyz, mse_wpqr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_tf_20240812",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
